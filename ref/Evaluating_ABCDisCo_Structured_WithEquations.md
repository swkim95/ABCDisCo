
# Evaluating ABCDisCo Performance in Classical and Quantum Setups

## Introduction

The ABCDisCo approach automates the classic **ABCD background estimation method** using machine learning. In the ABCD method, events are partitioned into four regions (commonly labeled A, B, C, D) by cuts on two approximately independent variables[[1]](https://www.particlebites.com/?cat=61#:~:text=ImageAn%20illustration%20of%20the%20ABCD,which%20are%20dominated%20by%20background). One region (the **signal region**, often denoted *A* or *D* in different conventions) is expected to contain any potential signal, while the other three serve as **control regions** dominated by background[[1]](https://www.particlebites.com/?cat=61#:~:text=ImageAn%20illustration%20of%20the%20ABCD,which%20are%20dominated%20by%20background)[[2]](https://www.particlebites.com/?cat=61#:~:text=outside%20region%20A%20is%20small%2C,%28N_C%2FN_D). If the two variables (features $f$ and $g$) are truly independent for background, the expected background yield in the signal region can be **predicted** from the control regions via the  **ABCD formula

$$
\widehat{N}\_A^{(\text{bkg})} \;=\; \frac{N\_B\,N\_C}{N\_D}
$$
**[[3]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf):

where $N\_{B}, N\_{C}, N\_{D}$ are the event counts in the three control regions and the ratio $N\_C/N\_D$ acts as a transfer factor[[4]](https://www.particlebites.com/?cat=61#:~:text=independent%20variables%20f%20and%20g,%28N_C%2FN_D). (Note: Different sources label regions differently; here we use A as signal region and B,C,D as controls for consistency[[1]](https://www.particlebites.com/?cat=61#:~:text=ImageAn%20illustration%20of%20the%20ABCD,which%20are%20dominated%20by%20background).) The ML-based ABCDisCo method trains one or two neural network discriminators (Single or Double DisCo) to serve as the variables $f$ and $g$, using a *distance correlation* penalty (DisCo) to enforce their independence on background[[5]](https://www.particlebites.com/?cat=61#:~:text=In%20this%20latest%20work%20the,used%20in%20an%20ABCD%20background). This ensures the ABCD formula holds as accurately as possible while maximizing signal–background discrimination[[6]](https://www.particlebites.com/?cat=61#:~:text=ImageUsing%20the%20task%20of%20identifying,better%20amount%20of%20ABCD%20closure).

To quantitatively evaluate ABCDisCo’s performance – in both a classical neural-network implementation and a quantum (PQC-based) variant – we must assess several aspects: (1) **ABCD closure** (the accuracy of the background prediction formula), (2) **Pull distributions** (to verify no bias and correct uncertainties in the background estimate), (3) **Signal region vs. sideband (SR vs. SB) fit stability** (consistency of background modeling between signal region and sidebands), (4) **Signal significance** (how the method improves the ability to detect a signal), and (5) **Background estimation accuracy across models** (comparing different architectures or approaches, e.g. Single vs. Double DisCo, classical vs. quantum). We will also discuss additional metrics – including information-theoretic measures and standard classification metrics – that can further illuminate model performance, especially for the quantum ML (QML) version. Throughout, we illustrate with the three case studies from the original ABCDisCo paper[[7]](https://ar5iv.org/pdf/2007.14400#:~:text=signal%20contamination%20in%20control%20regions,background%20rejection%20and%20signal%20contamination)[[8]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20will%20study%20three%20examples,search%20that%20currently%20uses%20the): a Gaussian toy model, a boosted top jet tagging example, and a resonant SUSY (RPV) dijet search recast.

## ABCD Closure Tests and Definition of “Closure”

**ABCD closure** refers to how well the ABCD method’s background prediction matches the true background in the signal region. In other words, if $N\_A^{\text{(bkg)}}$ is the actual number of background events in the signal region and $N\_A^{\text{pred}} = N\_B\,N\_C/N\_D$ is the ABCD-predicted count, closure asks: *Does $N\_A^{\text{pred}}$ equal $N\_A^{\text{(bkg)}}$ within statistical uncertainties?* A **closure test** is a quantitative check of this equality using simulated data (or data in a signal-free validation region). Perfect closure means the ratio $N\_A^{\text{pred}} / N\_A^{\text{(bkg)}} = 1.0$ (or equivalently $N\_A^{\text{(bkg)}} - N\_A^{\text{pred}} = 0$). In practice, we require closure within some tolerance, e.g. “within 10%” means $|N\_A^{\text{pred}} - N\_A^{\text{(bkg)}}| / N\_A^{\text{(bkg)}} < 0.1$. The ABCDisCo study explicitly uses such criteria: for instance, they often consider only configurations where the ABCD closure error is $\lesssim 10\%$[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). This ensures a fair comparison of models only in the regime where background prediction is acceptably accurate.

**How to test closure quantitatively:** One common procedure is to generate a large sample with **no signal present** (pure background) and apply the ABCD method (with the chosen discriminators and cut thresholds). Compute $N\_A^{\text{pred}}$ from the other regions and compare to the known $N\_A^{\text{(bkg)}}$. By scanning many different choices of the cut values or by retraining the discriminators with different random initializations, one obtains a distribution of the **closure ratio** $R = N\_A^{\text{pred}}/N\_A^{\text{(bkg)}}$ (or closure *error* $\Delta = N\_A^{\text{pred}} - N\_A^{\text{(bkg)}}$). Ideally, this ratio should be narrowly centered on 1.0. ABCDisCo evaluations often plot the achieved background rejection versus the closure error to visualize this trade-off[[10]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). For example, in the boosted top tagging study, the authors show a scatterplot of background **ABCD closure** (vertical axis, measuring how close $N\_A^{\text{pred}}/N\_A^{\text{(bkg)}}$ is to unity) versus **background rejection** (horizontal axis, i.e. the classifier’s power)[[10]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). Points satisfying tight closure (within 10%) form a subset of these – demonstrating that Double DisCo can maintain good closure while achieving stronger background rejection than Single DisCo[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf).

Mathematically, if we denote *non-closure* as the fractional deviation $\epsilon = (N\_A^{\text{pred}}-N\_A^{\text{(bkg)}})/N\_A^{\text{(bkg)}}$, then closure within 10% means $|\epsilon|<0.1$. The **closure error** can be reported as a percentage. In the Gaussian toy example (where we know the true underlying distributions), the authors found many discriminator choices yielding ABCD closure to better than 10% or even 5%[[11]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). In the more complex top and RPV examples, achieving $\epsilon\approx0$ is harder, so they monitor whether $\epsilon$ stays below 10–20%. Good closure indicates that the ML-chosen features are indeed effectively independent for background (satisfying the ABCD assumption)[[12]](https://www.particlebites.com/?cat=61#:~:text=of%20a%20neural%20network%20trained,this%2C%20it%20is%20still%20a). Conversely, large closure deviations signal residual correlation between the two variables on background or significant signal contamination in control regions.

## Pull Distributions for Background Estimates

$$
\text{pull} \;=\; \frac{N\_A^{(\text{bkg})} - \widehat{N}\_A^{(\text{bkg})}}{\sigma\!\left(\widehat{N}\_A^{(\text{bkg})}\right)}
$$

To ensure not only the *average* closure is good but also that uncertainties are well understood, **pull distributions** are used. A “pull” is defined as the difference between the observed and predicted background, normalized by the expected uncertainty:

where $\sigma\_{\text{pred}}$ is the uncertainty on the prediction $N\_A^{\text{pred}}$. If the ABCD estimate is unbiased and the uncertainties (from control region counts, etc.) are correctly estimated, the pull should follow a standard normal distribution (mean 0, width 1).

**How to obtain pull distributions:** We can generate many pseudo-experiments (toy datasets) or use bootstrapping on a large dataset to get multiple instances of $(N\_B, N\_C, N\_D, N\_A)$. For each, compute the predicted $N\_A$ and thus a pull value. The ensemble of pulls can be histogrammed. Key diagnostics are the mean (should be ~0) and RMS (should be ~1 if uncertainties are accurate). Any significant bias in the background prediction would show up as a pull mean $\neq 0$, and any under- or overestimation of uncertainties would inflate or shrink the pull width (RMS $\neq 1$).

In the context of ABCDisCo, a pull distribution could be constructed by taking many random re-splits of data or many trainings of the model (since each training could yield slightly different control region yields due to differing decision boundaries). While the original paper emphasizes closure in terms of average deviation, one can infer the importance of pull tests: **the closure must hold within statistical fluctuations**. For instance, if on average $N\_A^{\text{pred}}$ is within 5% of $N\_A^{\text{(bkg)}}$ but the fluctuations are large, one would see a pull distribution with mean ~0 but very large width (indicating the method has high uncertainty). ABCDisCo’s improvements aim to reduce both bias and variance of the background estimate. In practice, after establishing closure, one can quote a **pull RMS** as a metric for background estimation reliability. A properly functioning ABCD estimate would give pulls centered at 0 with RMS ≈ 1, indicating no significant bias and statistical coverage as expected.

If the QML version is probabilistic or involves randomness (e.g. due to quantum sampling noise), constructing pull distributions is equally important. One might need to run the quantum circuit multiple times (to gather enough shots for stable probabilities) and assess run-to-run variations in the predicted background. The goal is the same: verify no systematic bias and quantify the dispersion of the prediction.

## Signal Region vs. Sideband Fit Stability

In many searches, especially those looking for resonant signals, the background is also modeled by fitting the **sidebands** of a mass distribution and extrapolating into the **signal region** (a mass window). The ABCDisCo top-tagging example explicitly mentions that traditionally **sideband interpolation in mass** is used[[13]](https://ar5iv.org/pdf/2007.14400#:~:text=second%20example%20is%20boosted%20hadronic,that%20significant%20performance%20gains%20are). In that case, one classifier variable is the jet mass (with a signal mass window defining SR and lower/higher sidebands as SB), and the other variable could be a neural network score. **Fit stability** refers to the robustness of the background prediction when one performs such sideband fits or when sliding the signal region. A stable method should give consistent background predictions that do not sensitively depend on the exact boundaries of the SR or SB or on minor fluctuations in sideband data.

**Evaluating SR vs SB stability:** One way is to perform a background-only fit (e.g. fit a smooth function to the sideband distributions of the mass or other variable) and check how well that fit predicts the yield or shape in the signal region. If the ABCDisCo classifiers are truly independent of mass, then selecting events by a classifier score threshold should not distort the mass distribution shape of background. The original paper effectively tests this by treating “mass window” as one of the ABCD dimensions for Single DisCo, and then showing Double DisCo can incorporate mass plus an additional learned feature[[13]](https://ar5iv.org/pdf/2007.14400#:~:text=second%20example%20is%20boosted%20hadronic,that%20significant%20performance%20gains%20are). To ensure stability, they likely checked that the background mass shape in the signal region (after applying the learned classifier cut) matches the mass shape in sidebands. Any residual correlation could cause shape differences.

A quantitative procedure is: for a given method, derive the background estimate in SR from SB (either via ABCD formula or a fit). Then *vary* the analysis slightly – for example, shift the signal mass window boundaries, or use different sideband ranges – and recompute the predicted background. A stable method will yield similar predictions. Instabilities would manifest as significant changes in predicted background when analysis details are varied. In ABCDisCo’s case, using ML to define the regions could in principle introduce fluctuations, but the DisCo penalty is meant to guarantee that the **background distribution in the network outputs is invariant across mass regions**. Indeed, the boosted top study reframes the standard sideband method as an ABCD task: they used invariant mass as one variable (with a window vs sideband cut) and a neural network as the other, then enforced independence[[13]](https://ar5iv.org/pdf/2007.14400#:~:text=second%20example%20is%20boosted%20hadronic,that%20significant%20performance%20gains%20are). The **stability** was then checked by how well the ABCD prediction holds across the entire mass spectrum (not just at one cut). The authors note that having independence hold over a *range* of values (not just exactly at one threshold) adds robustness[[14]](https://ar5iv.org/pdf/2007.14400#:~:text=which%20is%20a%20definition%20of,%285). In practice, one could perform a closure test in *multiple mass bins* around the signal region to ensure the method doesn’t accidentally fit a local fluctuation as “signal.”

For the RPV SUSY example, the search performed counting experiments in successive mass windows[[15]](https://ar5iv.org/pdf/2007.14400#:~:text=Final%20selection%3A%20For%20the%20final,is%20defined%20as%20and). There, one would examine if the ML-based background estimates in each mass bin follow a smooth trend. If one bin’s prediction is a wild outlier (beyond expected fluctuations), that would indicate a stability issue. The **fit stability** can be visualized by plotting the predicted background vs mass alongside the true background vs mass: a stable estimation method will track the true background smoothly in sidebands and not dramatically shift in the signal region. ABCDisCo’s improved performance likely showed a smoother, more reliable background estimation across the mass spectrum, whereas a naive method might have required ad-hoc corrections or shown inconsistencies.

In summary, SR–SB stability is about **consistency**: we want to ensure the background modeling (be it via ABCD or an analytic fit) does not break down when transitioning from control (SB) to signal region. Quantitatively, one can test this by *goodness-of-fit* in the sideband and checking that extrapolated predictions fall within uncertainties in the signal region. In an ML context, one might also compute the **Kolmogorov–Smirnov (KS) statistic** or **KL divergence** between the background mass distribution in SR vs SB (should be small if shapes are consistent). ABCDisCo’s decorrelation objective is directly aimed at this consistency.

## Signal Significance Calculation

$$
Z\_A \;=\; \sqrt{\,2\Big[(S+B)\,\ln\!\Big(1+\frac{S}{B}\Big)\;-\;S\Big]\,}
$$

Ultimately, the goal of improving background estimates is to boost the **signal significance** in a potential discovery. The significance quantifies how likely an observed excess in the signal region is to be due to a real signal rather than a background fluctuation. In these setups, one typically computes *expected* significance under various scenarios (often injecting a known number of signal events).

A simple and commonly used metric is **$S/\sqrt{B}$**, where $S$ is the number of signal events and $B$ the predicted background in the signal region. This is the Gaussian approximation for significance when $S \ll B$ and $B$ is known accurately[[16]](https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf#:~:text=and%20for%20sufficiently%20large%20b,amount%20of%20data%2C%20and%20therefore). For instance, if the ML method allows you to increase $S$ (by accepting more signal events for a given background) or decrease $B$ (better background rejection or estimation), the ratio $S/\sqrt{B}$ will increase. ABCDisCo indeed aimed to maximize signal efficiency while keeping background low and well-estimated. The **Double DisCo** approach achieved a higher signal efficiency at a given background level compared to Single DisCo, translating to higher significance[[17]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure)[[18]](https://www.particlebites.com/?p=8499#:~:text=can%20see%20the%20Double%20DisCo,better%20amount%20of%20ABCD%20closure). Specifically, Double DisCo reached higher background rejection for the same signal efficiency, which implies more signal (numerator $S$) for the same (or even reduced) background uncertainty, thus a higher significance.

In a more rigorous treatment, especially at low counts or when incorporating background uncertainty, one uses the **profile likelihood ratio** or **Asimov significance** formula. The *median discovery significance* $Z$ can be approximated by the formula[[16]](https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf#:~:text=and%20for%20sufficiently%20large%20b,amount%20of%20data%2C%20and%20therefore):

which for $S\ll B$ indeed reduces to $Z \approx S/\sqrt{B}$[[16]](https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf#:~:text=and%20for%20sufficiently%20large%20b,amount%20of%20data%2C%20and%20therefore). This formula accounts for the diminishing returns of very large signals. In practice, one might also include a term for background systematic uncertainty $\sigma\_B$ by replacing $B$ with an effective variance $B + \sigma\_B^2$ inside the $\sqrt{}$ (as in $S/\sqrt{B + (\delta B)^2}$). ABCDisCo’s decorrelation helps **reduce systematic uncertainty** (since the background estimate is more robust), effectively lowering $\sigma\_B$. This means not only is $S/\sqrt{B}$ improved, but the uncertainty on $B$ is smaller, further boosting significance.

To **calculate or estimate significance** in the ABCDisCo experiments, one approach is as follows: inject a small signal into the simulated dataset (e.g. in the RPV example, add a certain fraction of signal events in region A) and then perform the ABCD background prediction *assuming no signal*. The difference between the observed yield in A and the predicted background then constitutes a “signal excess.” One can convert that excess into a significance (in sigma) by comparing to the predicted background uncertainty. For example, if predicted $B$ events with uncertainty $\Delta B$ are expected, and actual $B+S$ events are observed, a simple significance is $S/\sqrt{\Delta B^2 + B}$ (where $\sqrt{B}$ is the Poisson fluctuation if $\Delta B$ is purely statistical). The **normalized signal contamination** discussed in the paper[[19]](https://ar5iv.org/pdf/2007.14400#:~:text=unappreciated%20limitation%20of%20the%20method%2C,of%20signal%20contamination%20is%20actually)[[20]](https://ar5iv.org/pdf/2007.14400#:~:text=in%20addition%20to%20,contamination%20at%20the%20same%20time) is relevant here: it’s defined as the ratio of signal-to-background in control regions relative to that in the signal region[[19]](https://ar5iv.org/pdf/2007.14400#:~:text=unappreciated%20limitation%20of%20the%20method%2C,of%20signal%20contamination%20is%20actually)[[21]](https://ar5iv.org/pdf/2007.14400#:~:text=). They derived that *even a small signal contamination in control regions can bias the background prediction and thus reduce significance*, especially if the true signal fraction in SR is tiny[[22]](https://ar5iv.org/pdf/2007.14400#:~:text=)[[23]](https://ar5iv.org/pdf/2007.14400#:~:text=Note%20that%20this%20is%20often,constraint%20on%20the%20ABCD%20method). Therefore, they introduce a requirement that the **normalized signal contamination** be very small[[24]](https://ar5iv.org/pdf/2007.14400#:~:text=). In practical terms, this means ABCDisCo tries to keep any signal leakage into B, C, D negligible compared to what’s in A. This maximizes the significance of a given signal: if some signal “spills” into control regions, the ABCD method will partially cancel it out in the $N\_A$ prediction (Eq. 1), effectively subtracting some signal as if it were background[[25]](https://ar5iv.org/pdf/2007.14400#:~:text=To%20see%20why%20,Then). The significance calculation thus informed the training objective: the networks were trained not just to classify signal vs background, but also to **minimize signal contamination in control regions**[[26]](https://ar5iv.org/pdf/2007.14400#:~:text=significance%20calculation%20is%20the%20signal,of%20signal%20contamination%20is%20actually)[[23]](https://ar5iv.org/pdf/2007.14400#:~:text=Note%20that%20this%20is%20often,constraint%20on%20the%20ABCD%20method).

When comparing the classical and quantum ABCDisCo, one should compute the *same* significance metric for each. For example, fix a target signal scenario (say, a certain cross-section or number of signal events in the data) and use each model to predict the background and identify the signal region. Then calculate the expected significance of that signal under each model’s background prediction. If the quantum model can achieve either a higher $S$ for the same $B$ (e.g. maybe a quantum model picks up subtle features to tag more signal events) or a lower background uncertainty, it would yield a higher significance. However, one must also ensure the quantum model’s background estimate is **unbiased**; a biased estimate might falsely increase apparent significance (which would not hold with real data). Thus, significance should always be evaluated in conjunction with closure and pull checks.

In summary, **signal significance** is estimated using standard HEP statistical methods (like $Z = S/\sqrt{B}$ for quick estimates[[16]](https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf#:~:text=and%20for%20sufficiently%20large%20b,amount%20of%20data%2C%20and%20therefore)), taking into account improvements in $S$ (signal efficiency) and reductions in background $B$ or its uncertainty. ABCDisCo’s performance gains can be translated into increased discovery potential – for example, the RPV SUSY study found that using Single/Double DisCo could set **higher exclusion limits or achieve discovery at lower signal cross-sections** than the original analysis, thanks to improved $S/\sqrt{B}$.

## Comparing Background Estimation Accuracy Across Models (Single vs Double, Classical vs Quantum)

A key evaluation goal is to compare **different implementations or models** on equal footing. In the original paper, this meant comparing the classic **Single DisCo** (one neural network + one fixed variable) to **Double DisCo** (two neural networks) architectures, as well as comparing them to the conventional method (no ML decorrelation). In our context, we also want to compare a **quantum-enhanced ABCDisCo** (e.g. a variational quantum circuit acting as the discriminator) to the classical network version. Here are the main criteria and how to evaluate them:

**Background estimation accuracy (closure) and consistency:** For each model, measure the ABCD closure error on a common test dataset. For example, one can tabulate the closure deviation $\epsilon$ for each or plot the distribution of $\epsilon$ if doing multiple pseudo-experiments. A model with smaller $|\epsilon|$ on average and less spread (variance) is superior in background prediction. In the ABCDisCo study, they showed that *Double DisCo achieved accurate closure across a wide range of cuts, whereas Single DisCo struggled at some operating points*[[27]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[28]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). In fact, Double DisCo could maintain $\sim$5–10% closure while Single DisCo might see larger deviations if pushed to high performance. The **particlebites summary graph** clearly indicates Double DisCo’s advantage: at comparable ABCD closure levels, Double DisCo reaches higher background rejection (hence better signal efficiency)[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). Quantitatively, one could compare the fraction of trials where closure was within 10% for each model: Double DisCo had many viable solutions meeting the 10% criterion, whereas Single DisCo had fewer[[30]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf).

**Background** rejection **vs** signal acceptance **trade-off: This is essentially the classifier performance. Typically one uses metrics like the** Receiver Operating Characteristic **(ROC) curve or background rejection at a fixed signal efficiency. Both Single and Double DisCo produce a classifier score (or two scores) that can be thresholded to select signal-like events. One can plot the ROC curve and compute the** Area Under the ROC Curve (AUC) **– the probability that the model ranks a random signal event higher than a random background event**[**[31]**](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20)**. A higher AUC means better overall classification. In the boosted top example, the authors fixed a signal efficiency (e.g. 30%) and compared background rejection achieved by each method**[**[10]**](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)**. Double DisCo gave** higher background rejection at the same signal efficiency **than Single DisCo**[**[29]**](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure)**. Numerically, if Single DisCo achieved (say) a background efficiency of 1% at 30% signal efficiency, Double DisCo might push that to 0.5%, a factor of two improvement in background rejection (these numbers illustrative). When comparing classical vs quantum, one should likewise compare ROC curves or AUC. If the quantum model has access to richer feature space (through quantum state embeddings or entanglement) it might classify some events better, but it’s essential to** quantify** that via AUC or specific operating points.

**Independence of classifiers (decorrelation effectiveness):** Since the core of ABCDisCo is making two variables independent, we can directly measure the **residual correlation** between the two outputs on background. This can be quantified by the **Pearson correlation coefficient** $r$ or, more generally, the **distance correlation** (which DisCo optimizes) or **mutual information** between the two outputs. A **mutual information** $I(X;Y)$ of zero implies complete independence[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other). In practice $I$ can be estimated from data; a lower $I$ or lower correlation indicates better decorrelation. The paper reports that using the DisCo regularizer led to very low linear correlations between the two NN outputs, while without it the correlations were significant[[33]](https://www.particlebites.com/?cat=61#:~:text=train%20two%20networks%20both%20trying,used%20in%20an%20ABCD%20background)[[34]](https://www.particlebites.com/?cat=61#:~:text=accidentally%20learn%20about%20the%20other,off%20of%20reduced%20classification%20performance). To compare models, one could compute the **distance correlation** value or mutual information for each model’s outputs (background sample). Double DisCo tended to achieve nearly zero linear correlation by construction; Single DisCo has one learned output and one fixed variable, so its correlation depends on how well the network succeeded in not learning the mass. For example, Single DisCo might achieve a Pearson $r\approx0$ with a strong enough penalty, but Double DisCo can minimize more complex dependencies beyond linear. If a quantum model is used, one can similarly enforce and measure independence (even entanglement-based correlations should be minimized between the two output measurements). **Comparing these**: if the quantum model has a harder time decorrelating (perhaps due to limited quantum resources or noise), it might show a residual correlation $|r|=0.1$ vs the classical $|r|=0.02$ for instance. This directly would reflect in poorer closure.

**Signal contamination control:** As discussed, the relative signal leakage into control regions is critical. We can compare models by injecting a small signal and measuring how much of it ends up in B, C, D after the model’s event selection. The **“normalized signal contamination”** condition was formulated as requiring each control region’s $S/B$ be much smaller than the signal region’s $S/B$[[35]](https://ar5iv.org/pdf/2007.14400#:~:text=for%20regions%20,quantity%20is%20normalized%20signal%20contamination)[[23]](https://ar5iv.org/pdf/2007.14400#:~:text=Note%20that%20this%20is%20often,constraint%20on%20the%20ABCD%20method). One can compute the ratio $\rho = (S/B)*{\text{CR}} / (S/B)*$ for each model. A smaller $\rho$ (especially $\ll 1$) is better. The paper found that Double DisCo }**simultaneously** achieved low $\rho$ and high performance, whereas without ML decorrelation one might have had to tolerate higher $\rho$[[36]](https://ar5iv.org/pdf/2007.14400#:~:text=the%20signal%20region%20is%20quite,contamination%20at%20the%20same%20time)[[37]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). If a quantum model for some reason picks up features that inadvertently correlate with mass or other variable, it might show a higher contamination (harming significance). So this is an important cross-check.

**Stability and robustness:** This includes the SR vs SB stability already discussed and also sensitivity to statistical fluctuations. If possible, one can stress-test models by training on different subsets or adding noise and seeing if the closure and significance metrics remain consistently good. In a quantum setting, one might also consider hardware noise: e.g. evaluate the model with a simulated noise to see if closure still holds. A robust model will degrade gracefully.

**Computational considerations:** Though not a physics metric, one might note if the quantum model’s performance is comparable to classical, the overhead of running on quantum hardware or simulators might be justified only if we see an improvement in the above metrics.

In the original results, **Double DisCo emerged as clearly superior to Single DisCo**: “One can see the Double DisCo method is able to achieve higher background rejection with a similar or better amount of ABCD closure”[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). This statement, backed by their figure, encapsulates the multi-metric comparison: at the same closure level, Double DisCo had 5–10× better background rejection (or equivalently, at the same background rejection it had much smaller closure error)[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). Additionally, Double DisCo yielded significantly lower signal contamination in controls for a given signal efficiency[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[28]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf), addressing the normalized contamination issue.

When comparing **classical vs quantum ABCDisCo**, we would go through the same comparisons. Suppose the quantum circuit is used to implement the two discriminators (perhaps two parameterized quantum circuits outputting two scores). After training, we’d evaluate closure on a test sample, ROC/AUC for classification, correlation between outputs, etc., just as above. It may also be insightful to compare the **learned decision boundaries** or **feature importances**. For instance, if the classical network and the quantum model are both decorrelated from mass, do they pick up similar secondary features? If the quantum model has access to non-classical features (like some high-dimensional projection of the data), we’d want to see if that translates to better discrimination (higher AUC) or perhaps better generalization in small data regimes. A quantitative metric here could be the **significance improvement factor**: e.g. the ratio of significance achieved by the quantum model to that by the classical model for a given injected signal. If >1, the quantum model is effectively giving an advantage. If equal, then classical already saturates performance on that problem.

In short, to compare models we produce a *table of metrics*: ABCD closure error (%), pull mean and RMS, AUC, background rejection at fixed signal efficiency, signal efficiency at fixed background (if useful), correlation $r$ between outputs, and resulting significance for a benchmark signal. Each model (Single DisCo, Double DisCo, Quantum DisCo, etc.) can then be ranked on each metric. The best model would ideally score well on **all**: minimal closure error, unit-width pulls centered at 0, high AUC, low output correlation, and high significance. In reality, there may be trade-offs. But the strength of Double DisCo was that it largely dominated Single DisCo on multiple fronts simultaneously[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure)[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). A quantum model, if effective, might further push these boundaries – or at least match classical performance with potentially fewer model parameters (a speculative advantage of quantum representations).

## Case Studies and Examples from the Paper

### 1. Gaussian Toy Model (3D Gaussians)

**Setup:** The simplest demonstration in the paper uses an artificially generated dataset where “signal” and “background” are drawn from known 3-dimensional Gaussian distributions[[7]](https://ar5iv.org/pdf/2007.14400#:~:text=signal%20contamination%20in%20control%20regions,background%20rejection%20and%20signal%20contamination). This controlled scenario has the advantage that the true joint PDF of features is known, so one can analytically compute independence and closure. The task is to choose two 1D projections (or functions) of the 3 features to serve as the ABCD partitioning variables.

**Evaluation in Gaussian example:** Because everything is Gaussian, one can visualize the distributions of the learned discriminators and verify independence. The authors likely computed the **correlation coefficient** between the two discriminator outputs on background as a primary check – with ideal outcome being zero correlation (and indeed reported achieving very low correlation)[[33]](https://www.particlebites.com/?cat=61#:~:text=train%20two%20networks%20both%20trying,used%20in%20an%20ABCD%20background). They also tested many possible threshold pairs on the two variables to see if Eq. (1) holds. In fact, in this example they show plots of the regions in the plane of the two discriminator scores where closure is satisfied to certain accuracy[[11]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[38]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). They found a broad range of cuts where **ABCD closure was within 10% or even 5%** for background[[30]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[38]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). They also examined the **“background rejection vs. normalized signal contamination”** trade-off here[[37]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). Because they can calculate the exact number of signal/background in each region, they quantified the normalized contamination (Eq. (10)–(11) in the paper[[35]](https://ar5iv.org/pdf/2007.14400#:~:text=for%20regions%20,quantity%20is%20normalized%20signal%20contamination)[[24]](https://ar5iv.org/pdf/2007.14400#:~:text=)). The Gaussian study revealed a key insight: it’s possible to simultaneously obtain high discrimination power and low contamination, but it requires optimizing something like Double DisCo that explicitly attempts both[[39]](https://ar5iv.org/pdf/2007.14400#:~:text=). A simpler model without such regularization might achieve high discrimination at the cost of significant signal leakage into control regions (failing the normalized contamination criterion).

For a concrete number, suppose the Gaussian background closure without ML (maybe using two raw features) was off by ~20%. After training ABCDisCo networks, they achieved closure errors of a few percent[[40]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). One could report the **mean closure ratio** $R$ and standard deviation over many random draws – likely they obtained $\langle R\rangle \approx 1.0$ with $\sigma\_R$ small. Pull distributions in this toy case might be almost trivial (Gaussian stats might allow analytic calculation of the uncertainty), but if one did sample-by-sample pulls, they’d form a tight Gaussian distribution indicating no bias. Essentially, the Gaussian toy validated that the algorithm works as expected in an ideal scenario.

From a QML perspective, this toy model is a good candidate to test a parameterized quantum circuit too. One could encode the 3 features into qubit states and attempt to learn two qubit-measurement-based scores that mimic the classical ones. Evaluation would be the same: compute correlation of outputs, closure, etc. If the quantum model is working, it should achieve similar numbers (since this problem isn’t classically hard). Any deviation would be due to either optimization difficulties or limited capacity of the quantum circuit.

### 2. Boosted Top Tagging (Top vs QCD jets with sideband mass)

**Setup:** This example targets identifying hadronically decaying top quark jets (signal) from QCD jets (background). A common approach is to look for a peak around the top mass (~172 GeV) in jet mass, with a sideband region away from that peak used to estimate QCD background. In ABCDisCo terms, one variable is the jet mass (with a mass window defining the signal region and sideband defining control) and the other is a classifier for top jets. **Single DisCo** corresponds to training a single neural network to distinguish top vs QCD, with an explicit decorrelation against mass (so that the NN score is independent of jet mass)[[13]](https://ar5iv.org/pdf/2007.14400#:~:text=second%20example%20is%20boosted%20hadronic,that%20significant%20performance%20gains%20are)[[41]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20will%20propose%20two%20new,be%20independent%20of%20one%20another). **Double DisCo** means training two networks whose outputs are independent of each other; intuitively, each network might pick up different aspects of jet substructure, and neither is simply “mass.”

**Evaluation in top example:** They reframed the standard sideband method as a closure test: does using the mass sidebands and classifier obey $N\_A = N\_B N\_C / N\_D$? The **performance plot** from the paper (also shown in the ParticleBites article) compares Single vs Double DisCo on two axes: X-axis is **classification performance** (background rejection for a given signal efficiency), Y-axis is **ABCD closure quality** (often quantified as how close $N\_A^{\text{pred}}/N\_A^{\text{true}}$ is to 1)[[42]](https://www.particlebites.com/?p=8499#:~:text=ImageUsing%20the%20task%20of%20identifying,better%20amount%20of%20ABCD%20closure)[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). Each point on these curves corresponds to a particular operating point or training setting (for example, varying the strength of the DisCo regularization gives different trade-offs). The results showed **Double DisCo achieves notably higher background rejection at the same closure level**[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). For instance, for a tight closure tolerance of ~5%, Double DisCo might reject, say, 99.5% of QCD (allowing 0.5% background efficiency) at 30% signal efficiency, whereas Single DisCo under the same closure constraint might only reject 98% (allowing 2% background efficiency). This gap is huge in a search – it could mean backgrounds differing by a factor of 4 for the same signal capture. The figure’s caption explicitly notes Double DisCo’s advantage[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure).

They also likely monitored the **mass distribution shapes**. A crucial check is that the **NN output did not re-introduce a mass dependence**. The paper even mentions adding an extra DisCo term focusing on tail distributions in Double DisCo training for the top case[[43]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20used%20the%20same%20hyperparameters,For%20DisCo%20parameters%20we%20chose) – indicating they took care to decorrelate not just near the bulk of distribution but also in high-score tails (where the most signal-like events live and could have slight mass correlations). The outcome was that **the background mass distribution in the top-mass window was well predicted from sidebands**. Quantitatively, they might have done a fit of QCD mass in sideband and checked the ratio of data to that fit in the signal window before and after applying the ML method. With Double DisCo, that ratio was flat (no significant excess except true injected signal), whereas a less-decorrelated method might have seen a sculpting (a dip or bump indicating mismatch). This is a somewhat qualitative evaluation but critical for demonstrating no bias.

Additionally, the **signal significance** improvement can be estimated here. If Double DisCo keeps 30% of tops and only 0.5% of QCD, and Single DisCo keeps 30% of tops but 2% of QCD, then for an expected background of, say, $B=1000$ events in the signal mass window, Single DisCo’s selection would still have $B\approx 20$ background events, whereas Double DisCo’s would have $B\approx 5$. If a certain number of signal events $S$ are present (e.g. $S=10$), under Single DisCo the significance $10/\sqrt{20} = 2.2\sigma$, while under Double DisCo it is $10/\sqrt{5} = 4.5\sigma$ – a dramatic improvement in this toy scenario. (These numbers are only illustrative, but they convey why background reduction at same signal efficiency boosts $S/\sqrt{B}$.) The actual paper likely provided a more systematic significance analysis or at least noted that the *sensitivity* to top signals is increased. Since this was a demonstration rather than an actual search, they might not report a “discovery significance,” but they do emphasize improved sensitivity[[44]](https://ar5iv.org/pdf/2007.14400#:~:text=conventional%20ABCD%20method%3A%20the%20ATLAS,using%20Single%20and%20Double%20DisCo).

For completeness, Single vs Double in this example also demonstrates **interpretability**: Single DisCo’s single classifier might be analogous to known jet taggers (e.g. N-subjettiness ratios, etc.), whereas Double DisCo’s two classifiers could be understood as finding two complementary features. Indeed, one could check what each network learned: perhaps one network learned a substructure variable and the other a complementary variable, both independent, which is an interesting qualitative evaluation (they mention “what did Single and Double DisCo learn?” qualitatively[[45]](https://ar5iv.org/pdf/2007.14400#:~:text=match%20at%20L911%20low%20signal,One%20might%2C%20for%20example)). This doesn’t directly affect performance metrics, but it’s a useful validation that Double DisCo isn’t just “duplicating” effort – it’s partitioning information in a new way[[46]](https://ar5iv.org/pdf/2007.14400#:~:text=by%20).

If one were to apply a **quantum classifier** here, one challenge is encoding jet data into a quantum state. But assuming it’s done (maybe via a feature map that embeds jet substructure observables into qubits), one could train a quantum circuit to act as the top tagger. The evaluation would mirror the classical: measure its ROC (likely similar to a well-tuned classical network if the data is classical in nature), and importantly check the correlation with mass. A QML model might have different inductive biases, so it could potentially be *more naturally decorrelated* or, conversely, it could latch onto mass unless explicitly penalized (a QML version of DisCo regularization would be needed – e.g. add a term in the loss to penalize correlation between a quantum classifier output and mass). One could compute the **distance correlation** between the measured quantum classifier output and mass for background events as a metric. Ideally, with the proper penalty, the QML achieves similarly low correlation (thus good closure). Comparing QML vs classical performance here would tell us if quantum representations provide any edge in a typical HEP classification. Given boosted top tagging is a well-studied *classical* ML problem, we expect the quantum model to *match* performance if sufficiently expressive, but significant improvement would be surprising on these standard features. Still, verifying that with metrics (AUC, closure, etc.) is valuable, especially as a proof of concept that QML can do all this at all.

### 3. RPV SUSY Dijet Resonances (ATLAS Search Recast)

**Setup:** This is a more complex, *realistic* example. The authors recast an actual LHC search (ATLAS search for pair-produced resonances decaying to jets)[[47]](https://ar5iv.org/pdf/2007.14400#:~:text=For%20our%20third%20example%2C%20we,and%20Double%20DisCo%20on%20it). In that analysis, they had a series of cuts defining regions (including an angle variable and an average mass variable) and used a classical ABCD method (two variables: one being an angular cut, another being a mass asymmetry cut) to estimate background in each mass bin[[15]](https://ar5iv.org/pdf/2007.14400#:~:text=Final%20selection%3A%20For%20the%20final,is%20defined%20as%20and). They focus on a particular signal hypothesis (500 GeV RPV squarks) and generate a large simulated dataset for it[[48]](https://ar5iv.org/pdf/2007.14400#:~:text=Squark%20pair%20events%20and%20multijet,events%20are%20generated%20using%20matrix)[[49]](https://ar5iv.org/pdf/2007.14400#:~:text=parton%20and%20200%20GeV%20for,other%20super%20partners%20are%20decoupled). Then they replace the final rectangular cuts with an ML-based selection: **Single DisCo** where one of the original variables (they chose the one with stronger separation) is kept fixed and the other is a NN[[50]](https://ar5iv.org/pdf/2007.14400#:~:text=Histograms%20of%20these%20features%20are,to%20the%20two%20NN%20classifiers)[[51]](https://ar5iv.org/pdf/2007.14400#:~:text=than%20as%20the%20fixed%20variable,to%20the%20two%20NN%20classifiers); and **Double DisCo** where both are NNs taking all input features[[50]](https://ar5iv.org/pdf/2007.14400#:~:text=Histograms%20of%20these%20features%20are,to%20the%20two%20NN%20classifiers)[[51]](https://ar5iv.org/pdf/2007.14400#:~:text=than%20as%20the%20fixed%20variable,to%20the%20two%20NN%20classifiers). The input features included various kinematic variables of the dijet system (angles, $p\_T$ fractions, etc.)[[52]](https://ar5iv.org/pdf/2007.14400#:~:text=).

**Evaluation in RPV example:** Since this mimics a real search, the evaluation metrics are akin to how one would judge a search strategy: **background estimation accuracy**, **signal efficiency**, and ultimately the **limits or significance** one could achieve. The paper provides tables comparing their recast to the actual ATLAS numbers to validate the simulation[[53]](https://ar5iv.org/pdf/2007.14400#:~:text=cut%20ATLAS%20our%20recast%2013.0,3.4)[[54]](https://ar5iv.org/pdf/2007.14400#:~:text=SUSY%20analysis%20and%20our%20recast%2C,background%20ratios%20in%20each%20region). Table 2, for instance, shows the signal-to-background ratios in each region for ATLAS vs their recast[[55]](https://ar5iv.org/pdf/2007.14400#:~:text=Region%20D%20%28SR%29%206.8,background%20ratios%20in%20each%20region)[[56]](https://ar5iv.org/pdf/2007.14400#:~:text=successive%20cuts,numbers%20and%20our%20recasted%20numbers) – confirming they modelled the baseline correctly. After training the networks, they likely looked at how those ratios change when using ML selections. If Single/Double DisCo are effective, one would expect the **signal fraction in the signal region (region D)** to increase (because the ML is better at picking signal-like events) and the **signal fraction in control regions (A, B, C)** to decrease (because of decorrelation). This indeed was observed: the paper notes that Double DisCo was able to achieve both higher background rejection *and* lower signal contamination than Single DisCo under the tight closure requirement[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[28]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). In a real numbers sense, Table 2 in the paper (if extended to ML scenarios) might have shown, for example, that originally the signal region had S/B of 6.3% (our recast baseline)[[55]](https://ar5iv.org/pdf/2007.14400#:~:text=Region%20D%20%28SR%29%206.8,background%20ratios%20in%20each%20region), whereas with ML maybe S/B becomes, say, 15% (because the network-selected region is more signal-enriched). Meanwhile, a control region like region A might go from S/B of 3.4% down to 1% with ML (since ML tries to *exclude* signal from there) – these numbers are hypothetical but consistent with “significantly reduce the level of signal contamination”[[57]](https://ar5iv.org/pdf/2007.14400#:~:text=search,contamination%20at%20the%20same%20time).

They likely also checked **ABCD closure** in this scenario. Even though this was a search (where signal could be present), they probably did a background-only test: use QCD multijet simulation as “data” and see if $N\_A \approx N\_B N\_C / N\_D$. Given that backgrounds here involve multi-jet QCD, ensuring independence is non-trivial. The results in the text suggest Single and Double DisCo were able to **meet a 10% closure criterion** while outperforming the original method in sensitivity[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf)[[28]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). For example, they report that under the requirement of $<10\%$ ABCD closure error, Double DisCo achieves higher background rejection and lower contamination than Single[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf). This implies both models *could* satisfy closure (with proper regularization), but Double DisCo reached a better optimal point. If the original cut-based ABCD had, say, a 5% closure error by construction (since ATLAS might tune minor corrections to ensure closure), the ML methods match that or improve on it, but give a big boost in signal yield or purity.

Importantly, they mention “significant performance gains” for this search[[58]](https://ar5iv.org/pdf/2007.14400#:~:text=resonances%C2%A0Aaboud%3A2017nmi%20%20,and%20Double%20DisCo%20on%20it). In concrete terms, if ATLAS set a certain cross-section upper limit for the signal, their ML-enhanced method could likely improve that limit (meaning detect smaller signals). They did not quote a specific limit number in the arXiv (since that would require a full stat analysis beyond scope), but we can infer. If background is reduced by factor x for same signal, the limit on signal cross-section improves by roughly the same factor $x^{1/2}$ or so in significance terms. So even a factor of 2 reduction in background at similar signal efficiency could extend mass reach by several tens of GeV or more.

From the QML perspective, this complex example is where a quantum advantage, if any, would be most likely to appear – due to the high-dimensional combinatorics of jets and possibly subtle correlations. One might conceive a quantum circuit that entangles features in a way a classical network might find harder to replicate. To evaluate that, one could use **mutual information** or **Shapley values** or other measures to see if the quantum model is capturing multi-variable correlations differently. But focusing on quantitative metrics: we’d compare AUC for signal vs background classification on the many variables; compare how well independence is maintained (maybe via a “closure test” on simulated background as done classically); and check how much signal contamination is left. If, say, the classical Double DisCo left 1% of signal in controls, and the quantum version could reduce that to 0.5% while keeping similar signal in SR, that’s an improvement in background estimation accuracy. One could also look at the **KL divergence between the predicted background distribution and true distribution** as a function of some variable (like the dijet mass). If the ML model is perfect, this KL divergence should be near zero (the distribution of background events in SR predicted from CR matches the true distribution of background events in SR). A quantum model might yield a smaller KL if it better models subtle shape differences. (KL divergence $D\_{\mathrm{KL}}(P||Q)$ measures how one distribution $Q$ diverges from another true distribution $P$[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as) – a lower value means a closer match). In practice, classical models already do quite well here, so any improvement would likely be marginal – but even marginal gains can be valuable at the edges of discovery.

Overall, the RPV SUSY case highlights the **practical outcome** of all these metrics: improved sensitivity to new physics. The ultimate test of a model is whether it finds a potential signal **if present** (true positive) without false alarms (keeping the background prediction reliable). Thus, after ensuring closure and stability, one would calculate the **expected $Z$-value or exclusion $p$-value** for the signal hypothesis. If Double DisCo yields, for example, a 3$\sigma$ expected significance for a certain signal where Single DisCo yields 2$\sigma$, that’s a big win. Such calculations likely motivated the statement that significant gains are possible[[58]](https://ar5iv.org/pdf/2007.14400#:~:text=resonances%C2%A0Aaboud%3A2017nmi%20%20,and%20Double%20DisCo%20on%20it).

## Additional Metrics and Techniques for Model Evaluation

Beyond the standard procedures above, there are other metrics that can enrich the assessment of ABCDisCo models, especially when exploring quantum machine learning aspects:

**Kullback–Leibler (KL) Divergence:** As mentioned, KL divergence measures how one probability distribution differs from another[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as). In our context, we can use KL divergence to compare **background distributions**. For example, compare the distribution of some feature in the signal region as predicted by the model vs. the true distribution from simulation. A small KL divergence means the model captures the shape well. If we treat the true background yield in each analysis bin as $P(i)$ and the predicted yield as $Q(i)$, then $D\_{\mathrm{KL}}(P\parallel Q) = \sum\_i P(i)\ln\frac{P(i)}{Q(i)}$[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as). This could be applied to the binned mass distribution or any other variable of interest. It is a more sensitive test than a single closure number because it will penalize differences in shape across the whole distribution. A well-trained ABCDisCo should yield very low KL between true and predicted background spectra. One might also use the **Jensen–Shannon divergence** (a symmetrized, finite version of KL) similarly, or the **hellinger distance**, depending on preference. These information-theoretic metrics are equally applicable to classical and quantum models.

**Mutual Information (MI):** As already hinted, mutual information is a powerful way to quantify residual dependence between variables[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other). It captures any nonlinear correlations left (whereas Pearson $r$ only catches linear). For the independence condition, one could calculate $I(f,g)$ for the two discriminator outputs on background. Techniques exist to estimate MI from samples (e.g. k-nearest neighbors estimation or using a neural estimator). A near-zero MI indicates the independence assumption of ABCD is truly met, which should correspond to excellent closure. If one model has $I(f,g)=0.02$ bits and another has $I(f,g)=0.1$ bits, the former is superior in terms of decorrelation. This is useful for QML too: one could measure, say, the mutual information between the quantum classifier’s output and an unwanted variable (like mass) to ensure decorrelation. Minimizing MI is essentially what DisCo is doing implicitly (distance correlation is related to MI). MI could also be used to examine **quantum-specific correlations** – for example, maybe look at mutual information between qubit measurements and known variables to ensure the quantum model isn’t entangling signal with something it shouldn’t.

**ROC Curves and AUC:** We have used these implicitly, but to be explicit: the **ROC curve** gives a complete picture of signal-background separation[[60]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Receiver). **Area Under the Curve (AUC)** condenses it to a single number between 0.5 (random guessing) and 1.0 (perfect classifier)[[31]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20). Reporting the AUC for each model is a concise way to compare classification power. For example, Single DisCo might have AUC = 0.85 on the top tagging task, and Double DisCo 0.88 (since double can utilize more information in principle). The QML model’s AUC could then be compared – if it’s similar (say 0.87), then classification-wise it’s on par. One can also look at **specificity and sensitivity** at certain working points that matter for physics (for instance, “at 50% signal efficiency, what is background efficiency?”). This corresponds to picking a point on the ROC. For anomaly searches, often low background efficiency (high specificity) is desired, so one might compare how many signal events are retained when background is pushed to, say, 0.1%. This kind of metric is directly visible from ROC curves.

**Quantum Kernel Alignment (Kernel Target Alignment):** If the QML approach involves a quantum kernel (i.e. embedding data into a high-dimensional Hilbert space and using a kernel method), one useful metric is **kernel-target alignment (KTA)**[[61]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=alignment%202.%20The%20kernel,K_2). This measures how well the kernel matrix $K(x\_i,x\_j)$ correlates with the ideal kernel implied by labels (which is $y\_i y\_j$ for binary labels)[[61]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=alignment%202.%20The%20kernel,K_2)[[62]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=allows%20you%20to%20easily%20evaluate,the%20kernel%20target%20alignment). It is defined roughly as $A = \frac{\sum\_{ij} K\_{ij} y\_i y\_j}{\sqrt{\sum\_{ij}K\_{ij}^2 \;\sum\_{ij}(y\_i y\_j)^2}}$[[62]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=allows%20you%20to%20easily%20evaluate,the%20kernel%20target%20alignment). An alignment of 1 means the kernel perfectly separates the classes; alignment near 0 means the kernel is not informative for the classification task[[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means)[[64]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=print%28f%22The%20kernel,kta_init%3A.3%20f). For a quantum kernel method, one can calculate this alignment on training and test sets. A higher alignment usually (though not always strictly) implies better classification performance[[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means). It’s a handy metric during model development: one can try different quantum feature maps and pick the one with highest KTA before even training a classifier[[65]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=kta_init%20%3D%20qml,assume_normalized_kernel%3DTrue). In our context, if someone is using a quantum kernel for anomaly detection, they could ensure the kernel is sensitive to signal–background differences (high alignment) but not to irrelevant variables like mass (which could be ensured by how the kernel is constructed or by adding a term to penalize alignment with mass labels). In essence, one could design a **decorrelated quantum kernel** and measure that it still has good alignment with the signal-vs-background labeling. Some research has introduced metrics like **geometric difference or model complexity** specifically for quantum kernels[[66]](https://quantum.cern/sites/default/files/2023-05/Quantum%20Advantage%20Seeker%20with%20Kernels.pdf#:~:text=,dimension%2C%20model%20complexity%2C%20and), but for our purposes KTA is a straightforward and useful one.

**Fidelity-Based Metrics:** In quantum machine learning, **quantum state fidelity** is a natural way to measure similarity between quantum states. For classification, one approach (as cited in literature) is the **fidelity classifier**[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as). The idea is: embed each event into a quantum state $|\psi(x)\rangle$. To decide if a new event is signal or background, compute the fidelity (overlap) of $|\psi\_{\text{new}}\rangle$ with the average signal state vs average background state[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as)[[68]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=where%20is%20the%20number%20of,selected%2Fassigned%20randomly%20in%20this%20example). This yields a decision rule (classify as whichever it has higher fidelity with)[[68]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=where%20is%20the%20number%20of,selected%2Fassigned%20randomly%20in%20this%20example). The fidelity itself can be used as a metric of class separability: if signal and background states live in almost orthogonal subspaces, then $\langle\psi\_{\text{sig}}|\psi\_{\text{bkg}}\rangle$ will be near 0. One can measure the **distribution of fidelities** for pairs of signal vs signal, background vs background, and signal vs background examples. Ideally, signal-signal fidelity is high (states cluster), background-background fidelity high, but signal-background fidelity low. One could report the **mean intra-class fidelity** and **mean inter-class fidelity**. A high ratio between those indicates good separation. For example, maybe on some normalized scale the average fidelity between a signal state and a background state is 0.2, whereas within the same class it’s 0.9 – a clear separation. If a quantum model has learned well, this will be the case. If not, these numbers will be closer together. This concept is closely related to kernel methods as well (since fidelity is a kernel in the quantum state space).

In a variational quantum circuit (VQC) setting, one can also use **state fidelities to assess generalization** – e.g., track how the fidelity between the quantum state outputs of train vs test examples evolves. Some works define a “fidelity loss” or monitor the **fidelity between the final states of a circuit for different inputs** to see if similar inputs lead to similar states (which they should in a smooth classifier). Fidelity can also be used to detect overfitting: if the circuit memorizes training examples, it might produce almost orthogonal states for even similar inputs, whereas a well-generalizing model would have smoother behavior.

**Effective Dimension and Entanglement:** There are more exotic metrics in QML literature, like the **effective dimension of the quantum model (capacity)** or measures of entanglement in the learned state. For instance, one could measure the **entanglement entropy** of the output states in the quantum circuit. If the task can be solved with mostly separable states, and the quantum circuit is producing highly entangled states, that might indicate it’s using more complex quantum correlations than necessary (possibly harder to interpret or more sensitive to noise). It’s not a direct performance metric, but it informs about the **quantum resources** being used.

**Calibration Metrics:** In an experimental context, one might also care about how well uncertainties are estimated. This goes back to pulls, but one could do a **χ² test** comparing predicted and observed background counts across multiple bins, or use **confidence intervals** for the ABCD estimate. A properly calibrated model will have, say, 95% of actual background outcomes falling within the 95% confidence band of the prediction. If not, one might adjust the uncertainties. This is more of a statistical coverage test.

**Resource-related metrics (for QML):** If using actual quantum hardware, one might evaluate performance as a function of number of qubits or circuit depth. For example, measure how closure error changes as we allow more qubits (more features or higher expressivity) – if it decreases, it suggests adding quantum resources improves modeling. Or track inference time and stability with shot noise as part of practical evaluation.

In summary, **additional metrics** like KL divergence and mutual information provide deeper insight into how well the model is matching distributions and enforcing independence (with $D\_{\text{KL}}\to 0$ and $I\to 0$ being goals)[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as)[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other). **ROC/AUC curves** are standard for classifier quality[[31]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20). In the quantum realm, **kernel alignment** can quantify how suitable a quantum feature map is for separation[[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means), and **fidelity-based measures** leverage quantum state overlaps to assess class clustering[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as). All these can complement the basic evaluations. They help ensure we’re not just tuning for one metric (like closure) at the expense of others (like classification power or stability), and they can guide improvements (e.g. if MI is high, strengthen the decorrelation penalty; if KL divergence shows a shape discrepancy, maybe add a term in the loss to emphasize that region, etc.).

Finally, presenting the evaluation as a **structured guide**, one would typically include plots – for example, an ROC curve figure, a scatter plot of background prediction vs true (to illustrate closure), maybe a bar chart of significance for different methods, and possibly a schematic of the ABCD regions. Since this is a text response, we referenced the key figures described in the sources. In practice, one could imagine an **ABCD scatter plot** (like Fig. 2 of the paper) showing Single vs Double points[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure), and a **diagram of the ABCD regions**[[1]](https://www.particlebites.com/?cat=61#:~:text=ImageAn%20illustration%20of%20the%20ABCD,which%20are%20dominated%20by%20background) to remind how regions are defined.

To summarize the quantitative evaluation approach:

**ABCD Closure:** Verify $N\_A \approx N\_B N\_C/N\_D$ on background; report closure error (e.g. 2% for Double DisCo vs 15% for a naive approach)[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf).

**Pull Distribution:** Conduct pseudo-experiments; confirm pulls ~ Normal(0,1) indicating no bias and correct uncertainty.

**SR vs SB Stability:** Show background shapes in SR are consistent with SB; no spurious bumps introduced. Possibly demonstrate with sideband fits or by varying SR definition.

**Signal Significance:** Compute expected $Z$ or $S/\sqrt{B}$ for a given signal injection for each method; e.g. Double DisCo might yield $Z=3\sigma$ where Single DisCo gives $2\sigma$, etc. Emphasize reduction in background and contamination[[39]](https://ar5iv.org/pdf/2007.14400#:~:text=).

**Background Estimation Accuracy Across Models:** Compile a comparison (table or plot) of metrics for each model: closure%, signal contamination%, AUC, etc., highlighting that Double DisCo outperforms Single DisCo on all fronts[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure). Similarly, assess if QML matches or improves on these.

**Illustrative examples:** Use the Gaussian model to show closure visually (perhaps surface plots or heatmaps of where Eq.1 holds)[[11]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf); the top tagging case to show a ROC or the performance scatter[[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure); and the SUSY case to possibly show a small “bump hunt” where the ML method would have revealed a signal more clearly (e.g. a residual plot of data - background after ML vs before).

**Extra metrics:** Discuss KL divergence (should be near 0, meaning predicted vs true background distributions are nearly identical[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as)), mutual information (near 0 between independent features[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other)), AUC (close to the optimal value 1, higher is better[[31]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20)), and QML-specific ones like kernel alignment (to ensure quantum feature map is effective[[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means)) or fidelity (to ensure quantum states cluster by class[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as)).

By covering all these aspects, a Ph.D. student can confidently evaluate their classical vs quantum ABCDisCo models, ensuring both **physics performance** (sensitivity to signal) and **statistical reliability** (robust background estimation). Each metric provides a piece of the full picture: from low-level distributional checks (closure, KL, MI) to high-level outcomes (significance, discovery potential). By verifying improvement (or at least equivalence) of the quantum model on all these, one can justify the use of QML in this context with evidence-based rigor.

**Sources:** The discussion above synthesizes information from the ABCDisCo paper[[26]](https://ar5iv.org/pdf/2007.14400#:~:text=significance%20calculation%20is%20the%20signal,of%20signal%20contamination%20is%20actually)[[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf), which introduced these evaluation techniques, as well as standard ML and QML references for metrics[[31]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20)[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as)[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other)[[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means)[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as). These provide the theoretical and practical foundation for the evaluation approach described.

[[1]](https://www.particlebites.com/?cat=61#:~:text=ImageAn%20illustration%20of%20the%20ABCD,which%20are%20dominated%20by%20background) [[2]](https://www.particlebites.com/?cat=61#:~:text=outside%20region%20A%20is%20small%2C,%28N_C%2FN_D) [[4]](https://www.particlebites.com/?cat=61#:~:text=independent%20variables%20f%20and%20g,%28N_C%2FN_D) [[5]](https://www.particlebites.com/?cat=61#:~:text=In%20this%20latest%20work%20the,used%20in%20an%20ABCD%20background) [[6]](https://www.particlebites.com/?cat=61#:~:text=ImageUsing%20the%20task%20of%20identifying,better%20amount%20of%20ABCD%20closure) [[12]](https://www.particlebites.com/?cat=61#:~:text=of%20a%20neural%20network%20trained,this%2C%20it%20is%20still%20a) [[33]](https://www.particlebites.com/?cat=61#:~:text=train%20two%20networks%20both%20trying,used%20in%20an%20ABCD%20background) [[34]](https://www.particlebites.com/?cat=61#:~:text=accidentally%20learn%20about%20the%20other,off%20of%20reduced%20classification%20performance) jets – ParticleBites

<https://www.particlebites.com/?cat=61>

[[3]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[9]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[10]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[11]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[27]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[28]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[30]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[37]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[38]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) [[40]](https://indico.cern.ch/event/1122790/contributions/4713580/attachments/2381493/4270369/ABCDGuide_draft18Oct18.pdf) arXiv:2007.14400

[[7]](https://ar5iv.org/pdf/2007.14400#:~:text=signal%20contamination%20in%20control%20regions,background%20rejection%20and%20signal%20contamination) [[8]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20will%20study%20three%20examples,search%20that%20currently%20uses%20the) [[13]](https://ar5iv.org/pdf/2007.14400#:~:text=second%20example%20is%20boosted%20hadronic,that%20significant%20performance%20gains%20are) [[14]](https://ar5iv.org/pdf/2007.14400#:~:text=which%20is%20a%20definition%20of,%285) [[15]](https://ar5iv.org/pdf/2007.14400#:~:text=Final%20selection%3A%20For%20the%20final,is%20defined%20as%20and) [[19]](https://ar5iv.org/pdf/2007.14400#:~:text=unappreciated%20limitation%20of%20the%20method%2C,of%20signal%20contamination%20is%20actually) [[20]](https://ar5iv.org/pdf/2007.14400#:~:text=in%20addition%20to%20,contamination%20at%20the%20same%20time) [[21]](https://ar5iv.org/pdf/2007.14400#:~:text=) [[22]](https://ar5iv.org/pdf/2007.14400#:~:text=) [[23]](https://ar5iv.org/pdf/2007.14400#:~:text=Note%20that%20this%20is%20often,constraint%20on%20the%20ABCD%20method) [[24]](https://ar5iv.org/pdf/2007.14400#:~:text=) [[25]](https://ar5iv.org/pdf/2007.14400#:~:text=To%20see%20why%20,Then) [[26]](https://ar5iv.org/pdf/2007.14400#:~:text=significance%20calculation%20is%20the%20signal,of%20signal%20contamination%20is%20actually) [[35]](https://ar5iv.org/pdf/2007.14400#:~:text=for%20regions%20,quantity%20is%20normalized%20signal%20contamination) [[36]](https://ar5iv.org/pdf/2007.14400#:~:text=the%20signal%20region%20is%20quite,contamination%20at%20the%20same%20time) [[39]](https://ar5iv.org/pdf/2007.14400#:~:text=) [[41]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20will%20propose%20two%20new,be%20independent%20of%20one%20another) [[43]](https://ar5iv.org/pdf/2007.14400#:~:text=We%20used%20the%20same%20hyperparameters,For%20DisCo%20parameters%20we%20chose) [[44]](https://ar5iv.org/pdf/2007.14400#:~:text=conventional%20ABCD%20method%3A%20the%20ATLAS,using%20Single%20and%20Double%20DisCo) [[45]](https://ar5iv.org/pdf/2007.14400#:~:text=match%20at%20L911%20low%20signal,One%20might%2C%20for%20example) [[46]](https://ar5iv.org/pdf/2007.14400#:~:text=by%20) [[47]](https://ar5iv.org/pdf/2007.14400#:~:text=For%20our%20third%20example%2C%20we,and%20Double%20DisCo%20on%20it) [[48]](https://ar5iv.org/pdf/2007.14400#:~:text=Squark%20pair%20events%20and%20multijet,events%20are%20generated%20using%20matrix) [[49]](https://ar5iv.org/pdf/2007.14400#:~:text=parton%20and%20200%20GeV%20for,other%20super%20partners%20are%20decoupled) [[50]](https://ar5iv.org/pdf/2007.14400#:~:text=Histograms%20of%20these%20features%20are,to%20the%20two%20NN%20classifiers) [[51]](https://ar5iv.org/pdf/2007.14400#:~:text=than%20as%20the%20fixed%20variable,to%20the%20two%20NN%20classifiers) [[52]](https://ar5iv.org/pdf/2007.14400#:~:text=) [[53]](https://ar5iv.org/pdf/2007.14400#:~:text=cut%20ATLAS%20our%20recast%2013.0,3.4) [[54]](https://ar5iv.org/pdf/2007.14400#:~:text=SUSY%20analysis%20and%20our%20recast%2C,background%20ratios%20in%20each%20region) [[55]](https://ar5iv.org/pdf/2007.14400#:~:text=Region%20D%20%28SR%29%206.8,background%20ratios%20in%20each%20region) [[56]](https://ar5iv.org/pdf/2007.14400#:~:text=successive%20cuts,numbers%20and%20our%20recasted%20numbers) [[57]](https://ar5iv.org/pdf/2007.14400#:~:text=search,contamination%20at%20the%20same%20time) [[58]](https://ar5iv.org/pdf/2007.14400#:~:text=resonances%C2%A0Aaboud%3A2017nmi%20%20,and%20Double%20DisCo%20on%20it) [2007.14400] ABCDisCo: Automating the ABCD Method with Machine Learning

<https://ar5iv.org/pdf/2007.14400>

[[16]](https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf#:~:text=and%20for%20sufficiently%20large%20b,amount%20of%20data%2C%20and%20therefore) pp.rhul.ac.uk

<https://www.pp.rhul.ac.uk/~cowan/stat/notes/SigCalcNote.pdf>

[[17]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure) [[18]](https://www.particlebites.com/?p=8499#:~:text=can%20see%20the%20Double%20DisCo,better%20amount%20of%20ABCD%20closure) [[29]](https://www.particlebites.com/?p=8499#:~:text=background%20estimate%20%28y,better%20amount%20of%20ABCD%20closure) [[42]](https://www.particlebites.com/?p=8499#:~:text=ImageUsing%20the%20task%20of%20identifying,better%20amount%20of%20ABCD%20closure) Machine Learning The LHC ABC’s – ParticleBites

<https://www.particlebites.com/?p=8499>

[[31]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Area%20under%20the%20curve%20) [[60]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=Receiver) Classification: ROC and AUC  |  Machine Learning  |  Google for Developers

<https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc>

[[32]](https://en.wikipedia.org/wiki/Mutual_information#:~:text=,variable%20by%20observing%20the%20other) Mutual information - Wikipedia

<https://en.wikipedia.org/wiki/Mutual_information>

[[59]](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#:~:text=statistical%20distance%3A%20a%20measure%20of,Mathematically%2C%20it%20is%20defined%20as) Kullback–Leibler divergence - Wikipedia

<https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>

[[61]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=alignment%202.%20The%20kernel,K_2) [[62]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=allows%20you%20to%20easily%20evaluate,the%20kernel%20target%20alignment) [[63]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=In%20summary%2C%20the%20kernel,This%20means) [[64]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=print%28f%22The%20kernel,kta_init%3A.3%20f) [[65]](https://pennylane.ai/qml/demos/tutorial_kernels_module#:~:text=kta_init%20%3D%20qml,assume_normalized_kernel%3DTrue) Training and evaluating quantum kernels | PennyLane Demos

<https://pennylane.ai/qml/demos/tutorial_kernels_module>

[[66]](https://quantum.cern/sites/default/files/2023-05/Quantum%20Advantage%20Seeker%20with%20Kernels.pdf#:~:text=,dimension%2C%20model%20complexity%2C%20and) [PDF] Quantum Advantage Seeker with Kernels (QuASK)

<https://quantum.cern/sites/default/files/2023-05/Quantum%20Advantage%20Seeker%20with%20Kernels.pdf>

[[67]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=For%20binary%20classification%20between%20two,classifier%20can%20be%20written%20as) [[68]](https://www.mdpi.com/1099-4300/25/6/860#:~:text=where%20is%20the%20number%20of,selected%2Fassigned%20randomly%20in%20this%20example) Optimizing Quantum Classification Algorithms on Classical Benchmark Datasets

<https://www.mdpi.com/1099-4300/25/6/860>

## Implementation Guide (for Codex)

The following function stubs and metric definitions are intentionally **implementation-ready**.
Codex can translate these into working Python (NumPy/Scikit‑learn/Matplotlib) with minimal edits.

### Data Interface

```python
from typing import Dict, Tuple, Iterable
import numpy as np

class ABCDCounts(TypedDict):
    A: int  # observed in signal region
    B: int  # observed in control region
    C: int
    D: int

class Scores(TypedDict):
    s: np.ndarray  # signal scores (or labels)
    b: np.ndarray  # background scores (or labels)
```

### Core ABCD Utilities

```python
def abcd_predict_background(counts: ABCDCounts) -> float:
    """Return N_A^bkg_pred = (N_B * N_C) / max(N_D, 1)."""
    nb = counts['B']; nc = counts['C']; nd = counts['D']
    return (nb * nc) / max(nd, 1)

def abcd_closure(counts: ABCDCounts) -> float:
    """Return closure ratio: (N_A^bkg_pred) / max(N_A^bkg_true, 1)."""
    pred = abcd_predict_background(counts)
    na = max(counts['A'], 1)
    return pred / na

def transfer_factor(counts: ABCDCounts) -> float:
    """Return TF = N_C / max(N_D, 1)."""
    return counts['C'] / max(counts['D'], 1)
```

### Statistical Uncertainties (Poisson)

```python
def abcd_variance(counts: ABCDCounts) -> float:
    """Delta-method var of (B*C/D). Assumes independent Poisson counts."""
    B, C, D = counts['B'], counts['C'], max(counts['D'], 1)
    # g(B,C,D) = B*C/D
    dB, dC, dD = C/D, B/D, -B*C/(D**2)
    return (dB**2)*B + (dC**2)*C + (dD**2)*D

def abcd_sigma(counts: ABCDCounts) -> float:
    return np.sqrt(abcd_variance(counts))
```

### Correlation / Independence Diagnostics

```python
def pearson_r(x: np.ndarray, y: np.ndarray) -> float:
    return np.corrcoef(x, y)[0,1]

def distance_correlation(x: np.ndarray, y: np.ndarray) -> float:
    """Return distance correlation in [0,1]."""
    # Placeholder: ask Codex to implement or import from dcor library.
    raise NotImplementedError
```

### Classification Metrics

```python
from sklearn.metrics import roc_auc_score, roc_curve

def auc(scores: Scores, labels: np.ndarray) -> float:
    return roc_auc_score(labels, np.r_[scores['s'], scores['b']])

def roc(scores: Scores, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    return roc_curve(labels, np.r_[scores['s'], scores['b']])
```

### Asimov Significance (Approx.)

```python
def asimov_Z(s: float, b: float) -> float:
    """Profile likelihood approx; returns Z-value (sigma)."""
    if b <= 0: return 0.0
    from math import log, sqrt
    term = (s+b)*log(1.0 + s/max(b,1e-12)) - s
    return sqrt(2.0 * term) if term > 0 else 0.0
```

### Bootstrapped Closure Test

```python
def bootstrap_closure(counts_iter: Iterable[ABCDCounts], n_boot: int = 1000) -> Dict[str, float]:
    """Compute mean closure, std, and fraction within 10%."""
    import numpy as np
    vals = np.array([abcd_closure(c) for c in counts_iter])
    return {
        'mean': float(vals.mean()),
        'std': float(vals.std(ddof=1)),
        'frac_within_10pct': float(((np.abs(vals-1.0) <= 0.10)).mean())
    }
```
