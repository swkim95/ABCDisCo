{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3873f1a8",
   "metadata": {},
   "source": [
    "\n",
    "# ABCDisCo Single-DisCo Tutorial (Torch + optional PennyLane backend)\n",
    "\n",
    "This notebook reproduces the **Single-DisCo** workflow described in [T. Aarrestad *et al.*, *Eur. Phys. J. C* **81**, 1003 (2021), arXiv:2007.14400](https://arxiv.org/abs/2007.14400). It mirrors the reference scripts shipped with this repository so you can validate the mass-decorrelated baseline before moving to the Double-DisCo configuration.\n",
    "\n",
    "> **Mapping to repository scripts**\n",
    "> - Data ingestion and scaling follow `ABCD_topjets_HLF_mD.py` (lines 69-101) together with the dataset helpers in `data_loader.py` (lines 1-63).\n",
    "> - The neural-network head reuses `networks.DNNclassifier` (lines 8-44), while the DisCo penalty mirrors `model.py` (lines 24-86) plus `disco.py` (lines 14-118).\n",
    "> - Evaluation adapts the single-score diagnostics from `evaluation.py` (lines 1-70), including the Jensen-Shannon divergence vs. background rejection scan.\n",
    "\n",
    "The workflow is organised as:\n",
    "\n",
    "1. **Setup & configuration** (Single-DisCo hyperparameters).\n",
    "2. **Data loading and preprocessing** (min-max scaling, feature selection matching `ABCD_topjets_HLF_mD.py`).\n",
    "3. **Model definition** with interchangeable Torch/PennyLane heads.\n",
    "4. **Training** with the DisCo mass decorrelation penalty.\n",
    "5. **Diagnostics & evaluation**: ROC curves, distance correlations, JSD vs. background rejection, and mass sculpting checks.\n",
    "6. **Export** of trained weights and inference scores.\n",
    "\n",
    "> **Datasets**: The repository already ships reduced CMS top-tagging HLF samples (`topsample_*_tau.dat.gz`). You can run this notebook end-to-end without external downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015d9e7",
   "metadata": {},
   "source": [
    "\n",
    "## Environment preparation\n",
    "\n",
    "Run the following cell *once per environment* if you still need to install the CPU builds of PyTorch, PennyLane, and the lightweight analysis stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment the lines you need)\n",
    "# %pip install numpy pandas scikit-learn matplotlib tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install pennylane pennylane-lightning\n",
    "# %pip install pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe8e3b",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration (mirrors `ABCD_topjets_HLF_mD.py` lines 69-126)\n",
    "\n",
    "We keep the dataset limits, optimiser choices, and DisCo penalty normalisation consistent with the single-network script so that this notebook can reproduce the published baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Repository-local modules (mirror original scripts)\n",
    "from disco import distance_corr_unbiased\n",
    "from networks import DNNclassifier\n",
    "\n",
    "try:\n",
    "    import pennylane as qml  # optional backend\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "except ImportError:  # keep optional\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-talk\")\n",
    "\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_ROOT = Path(\".\")\n",
    "RAW_FILES = {\n",
    "    \"train\": DATA_ROOT / \"topsample_train_tau.dat.gz\",\n",
    "    \"val\": DATA_ROOT / \"topsample_val_tau.dat.gz\",\n",
    "    \"test\": DATA_ROOT / \"topsample_test_tau.dat.gz\",\n",
    "}\n",
    "\n",
    "# Toggle to match the full training statistics used in the paper.\n",
    "FULL_DATASET = False\n",
    "EVENT_LIMITS = {\n",
    "    \"train\": 50000 if not FULL_DATASET else None,\n",
    "    \"val\": 50000 if not FULL_DATASET else None,\n",
    "    \"test\": 50000 if not FULL_DATASET else None,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 25 if not FULL_DATASET else 200\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# DisCo regularisation weight (cf. model.py lines 24-86)\n",
    "LAMBDA_MASS = 200.0  # strength of the distance-correlation penalty between score and jet mass\n",
    "\n",
    "# QML backend knobs (used only when BACKEND == \"qml\")\n",
    "BACKEND = \"torch\"  # switch to \"qml\" after installing PennyLane\n",
    "N_QUBITS = 6\n",
    "QML_LAYERS = 2\n",
    "QML_DEVICE = \"default.qubit\"\n",
    "\n",
    "ORIGINAL_FEATURES = [\n",
    "    \"mass\",\n",
    "    \"pt\",\n",
    "    \"tau1_half\",\n",
    "    \"tau2_half\",\n",
    "    \"tau3_half\",\n",
    "    \"tau1\",\n",
    "    \"tau2\",\n",
    "    \"tau3\",\n",
    "    \"tau4\",\n",
    "    \"tau1_sq\",\n",
    "    \"tau2_sq\",\n",
    "    \"tau3_sq\",\n",
    "    \"tau4_sq\",\n",
    "]\n",
    "\n",
    "# The single-network setup removes the first two observables (mass, pT) from the classifier input\n",
    "SINGLE_FEATURE_INDICES = list(range(2, len(ORIGINAL_FEATURES)))\n",
    "FEATURE_NAMES = [ORIGINAL_FEATURES[i] for i in SINGLE_FEATURE_INDICES]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb8ffe",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data loading & scaling (`ABCD_topjets_HLF_mD.py` lines 69-95)\n",
    "\n",
    "The original script concatenates train/validation/test splits, applies a global min-max scaling to all 13 high-level features, and then selects the 11 observables used for the single-network classifier. We reproduce that procedure verbatim while keeping the jet mass available for decorrelation diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6980f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _load_tau_file(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "    with gzip.open(path, \"rt\") as handle:\n",
    "        array = np.loadtxt(handle, delimiter=\",\", skiprows=15)\n",
    "    if array.ndim != 2 or array.shape[1] != 14:\n",
    "        raise ValueError(f\"Unexpected shape {array.shape} in {path}\")\n",
    "    return array\n",
    "\n",
    "\n",
    "def load_and_scale(raw_files: Dict[str, Path]) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "    \"\"\"Replicate the min-max scaling used in `ABCD_topjets_HLF_mD.py`.\"\"\"\n",
    "    raw_arrays = {split: _load_tau_file(path) for split, path in raw_files.items()}\n",
    "\n",
    "    concatenated = np.vstack([arr[:, 1:] for arr in raw_arrays.values()])\n",
    "    feat_min = concatenated.min(axis=0)\n",
    "    feat_range = np.maximum(concatenated.max(axis=0) - feat_min, 1e-8)\n",
    "\n",
    "    features: Dict[str, np.ndarray] = {}\n",
    "    labels: Dict[str, np.ndarray] = {}\n",
    "    masses: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for split, array in raw_arrays.items():\n",
    "        scaled = (array[:, 1:] - feat_min) / feat_range\n",
    "        features[split] = scaled.astype(np.float32)\n",
    "        labels[split] = array[:, 0].astype(np.float32)\n",
    "        masses[split] = array[:, 1].astype(np.float32)\n",
    "\n",
    "    return features, labels, masses\n",
    "\n",
    "\n",
    "features_all, labels, masses = load_and_scale(RAW_FILES)\n",
    "features = {split: feats[:, SINGLE_FEATURE_INDICES] for split, feats in features_all.items()}\n",
    "\n",
    "summary = []\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    limit = EVENT_LIMITS[split]\n",
    "    n_events = features[split].shape[0] if limit is None else min(limit, features[split].shape[0])\n",
    "    summary.append({\n",
    "        \"split\": split,\n",
    "        \"events\": n_events,\n",
    "        \"signal_fraction\": float(labels[split][:n_events].mean()),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "display(summary_df)\n",
    "\n",
    "pd.DataFrame(features[\"train\"][:5], columns=FEATURE_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53adbcb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Torch datasets (`data_loader.py` lines 22-63)\n",
    "\n",
    "We wrap the min-max scaled arrays into PyTorch `Dataset` objects that expose the classifier inputs, labels, per-event weights, and jet masses used in the DisCo penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f014a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchTopTaggingDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, mass: np.ndarray, weight: np.ndarray | None = None):\n",
    "        self.x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "        if weight is None:\n",
    "            weight = np.ones_like(y, dtype=np.float32)\n",
    "        self.w = torch.as_tensor(weight, dtype=torch.float32)\n",
    "        self.mass = torch.as_tensor(mass, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx], self.w[idx], self.mass[idx]\n",
    "\n",
    "\n",
    "def _clip_events(split: str, array: np.ndarray) -> np.ndarray:\n",
    "    limit = EVENT_LIMITS[split]\n",
    "    if limit is None:\n",
    "        return array\n",
    "    return array[:limit]\n",
    "\n",
    "\n",
    "train_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"train\", features[\"train\"]),\n",
    "    _clip_events(\"train\", labels[\"train\"]),\n",
    "    _clip_events(\"train\", masses[\"train\"]),\n",
    ")\n",
    "val_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"val\", features[\"val\"]),\n",
    "    _clip_events(\"val\", labels[\"val\"]),\n",
    "    _clip_events(\"val\", masses[\"val\"]),\n",
    ")\n",
    "test_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"test\", features[\"test\"]),\n",
    "    _clip_events(\"test\", labels[\"test\"]),\n",
    "    _clip_events(\"test\", masses[\"test\"]),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22662386",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model backends (`networks.py` lines 8-44)\n",
    "\n",
    "The Single-DisCo setup uses a single `DNNclassifier` head. We also expose an optional PennyLane quantum layer to demonstrate how the architecture can be swapped for a variational quantum classifier without changing the loss logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb324d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchSingleDisco(nn.Module):\n",
    "    \"\"\"Wrap one `DNNclassifier` head exactly as in the Single-DisCo script.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.head = DNNclassifier(n_features, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        logits = self.head(x)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "\n",
    "class PennyLaneSingleDisco(nn.Module):\n",
    "    \"\"\"Minimal PennyLane VQC head compatible with the Single-DisCo loss.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int, n_qubits: int = 6, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "        if not PENNYLANE_AVAILABLE:\n",
    "            raise RuntimeError(\"PennyLane is not installed. Run `%pip install pennylane pennylane-lightning`.\")\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "\n",
    "        qdevice = qml.device(device_name, wires=n_qubits)\n",
    "        weight_shapes = {\"weights\": (layers, n_qubits)}\n",
    "\n",
    "        @qml.qnode(qdevice, interface=\"torch\")\n",
    "        def circuit(inputs, weights):\n",
    "            x_pad = torch.zeros(n_qubits, dtype=inputs.dtype, device=inputs.device)\n",
    "            take = min(inputs.shape[-1], n_qubits)\n",
    "            x_pad[:take] = inputs[..., :take]\n",
    "            qml.templates.AngleEmbedding(x_pad, wires=range(n_qubits), rotation=\"Y\")\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(2)]\n",
    "\n",
    "        self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "        self.head = nn.Linear(2, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        q_inputs = x[:, : self.n_qubits]\n",
    "        q_features = self.qlayer(q_inputs)\n",
    "        logits = self.head(q_features)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "\n",
    "def build_model(n_features: int) -> nn.Module:\n",
    "    if BACKEND == \"qml\":\n",
    "        model = PennyLaneSingleDisco(n_features, n_qubits=N_QUBITS, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "    else:\n",
    "        model = TorchSingleDisco(n_features)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "model = build_model(len(FEATURE_NAMES))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a33af9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Loss function with DisCo penalty (`model.py` lines 24-86 & `disco.py`)\n",
    "\n",
    "We compute the weighted binary cross-entropy loss and add the unbiased distance-correlation penalty between the classifier score and jet mass on background events, following the original `train_model(..., decorr_mode='dist_unbiased')` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance_corr_safe(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
    "    if x.numel() <= 2 or y.numel() <= 2:\n",
    "        return torch.zeros(1, device=x.device, dtype=x.dtype)\n",
    "    normed = weight / (weight.sum() + 1e-12) * len(weight)\n",
    "    return distance_corr_unbiased(x, y, normed, power=1)\n",
    "\n",
    "\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    weights = weights.to(DEVICE)\n",
    "    masses = masses.to(DEVICE)\n",
    "\n",
    "    logits, score = model(features)\n",
    "    loss_cls = F.binary_cross_entropy(score, labels, weight=weights)\n",
    "    loss = loss_cls\n",
    "\n",
    "    metrics = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "    }\n",
    "\n",
    "    background = labels < 0.5\n",
    "    if background.any() and LAMBDA_MASS > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + LAMBDA_MASS * d_mass\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d446a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training loop (`model.py` lines 88-170)\n",
    "\n",
    "We adapt the original `train`/`val` helpers to work seamlessly on CPU/GPU and to log the DisCo decorrelation term alongside the classification loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6468b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, list[float]] = {}\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    return {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    scores, labels_all, weights_all, masses_all = [], [], [], []\n",
    "    agg: Dict[str, list[float]] = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"eval\"):\n",
    "            loss, metrics = compute_losses(model, batch)\n",
    "            features, labels, weights, masses = batch\n",
    "            features = features.to(DEVICE)\n",
    "            _, score = model(features)\n",
    "            scores.append(score.cpu().numpy())\n",
    "            labels_all.append(labels.numpy())\n",
    "            weights_all.append(weights.numpy())\n",
    "            masses_all.append(masses.numpy())\n",
    "            agg.setdefault(\"loss\", []).append(float(loss.detach().cpu()))\n",
    "            for key, value in metrics.items():\n",
    "                agg.setdefault(key, []).append(value)\n",
    "    scores = np.concatenate(scores)\n",
    "    labels_all = np.concatenate(labels_all)\n",
    "    weights_all = np.concatenate(weights_all)\n",
    "    masses_all = np.concatenate(masses_all)\n",
    "    metrics_mean = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    return scores, labels_all, weights_all, masses_all, metrics_mean\n",
    "\n",
    "\n",
    "history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer)\n",
    "    s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "    fpr, tpr, _ = roc_curve(y_val, s_val, sample_weight=w_val)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    record = {\n",
    "        \"epoch\": epoch,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
    "        **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
    "    }\n",
    "    history.append(record)\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | AUC={roc_auc:.3f} | \"\n",
    "        f\"train_loss={train_metrics['loss_cls']:.3f} | val_loss={val_metrics['loss_cls']:.3f}\"\n",
    "    )\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46a892",
   "metadata": {},
   "source": [
    "\n",
    "### Training diagnostics\n",
    "\n",
    "We track the classification loss and the distance-correlation penalty to verify convergence and decorrelation strength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec09beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], label=\"train\")\n",
    "ax[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls\"], label=\"val\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Binary cross-entropy\")\n",
    "ax[0].set_title(\"Classification loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "if \"train_dCorr_s_m\" in history_df and \"val_dCorr_s_m\" in history_df:\n",
    "    ax[1].plot(history_df[\"epoch\"], history_df[\"train_dCorr_s_m\"], label=\"train\")\n",
    "    ax[1].plot(history_df[\"epoch\"], history_df[\"val_dCorr_s_m\"], label=\"val\")\n",
    "    ax[1].set_ylabel(\"Distance correlation\")\n",
    "else:\n",
    "    ax[1].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], color=\"tab:gray\")\n",
    "    ax[1].set_ylabel(\"Placeholder (enable LAMBDA_MASS to view decorrelation)\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_title(\"Score-mass decorrelation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483491cb",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluation (`evaluation.py` lines 1-70)\n",
    "\n",
    "We reproduce the single-score diagnostics: ROC curves, background mass sculpting, and the Jensen-Shannon divergence versus background rejection figure of merit used in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd85125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluation import JSDvsR\n",
    "\n",
    "s_test, y_test, w_test, m_test, test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, s_test, sample_weight=w_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Background-only correlation for reporting\n",
    "background = y_test < 0.5\n",
    "if background.any():\n",
    "    d_test = distance_corr_safe(\n",
    "        torch.as_tensor(s_test[background]),\n",
    "        torch.as_tensor(m_test[background]),\n",
    "        torch.ones_like(torch.as_tensor(m_test[background])),\n",
    "    ).item()\n",
    "else:\n",
    "    d_test = float(\"nan\")\n",
    "\n",
    "print(f\"Test AUC = {roc_auc:.3f}\")\n",
    "print(f\"Background distance-correlation(score, mass) = {d_test:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.3f})\")\n",
    "axes[0].set_xlabel(\"Background efficiency\")\n",
    "axes[0].set_ylabel(\"Signal efficiency\")\n",
    "axes[0].set_title(\"ROC curve\")\n",
    "axes[0].legend()\n",
    "\n",
    "nbins = 40\n",
    "mass_range = (m_test.min(), m_test.max())\n",
    "axes[1].hist(\n",
    "    m_test[background], bins=nbins, range=mass_range, weights=w_test[background],\n",
    "    histtype=\"step\", label=\"All background\", linewidth=2\n",
    ")\n",
    "cut = np.quantile(s_test[y_test > 0.5], 0.7)  # 30% background efficiency target\n",
    "sel = background & (s_test > cut)\n",
    "axes[1].hist(\n",
    "    m_test[sel], bins=nbins, range=mass_range, weights=w_test[sel],\n",
    "    histtype=\"stepfilled\", alpha=0.4, label=\"Background above score cut\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Jet mass [GeV]\")\n",
    "axes[1].set_ylabel(\"Weighted events\")\n",
    "axes[1].set_title(\"Mass sculpting check\")\n",
    "axes[1].legend()\n",
    "\n",
    "points = []\n",
    "for sigeff in (10, 30, 50):\n",
    "    rej, inv_jsd = JSDvsR(\n",
    "        sigscore=s_test[y_test > 0.5],\n",
    "        bgscore=s_test[background],\n",
    "        bgmass=m_test[background],\n",
    "        sigweights=w_test[y_test > 0.5],\n",
    "        bgweights=w_test[background],\n",
    "        sigeff=sigeff,\n",
    "        nbins=nbins,\n",
    "        minmass=mass_range[0],\n",
    "        maxmass=mass_range[1],\n",
    "    )\n",
    "    points.append((rej, inv_jsd, sigeff))\n",
    "\n",
    "for rej, inv_jsd, eff in points:\n",
    "    axes[2].scatter(rej, inv_jsd, label=f\"Sig eff {eff}%\")\n",
    "axes[2].set_xlabel(\"Background rejection (1 / ε_B)\")\n",
    "axes[2].set_ylabel(\"1 / JSD\")\n",
    "axes[2].set_title(\"JSD vs. background rejection\")\n",
    "axes[2].legend()\n",
    "axes[2].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ab8b0",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Persist artefacts\n",
    "\n",
    "Save inference scores and the trained model weights for downstream ABCDisCo or `pyhf` studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd407c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"score\": s_test,\n",
    "    \"label\": y_test,\n",
    "    \"weight\": w_test,\n",
    "    \"mass\": m_test,\n",
    "})\n",
    "\n",
    "results_path = Path(\"abcdisco_single_disco_scores.parquet\")\n",
    "weights_path = Path(\"abcdisco_single_disco_model.pt\")\n",
    "\n",
    "results_df.to_parquet(results_path, index=False)\n",
    "torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "results_path, weights_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811ab61",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Extending to Double-DisCo & QML studies\n",
    "\n",
    "- **Recovering the paper-level numbers**: set `FULL_DATASET = True`, increase `EPOCHS` to 200, and sweep `LAMBDA_MASS` in the range 50-400 as in the reference scans.\n",
    "- **Quantum experiments**: switch `BACKEND = \"qml\"`, tune `N_QUBITS`/`QML_LAYERS`, and initialise the PennyLane device with `qml.seed(SEED)` for reproducibility.\n",
    "- **Transition to Double-DisCo**: after validating this notebook, open `ABCDisCo_tutorial.ipynb` for the two-network variant and reuse the saved preprocessing steps to initialise the dual heads.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
