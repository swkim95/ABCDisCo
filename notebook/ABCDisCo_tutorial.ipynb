{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4bfa64",
   "metadata": {},
   "source": [
    "# ABCDisCo (Automating the ABCD Method) — Annotated Notebook (with optional QML backend)\n",
    "\n",
    "This notebook mirrors the official ABCDisCo scripts and adds cell-by-cell explanations, plots, and a plug‑in interface to swap the **classical NN** for a **Quantum ML (VQC)** model. Use it as a starting point to run ABCDisCo on CMS Run‑2/Run‑3 NanoAOD‑derived datasets.\n",
    "\n",
    "**Primary references**\n",
    "\n",
    "- ABCDisCo paper: Kasieczka, Nachman, Schwartz, Shih, *Phys. Rev. D* **103**, 035021 (2021), arXiv:2007.14400.\n",
    "- ABCDisCo official code (scripts): `davidshih17/ABCDisCo` (files like `model_ABCD_2NN.py`, `disco.py`, `ABCD_topjets_*.py`).\n",
    "- DisCo (distance correlation) repo with a notebook: `davidshih17/DisCo` (`plotter_v4.ipynb`).\n",
    "- CMS‑ML notes on **Double DisCo** architecture (why two decorrelated discriminants).\n",
    "- Likelihood‑based ABCD tutorial: `pyhf` docs and example notebooks.\n",
    "\n",
    "> **What this adds vs the scripts**\n",
    ">\n",
    "> 1. Clear separation of **data loading**, **model**, **loss (DisCo/Double‑DisCo)**, **training**, **ABCD closure checks**, and optional **likelihood fit** (pyhf).\n",
    "> 2. A **backend interface** so you can choose between a PyTorch NN and a PennyLane‑based VQC (QNN) with the *same* trainer and DisCo regularizer.\n",
    "> 3. CMS‑friendly convenience: uproot/awkward loading for NanoAOD, feature lists, scalers, per‑era reweighting hooks, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Installs (run in your env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cea64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies (uncomment as needed in your environment)\n",
    "# %pip install numpy pandas scikit-learn matplotlib tqdm pyhf uproot awkward coffea torch torchvision torchaudio\n",
    "# %pip install pennylane pennylane-lightning  # for the QML (VQC) backend\n",
    "# %pip install qiskit                         # optional: run VQC on Aer simulator or IBM backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300b8fa",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b51750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- User knobs (edit) ----\n",
    "DATA_PATH = Path(\"./data\")  # directory containing parquet/csv/root files\n",
    "TRAIN_FILE = \"train.parquet\"   # or \"train.root\"\n",
    "VAL_FILE   = \"val.parquet\"\n",
    "TEST_FILE  = \"test.parquet\"\n",
    "\n",
    "# Column names\n",
    "LABEL_COL = \"label\"           # 1 for signal, 0 for background\n",
    "WEIGHT_COL = \"weight\"         # event weight; if absent, a column will be created with ones\n",
    "MASS_COL = \"mass\"             # variable to decorrelate against for bkg (e.g., m_jj, m_SD, etc.)\n",
    "\n",
    "# Feature list (example; replace with your HLFs/LLFs)\n",
    "FEATURES = [\n",
    "    \"pt\", \"eta\", \"phi\",\n",
    "    \"tau21\", \"tau32\",\n",
    "    \"subjet1_pt\", \"subjet2_pt\"\n",
    "]\n",
    "\n",
    "# ABCD region definition: thresholds on the two discriminants s1, s2\n",
    "S1_CUT = 0.5\n",
    "S2_CUT = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "RANDOM_STATE = 1337\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 20\n",
    "\n",
    "# Loss weights\n",
    "LAMBDA_DISCO = 5.0      # weight for decorrelating each score from MASS_COL on background\n",
    "LAMBDA_MUTUAL = 1.0     # (optional) weight to decorrelate s1 and s2 from each other\n",
    "\n",
    "# Backend: \"torch\" or \"qml\"\n",
    "BACKEND = \"torch\"\n",
    "\n",
    "# VQC (QML) specifics (used only if BACKEND==\"qml\")\n",
    "QUBITS = 6\n",
    "VQC_LAYERS = 2\n",
    "QDEVICE = \"default.qubit\"  # or \"lightning.qubit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86600a5f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data loading (Parquet/CSV/ROOT via `uproot`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb12731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def _ensure_cols(df, label_col, weight_col, mass_col):\n",
    "    if weight_col not in df.columns:\n",
    "        df[weight_col] = 1.0\n",
    "    missing = [c for c in [label_col, mass_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "def load_table(path: Path, features, label_col, weight_col, mass_col):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    if p.suffix == \".parquet\":\n",
    "        df = pd.read_parquet(p)\n",
    "    elif p.suffix == \".csv\":\n",
    "        df = pd.read_csv(p)\n",
    "    elif p.suffix == \".root\":\n",
    "        import uproot, awkward as ak\n",
    "        with uproot.open(p) as f:\n",
    "            # Adjust tree and branches for your NanoAOD\n",
    "            tree = f[\"Events\"] if \"Events\" in f else list(f.keys())[0]\n",
    "            arr = f[tree].arrays(features + [label_col, mass_col], library=\"ak\")\n",
    "            df = ak.to_dataframe(arr).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {p.suffix}\")\n",
    "    df = _ensure_cols(df, label_col, weight_col, mass_col)\n",
    "    keep = features + [label_col, weight_col, mass_col]\n",
    "    return df[keep].copy()\n",
    "\n",
    "train_df = load_table(DATA_PATH/TRAIN_FILE, FEATURES, LABEL_COL, WEIGHT_COL, MASS_COL)\n",
    "val_df   = load_table(DATA_PATH/VAL_FILE,   FEATURES, LABEL_COL, WEIGHT_COL, MASS_COL)\n",
    "test_df  = load_table(DATA_PATH/TEST_FILE,  FEATURES, LABEL_COL, WEIGHT_COL, MASS_COL)\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ede8",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Preprocessing & Torch datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features (fit on train background+signal together by default)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[FEATURES].values)\n",
    "\n",
    "def make_xyw(df):\n",
    "    X = scaler.transform(df[FEATURES].values).astype(\"float32\")\n",
    "    y = df[LABEL_COL].values.astype(\"float32\")\n",
    "    w = df[WEIGHT_COL].values.astype(\"float32\")\n",
    "    m = df[MASS_COL].values.astype(\"float32\")\n",
    "    return X, y, w, m\n",
    "\n",
    "Xtr, ytr, wtr, mtr = make_xyw(train_df)\n",
    "Xva, yva, wva, mva = make_xyw(val_df)\n",
    "Xte, yte, wte, mte = make_xyw(test_df)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, X, y, w, m):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.w = torch.from_numpy(w)\n",
    "        self.m = torch.from_numpy(m)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i], self.w[i], self.m[i]\n",
    "\n",
    "dset_tr = ArrayDataset(Xtr, ytr, wtr, mtr)\n",
    "dset_va = ArrayDataset(Xva, yva, wva, mva)\n",
    "dset_te = ArrayDataset(Xte, yte, wte, mte)\n",
    "\n",
    "dl_tr = DataLoader(dset_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dl_va = DataLoader(dset_va, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dl_te = DataLoader(dset_te, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864aa6e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) DisCo and Double‑DisCo losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b21533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _cdist_centered(x):\n",
    "    # pairwise Euclidean distance matrix, double-centered (Gower's centering)\n",
    "    # x: (N,1)\n",
    "    x = x.view(-1, 1)\n",
    "    d = torch.cdist(x, x, p=2)\n",
    "    n = d.size(0)\n",
    "    J = torch.eye(n, device=d.device) - (1.0/n) * torch.ones((n,n), device=d.device)\n",
    "    A = J @ d @ J\n",
    "    return A\n",
    "\n",
    "def distance_correlation(x, y, eps=1e-9):\n",
    "    \"\"\"\n",
    "    x, y: 1D tensors of same length (N,)\n",
    "    Returns sample distance correlation in [0,1].\n",
    "    \"\"\"\n",
    "    Ax = _cdist_centered(x)\n",
    "    Ay = _cdist_centered(y)\n",
    "    dcov2 = (Ax * Ay).mean()\n",
    "    dvarx = (Ax * Ax).mean().clamp_min(eps)\n",
    "    dvary = (Ay * Ay).mean().clamp_min(eps)\n",
    "    dcor = (dcov2.clamp_min(0.0) / torch.sqrt(dvarx * dvary + eps)).clamp(0.0, 1.0)\n",
    "    return dcor\n",
    "\n",
    "def disco_loss(score, mass, bkg_mask, lam=1.0):\n",
    "    \"\"\"\n",
    "    Penalize distance correlation between score and mass for background events only.\n",
    "    score: (N,) sigmoid output\n",
    "    mass:  (N,)\n",
    "    bkg_mask: boolean (N,) True where y==0\n",
    "    \"\"\"\n",
    "    if bkg_mask.sum() < 2:\n",
    "        return torch.tensor(0.0, device=score.device)\n",
    "    s = score[bkg_mask]\n",
    "    m = mass[bkg_mask]\n",
    "    return lam * distance_correlation(s, m)\n",
    "\n",
    "def double_disco_loss(s1, s2, mass, bkg_mask, lam_each=1.0, lam_mutual=0.0):\n",
    "    l = disco_loss(s1, mass, bkg_mask, lam_each) + disco_loss(s2, mass, bkg_mask, lam_each)\n",
    "    if lam_mutual > 0.0:\n",
    "        l = l + lam_mutual * distance_correlation(s1, s2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6bc86",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Models: Torch (two MLPs) and optional QML (two VQCs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP1D(nn.Module):\n",
    "    def __init__(self, d_in, hidden=(64,64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = d_in\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(0.1)]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # logits\n",
    "\n",
    "class TorchTwoNets(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net1 = MLP1D(d_in)\n",
    "        self.net2 = MLP1D(d_in)\n",
    "    def forward(self, x):\n",
    "        z1 = self.net1(x)\n",
    "        z2 = self.net2(x)\n",
    "        s1 = torch.sigmoid(z1)\n",
    "        s2 = torch.sigmoid(z2)\n",
    "        return z1, z2, s1, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67019781",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca06d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qml_two_nets(d_in, n_qubits=6, layers=2, device_name=\"default.qubit\"):\n",
    "    \"\"\"\n",
    "    Returns (model, info_dict). The model mimics TorchTwoNets's interface.\n",
    "    \"\"\"\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as pnp\n",
    "    import torch.nn as nn\n",
    "    import torch, math\n",
    "\n",
    "    if n_qubits < d_in:\n",
    "        # encode via random linear projection if features > qubits\n",
    "        proj = torch.randn(d_in, n_qubits) / math.sqrt(d_in)\n",
    "    else:\n",
    "        proj = None\n",
    "\n",
    "    dev1 = qml.device(device_name, wires=n_qubits)\n",
    "    dev2 = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "    def feature_map(x, wires):\n",
    "        # simple angle encoding\n",
    "        for i, w in enumerate(wires):\n",
    "            qml.RX(x[i], wires=w)\n",
    "\n",
    "    def ansatz(params, wires):\n",
    "        for _ in range(layers):\n",
    "            for w in wires:\n",
    "                qml.RY(params[0, w], wires=w)\n",
    "            for i in range(len(wires)-1):\n",
    "                qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "        # final single-qubit rotations\n",
    "        for w in wires:\n",
    "            qml.RZ(params[1, w], wires=w)\n",
    "\n",
    "    @qml.qnode(dev1, interface=\"torch\")\n",
    "    def circuit1(x, params):\n",
    "        feature_map(x, range(n_qubits))\n",
    "        ansatz(params, range(n_qubits))\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    @qml.qnode(dev2, interface=\"torch\")\n",
    "    def circuit2(x, params):\n",
    "        feature_map(x, range(n_qubits))\n",
    "        ansatz(params, range(n_qubits))\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    class QTwoNets(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.params1 = nn.Parameter(torch.randn(2, n_qubits)*0.01)\n",
    "            self.params2 = nn.Parameter(torch.randn(2, n_qubits)*0.01)\n",
    "        def forward(self, x):\n",
    "            # project/trim features to n_qubits\n",
    "            if proj is not None:\n",
    "                xproj = x @ proj\n",
    "            else:\n",
    "                xproj = x[:, :n_qubits]\n",
    "            # normalize angles\n",
    "            xang = torch.tanh(xproj)\n",
    "            z1 = circuit1(xang, self.params1).view(-1)\n",
    "            z2 = circuit2(xang, self.params2).view(-1)\n",
    "            # map expectation [-1,1] to sigmoid-like [0,1]\n",
    "            s1 = (z1 + 1)/2\n",
    "            s2 = (z2 + 1)/2\n",
    "            return z1, z2, s1, s2\n",
    "\n",
    "    return QTwoNets(), {\"n_qubits\": n_qubits, \"layers\": layers, \"device\": device_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02329cca",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Training loop (classification + Double‑DisCo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if BACKEND == \"qml\":\n",
    "    model, info = build_qml_two_nets(len(FEATURES), QUBITS, VQC_LAYERS, QDEVICE)\n",
    "else:\n",
    "    model = TorchTwoNets(len(FEATURES))\n",
    "\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "bce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")  # we'll apply sigmoid manually for logits\n",
    "\n",
    "def run_epoch(dloader, train=True):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    cls_loss_sum = 0.0\n",
    "    disco_sum = 0.0\n",
    "\n",
    "    for X, y, w, m in tqdm(dloader, leave=False):\n",
    "        X, y, w, m = X.to(device), y.to(device), w.to(device), m.to(device)\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "\n",
    "        z1, z2, s1, s2 = model(X)\n",
    "\n",
    "        # classification loss (two heads; average)\n",
    "        loss1 = (bce(z1, y) * w).mean()\n",
    "        loss2 = (bce(z2, y) * w).mean()\n",
    "        cls_loss = 0.5 * (loss1 + loss2)\n",
    "\n",
    "        # decorrelate on *background* (y==0) only\n",
    "        bmask = (y < 0.5)\n",
    "        dloss = double_disco_loss(s1.detach() if BACKEND==\"qml\" else s1,  # safe for QNodes\n",
    "                                  s2.detach() if BACKEND==\"qml\" else s2,\n",
    "                                  m, bmask,\n",
    "                                  lam_each=LAMBDA_DISCO,\n",
    "                                  lam_mutual=LAMBDA_MUTUAL)\n",
    "\n",
    "        loss = cls_loss + dloss\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        total += float(loss.detach().cpu())\n",
    "        cls_loss_sum += float(cls_loss.detach().cpu())\n",
    "        disco_sum += float(dloss.detach().cpu())\n",
    "\n",
    "    n = len(dloader)\n",
    "    return {\"loss\": total/n, \"cls\": cls_loss_sum/n, \"disco\": disco_sum/n}\n",
    "\n",
    "hist = {\"train\": [], \"val\": []}\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = run_epoch(dl_tr, train=True)\n",
    "    va = run_epoch(dl_va, train=False)\n",
    "    hist[\"train\"].append(tr); hist[\"val\"].append(va)\n",
    "    print(f\"Epoch {ep:02d} | train: {tr} | val: {va}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17086859",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Evaluation: independence checks and ABCD closure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def scores_on(dl):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, w, m in dl:\n",
    "            X = X.to(device)\n",
    "            z1, z2, s1, s2 = model(X)\n",
    "            outs.append((s1.cpu().numpy(), s2.cpu().numpy(), y.numpy(), w.numpy(), m.numpy()))\n",
    "    s1 = np.concatenate([o[0] for o in outs])\n",
    "    s2 = np.concatenate([o[1] for o in outs])\n",
    "    y  = np.concatenate([o[2] for o in outs])\n",
    "    w  = np.concatenate([o[3] for o in outs])\n",
    "    m  = np.concatenate([o[4] for o in outs])\n",
    "    return s1, s2, y, w, m\n",
    "\n",
    "s1, s2, y, w, m = scores_on(dl_te)\n",
    "\n",
    "# distance correlations\n",
    "with torch.no_grad():\n",
    "    t_s1 = torch.from_numpy(s1.astype(\"float32\"))\n",
    "    t_s2 = torch.from_numpy(s2.astype(\"float32\"))\n",
    "    t_m  = torch.from_numpy(m.astype(\"float32\"))\n",
    "    bmask = torch.from_numpy((y<0.5).astype(bool))\n",
    "    d_s1_m = float(distance_correlation(t_s1[bmask], t_m[bmask]))\n",
    "    d_s2_m = float(distance_correlation(t_s2[bmask], t_m[bmask]))\n",
    "    d_s1_s2 = float(distance_correlation(t_s1, t_s2))\n",
    "\n",
    "print({\"dCor(s1,m)_bkg\": d_s1_m, \"dCor(s2,m)_bkg\": d_s2_m, \"dCor(s1,s2)\": d_s1_s2})\n",
    "\n",
    "# 2D hist for ABCD\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "hb = plt.hist2d(s1[y<0.5], s2[y<0.5], bins=40)  # background only\n",
    "plt.xlabel(\"s1\"); plt.ylabel(\"s2\"); plt.title(\"Background scores\")\n",
    "plt.colorbar(); plt.show()\n",
    "\n",
    "# ABCD counts on background\n",
    "def abcd_counts(s1, s2, y, w, c1=S1_CUT, c2=S2_CUT):\n",
    "    b = (y<0.5)\n",
    "    A = (s1<c1) & (s2<c2) & b\n",
    "    B = (s1>=c1) & (s2<c2) & b\n",
    "    C = (s1<c1) & (s2>=c2) & b\n",
    "    D = (s1>=c1) & (s2>=c2) & b\n",
    "    def wsum(mask): return float(w[mask].sum())\n",
    "    return wsum(A), wsum(B), wsum(C), wsum(D)\n",
    "\n",
    "A,B,C,D = abcd_counts(s1, s2, y, w)\n",
    "D_est = (B*C)/(A+1e-9)\n",
    "closure = D_est / (D+1e-9)\n",
    "print({\"A\":A, \"B\":B, \"C\":C, \"D_true\":D, \"D_est\":D_est, \"closure(D_est/D_true)\":closure})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969703bb",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Likelihood‑based ABCD (optional, needs `pyhf`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81184d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyhf\n",
    "\n",
    "    def make_abcd_model(A,B,C,D):\n",
    "        spec = {\n",
    "            \"channels\": [{\n",
    "                \"name\": \"abcd\",\n",
    "                \"samples\": [{\n",
    "                    \"name\": \"bkg\",\n",
    "                    \"modifiers\": [{\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}],\n",
    "                    \"data\": [A,B,C,D]\n",
    "                }],\n",
    "                \"inputs\": [{\"name\":\"bins\",\"data\":[A,B,C,D]}]\n",
    "            }]\n",
    "        }\n",
    "        return pyhf.Model(spec, poi_name=\"mu\")\n",
    "\n",
    "    model = make_abcd_model(A,B,C,D)\n",
    "    data = [A,B,C,D] + model.config.auxdata\n",
    "    result = pyhf.infer.mle.fit(data, model)\n",
    "    print(\"pyhf MLE (mu):\", result[model.config.poi_index])\n",
    "except Exception as e:\n",
    "    print(\"pyhf not available or failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadcdd9e",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Save scores & export models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadebc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append scores to the test dataframe and save\n",
    "out = test_df.copy()\n",
    "out[\"s1\"] = s1\n",
    "out[\"s2\"] = s2\n",
    "out.to_parquet(\"test_with_scores.parquet\")\n",
    "print(\"Wrote test_with_scores.parquet\")\n",
    "\n",
    "# Save Torch state dicts (works for both backends; for QML only the classical parameters are saved)\n",
    "torch.save(model.state_dict(), \"abcdisco_model.pt\")\n",
    "print(\"Saved abcdisco_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfba44",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Notes to reproduce paper examples\n",
    "\n",
    "- **3‑Gaussian toy:** generate three normal components and label one as “signal”; use two ML‑learned discriminants with DisCo to emulate Figure 2‑style closure plots.\n",
    "- **Boosted top (HLFs):** load the provided `topsample_*.dat.gz` or your own jet HLFs. Use soft‑drop mass (or \\(m_{SD}\\)) as `MASS_COL` for decorrelation, following the **Double‑DisCo** setup.\n",
    "- **Paired dijet (RPV SUSY‑like):** define signal proxies where the dijet mass peak appears; use total‑invariant-mass as `MASS_COL`.\n",
    "\n",
    "> Tip: The **thresholds** `(S1_CUT, S2_CUT)` can be scanned to pick the most stable closure; in real analyses you’d **lock them** before looking in data (or scan with toys and treat as a discrete nuisance).\n",
    "\n",
    "## 10) QML caveats & tips\n",
    "\n",
    "- Start with **few qubits** (4–8) and shallow depth; increase only if you see a learning plateau.\n",
    "- Always bench against the Torch NN with **identical inputs** and **identical ABCD metrics** (closure, contamination).\n",
    "- When training the VQC, batch sizes may need to be **smaller**; if gradients are noisy, try gradient‑accumulation or `optax`‑style optimizers via JAX backends in PennyLane.\n",
    "- For real hardware, restrict to **native gate sets** and transpile (e.g. to IBM basis) after you’re satisfied on simulators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}