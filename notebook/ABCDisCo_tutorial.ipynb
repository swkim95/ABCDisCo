{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546f6658",
   "metadata": {},
   "source": [
    "\n",
    "# ABCDisCo Double-DisCo Tutorial (Torch + optional PennyLane backend)\n",
    "\n",
    "This notebook reproduces the **Double-DisCo** training pipeline introduced in [T. Aarrestad *et al.*, *Eur. Phys. J. C* **81**, 1003 (2021), arXiv:2007.14400](https://arxiv.org/abs/2007.14400). It mirrors the reference scripts shipped with this repository and exposes the key stages one-by-one so you can validate the implementation, reproduce the published baselines, and swap the classical discriminant for a Quantum Machine Learning (QML) backend when desired.\n",
    "\n",
    "> **Mapping to repository scripts**\n",
    "> - Data ingestion and scaling follow `ABCD_topjets_HLF_DD.py` (lines 69-129) and `data_loader.py` (lines 1-63).\n",
    "> - Neural-network heads reuse `networks.DNNclassifier` (lines 8-44) while the DisCo penalty mirrors `model_ABCD_2NN.py` (lines 29-120) together with `disco.py` (lines 14-118).\n",
    "> - Evaluation routines adapt `evaluation.py` (lines 1-141) to produce the ABCD closure and JSD vs. background-rejection metrics discussed in the paper.\n",
    "\n",
    "The workflow is organised as:\n",
    "\n",
    "1. **Setup & configuration** (aligns with the hyperparameters used in the Double-DisCo study).\n",
    "2. **Data loading and preprocessing** (min-max scaling, label/weight bookkeeping).\n",
    "3. **Model definition** with interchangeable Torch/QML heads.\n",
    "4. **Training** with the DisCo decorrelation penalty.\n",
    "5. **Diagnostics & evaluation**: ROC curves, distance correlations, ABCD closure plots, JSD vs. background rejection.\n",
    "6. **Export** of trained weights and inference scores.\n",
    "\n",
    "> **Datasets**: The repository already ships reduced CMS top-tagging high-level feature (HLF) samples (`topsample_*_tau.dat.gz`) suitable for this tutorial. You can run the notebook end-to-end without any external downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c9f73",
   "metadata": {},
   "source": [
    "\n",
    "## Environment preparation\n",
    "\n",
    "Run the following cell *once per environment* if you still need to install the CPU builds of PyTorch, PennyLane, and analysis utilities. Keep it commented in committed versions to avoid accidental re-installs on shared clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment the lines you need)\n",
    "# %pip install numpy pandas scikit-learn matplotlib tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install pennylane pennylane-lightning\n",
    "# %pip install pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c0205",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration (mirrors `ABCD_topjets_HLF_DD.py` lines 69-129)\n",
    "\n",
    "We replicate the preprocessing hyperparameters used in the Double-DisCo reference run and expose additional knobs to make rapid CPU tests feasible. Set `FULL_DATASET = True` for a faithful reproduction of the paper-level event counts (requires substantial CPU/GPU time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a29f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Repository-local modules (mirror original scripts)\n",
    "from disco import distance_corr_unbiased\n",
    "from networks import DNNclassifier\n",
    "\n",
    "try:\n",
    "    import pennylane as qml  # optional backend\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "except ImportError:  # keep optional\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-talk\")\n",
    "\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_ROOT = Path('.')\n",
    "RAW_FILES = {\n",
    "    \"train\": DATA_ROOT / \"topsample_train_tau.dat.gz\",\n",
    "    \"val\": DATA_ROOT / \"topsample_val_tau.dat.gz\",\n",
    "    \"test\": DATA_ROOT / \"topsample_test_tau.dat.gz\",\n",
    "}\n",
    "\n",
    "# Toggle to match the full training statistics used in the paper.\n",
    "FULL_DATASET = False\n",
    "EVENT_LIMITS = {\n",
    "    \"train\": 50000 if not FULL_DATASET else None,\n",
    "    \"val\": 50000 if not FULL_DATASET else None,\n",
    "    \"test\": 50000 if not FULL_DATASET else None,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 25 if not FULL_DATASET else 200\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# DisCo regularisation weights (cf. model_ABCD_2NN.py lines 29-78)\n",
    "LAMBDA_MUTUAL = 50.0   # decorrelate the two Double-DisCo scores on background\n",
    "LAMBDA_MASS = 5.0      # optional: decorrelate each score from jet mass (set to 0.0 to disable)\n",
    "\n",
    "# ABCD region definitions used for closure checks\n",
    "S1_CUT = 0.5\n",
    "S2_CUT = 0.5\n",
    "\n",
    "# QML backend knobs (used only when BACKEND == \"qml\")\n",
    "BACKEND = \"torch\"  # switch to \"qml\" after installing PennyLane\n",
    "N_QUBITS = 6\n",
    "QML_LAYERS = 2\n",
    "QML_DEVICE = \"default.qubit\"\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"mass\",\n",
    "    \"pt\",\n",
    "    \"tau1_half\",\n",
    "    \"tau2_half\",\n",
    "    \"tau3_half\",\n",
    "    \"tau1\",\n",
    "    \"tau2\",\n",
    "    \"tau3\",\n",
    "    \"tau4\",\n",
    "    \"tau1_sq\",\n",
    "    \"tau2_sq\",\n",
    "    \"tau3_sq\",\n",
    "    \"tau4_sq\",\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a24f2",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data loading & scaling (`ABCD_topjets_HLF_DD.py` lines 69-87)\n",
    "\n",
    "The original script concatenates train/validation/test files, performs a global min-max scaling on all high-level features, and appends bookkeeping columns for labels, event weights, bin indices, and masses. We expose the same transformation, add integrity checks, and return a tidy `pandas.DataFrame` for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _load_tau_file(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "    with gzip.open(path, 'rt') as handle:\n",
    "        array = np.loadtxt(handle, delimiter=',', skiprows=15)\n",
    "    if array.ndim != 2 or array.shape[1] != 14:\n",
    "        raise ValueError(f\"Unexpected shape {array.shape} in {path}\")\n",
    "    return array\n",
    "\n",
    "\n",
    "def load_and_scale(raw_files: Dict[str, Path]) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "    \"\"\"Replicate the min-max scaling used in `ABCD_topjets_HLF_DD.py`.\"\"\"\n",
    "    raw_arrays = {split: _load_tau_file(path) for split, path in raw_files.items()}\n",
    "\n",
    "    concatenated = np.vstack([arr[:, 1:] for arr in raw_arrays.values()])\n",
    "    feat_min = concatenated.min(axis=0)\n",
    "    feat_range = np.maximum(concatenated.max(axis=0) - feat_min, 1e-8)\n",
    "\n",
    "    features: Dict[str, np.ndarray] = {}\n",
    "    labels: Dict[str, np.ndarray] = {}\n",
    "    masses: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for split, array in raw_arrays.items():\n",
    "        scaled = (array[:, 1:] - feat_min) / feat_range\n",
    "        features[split] = scaled.astype(np.float32)\n",
    "        labels[split] = array[:, 0].astype(np.float32)\n",
    "        masses[split] = array[:, 1].astype(np.float32)\n",
    "\n",
    "    return features, labels, masses\n",
    "\n",
    "\n",
    "features, labels, masses = load_and_scale(RAW_FILES)\n",
    "\n",
    "summary = []\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    limit = EVENT_LIMITS[split]\n",
    "    n_events = features[split].shape[0] if limit is None else min(limit, features[split].shape[0])\n",
    "    summary.append({\n",
    "        \"split\": split,\n",
    "        \"events\": n_events,\n",
    "        \"signal_fraction\": float(labels[split][:n_events].mean()),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "display(summary_df)\n",
    "\n",
    "pd.DataFrame(features[\"train\"][:5], columns=FEATURE_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a83ee",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Torch datasets (`data_loader.py` lines 22-63)\n",
    "\n",
    "We reproduce the `TopTaggingDataset` logic with a CPU/GPU-agnostic implementation and keep the bookkeeping fields required by the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchTopTaggingDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, mass: np.ndarray, weight: np.ndarray | None = None):\n",
    "        self.x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "        if weight is None:\n",
    "            weight = np.ones_like(y, dtype=np.float32)\n",
    "        self.w = torch.as_tensor(weight, dtype=torch.float32)\n",
    "        self.mass = torch.as_tensor(mass, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx], self.w[idx], self.mass[idx]\n",
    "\n",
    "\n",
    "def _clip_events(split: str, array: np.ndarray) -> np.ndarray:\n",
    "    limit = EVENT_LIMITS[split]\n",
    "    if limit is None:\n",
    "        return array\n",
    "    return array[:limit]\n",
    "\n",
    "\n",
    "train_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"train\", features[\"train\"]),\n",
    "    _clip_events(\"train\", labels[\"train\"]),\n",
    "    _clip_events(\"train\", masses[\"train\"]),\n",
    ")\n",
    "val_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"val\", features[\"val\"]),\n",
    "    _clip_events(\"val\", labels[\"val\"]),\n",
    "    _clip_events(\"val\", masses[\"val\"]),\n",
    ")\n",
    "test_set = TorchTopTaggingDataset(\n",
    "    _clip_events(\"test\", features[\"test\"]),\n",
    "    _clip_events(\"test\", labels[\"test\"]),\n",
    "    _clip_events(\"test\", masses[\"test\"]),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_set), len(val_set), len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327abef1",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model backends (`networks.py` lines 8-78)\n",
    "\n",
    "We expose two interchangeable heads:\n",
    "\n",
    "- `TorchDoubleDisco` embeds the exact `DNNclassifier` definition from the reference scripts and is the default.\n",
    "- `PennyLaneDoubleDisco` provides a minimal variational quantum circuit (VQC) realised with `qml.qnn.TorchLayer`, making it straightforward to prototype QML extensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchDoubleDisco(nn.Module):\n",
    "    \"\"\"Wrap two `DNNclassifier` heads exactly as in the Double-DisCo scripts.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.head1 = DNNclassifier(n_features, 2)\n",
    "        self.head2 = DNNclassifier(n_features, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        logits1 = self.head1(x)\n",
    "        logits2 = self.head2(x)\n",
    "        score1 = F.softmax(logits1, dim=1)[:, 1]\n",
    "        score2 = F.softmax(logits2, dim=1)[:, 1]\n",
    "        return logits1, logits2, score1, score2\n",
    "\n",
    "\n",
    "class PennyLaneDoubleDisco(nn.Module):\n",
    "    \"\"\"Minimal PennyLane VQC head compatible with the Double-DisCo loss.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int, n_qubits: int = 6, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "        if not PENNYLANE_AVAILABLE:\n",
    "            raise RuntimeError(\"PennyLane is not installed. Run `%pip install pennylane pennylane-lightning`.\")\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "\n",
    "        qdevice = qml.device(device_name, wires=n_qubits)\n",
    "        weight_shapes = {\"weights\": (layers, n_qubits)}\n",
    "\n",
    "        @qml.qnode(qdevice, interface=\"torch\")\n",
    "        def circuit(inputs, weights):\n",
    "            x_pad = torch.zeros(n_qubits, dtype=inputs.dtype, device=inputs.device)\n",
    "            take = min(inputs.shape[-1], n_qubits)\n",
    "            x_pad[:take] = inputs[..., :take]\n",
    "            qml.templates.AngleEmbedding(x_pad, wires=range(n_qubits), rotation=\"Y\")\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(2)]\n",
    "\n",
    "        self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "        self.head1 = nn.Linear(2, 2)\n",
    "        self.head2 = nn.Linear(2, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        q_inputs = x[:, : self.n_qubits]\n",
    "        q_features = self.qlayer(q_inputs)\n",
    "        logits1 = self.head1(q_features)\n",
    "        logits2 = self.head2(q_features)\n",
    "        score1 = F.softmax(logits1, dim=1)[:, 1]\n",
    "        score2 = F.softmax(logits2, dim=1)[:, 1]\n",
    "        return logits1, logits2, score1, score2\n",
    "\n",
    "\n",
    "def build_model(n_features: int) -> nn.Module:\n",
    "    if BACKEND == \"qml\":\n",
    "        model = PennyLaneDoubleDisco(n_features, n_qubits=N_QUBITS, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "    else:\n",
    "        model = TorchDoubleDisco(n_features)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "model = build_model(len(FEATURE_NAMES))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7a8ba",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Loss function with DisCo penalty (`model_ABCD_2NN.py` lines 29-78 & `disco.py`)\n",
    "\n",
    "We compute weighted binary cross-entropy for each head and add:\n",
    "\n",
    "1. A **mutual decorrelation term** between the two Double-DisCo scores on background events.\n",
    "2. An optional **mass decorrelation** term per head (toggle via `LAMBDA_MASS`).\n",
    "\n",
    "The distance-correlation implementation is imported directly from `disco.py`, ensuring parity with the reference code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance_corr_safe(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
    "    if x.numel() <= 2 or y.numel() <= 2:\n",
    "        return torch.zeros(1, device=x.device, dtype=x.dtype)\n",
    "    normed = weight / (weight.sum() + 1e-12) * len(weight)\n",
    "    return distance_corr_unbiased(x, y, normed, power=1)\n",
    "\n",
    "\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    weights = weights.to(DEVICE)\n",
    "    masses = masses.to(DEVICE)\n",
    "\n",
    "    logits1, logits2, score1, score2 = model(features)\n",
    "\n",
    "    loss_cls1 = F.binary_cross_entropy(score1, labels, weight=weights)\n",
    "    loss_cls2 = F.binary_cross_entropy(score2, labels, weight=weights)\n",
    "    loss = loss_cls1 + loss_cls2\n",
    "\n",
    "    metrics = {\n",
    "        \"loss_cls1\": float(loss_cls1.detach().cpu()),\n",
    "        \"loss_cls2\": float(loss_cls2.detach().cpu()),\n",
    "    }\n",
    "\n",
    "    background = labels < 0.5\n",
    "    if background.any():\n",
    "        w_bkg = weights[background]\n",
    "        s1_bkg = score1[background]\n",
    "        s2_bkg = score2[background]\n",
    "        m_bkg = masses[background]\n",
    "\n",
    "        if LAMBDA_MUTUAL > 0.0:\n",
    "            d_mutual = distance_corr_safe(s1_bkg, s2_bkg, torch.ones_like(w_bkg))\n",
    "            loss = loss + LAMBDA_MUTUAL * d_mutual\n",
    "            metrics[\"dCorr_s1_s2\"] = float(d_mutual.detach().cpu())\n",
    "\n",
    "        if LAMBDA_MASS > 0.0:\n",
    "            d_mass1 = distance_corr_safe(s1_bkg, m_bkg, torch.ones_like(w_bkg))\n",
    "            d_mass2 = distance_corr_safe(s2_bkg, m_bkg, torch.ones_like(w_bkg))\n",
    "            loss = loss + LAMBDA_MASS * (d_mass1 + d_mass2)\n",
    "            metrics[\"dCorr_s1_m\"] = float(d_mass1.detach().cpu())\n",
    "            metrics[\"dCorr_s2_m\"] = float(d_mass2.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007b2c5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training loop (`model_ABCD_2NN.py` lines 80-208)\n",
    "\n",
    "We adapt the original `train`/`val` helpers to work seamlessly on CPU or GPU, collect metrics per epoch, and stop only after the requested number of epochs (no early stopping for clarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76370118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, list[float]] = {}\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    return {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    scores1, scores2, labels_all, weights_all, masses_all = [], [], [], [], []\n",
    "    agg: Dict[str, list[float]] = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"eval\"):\n",
    "            _, metrics = compute_losses(model, batch)\n",
    "            features, labels, weights, masses = batch\n",
    "            features = features.to(DEVICE)\n",
    "            _, _, score1, score2 = model(features)\n",
    "            scores1.append(score1.cpu().numpy())\n",
    "            scores2.append(score2.cpu().numpy())\n",
    "            labels_all.append(labels.numpy())\n",
    "            weights_all.append(weights.numpy())\n",
    "            masses_all.append(masses.numpy())\n",
    "            for key, value in metrics.items():\n",
    "                agg.setdefault(key, []).append(value)\n",
    "    scores1 = np.concatenate(scores1)\n",
    "    scores2 = np.concatenate(scores2)\n",
    "    labels_all = np.concatenate(labels_all)\n",
    "    weights_all = np.concatenate(weights_all)\n",
    "    masses_all = np.concatenate(masses_all)\n",
    "    metrics_mean = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    return scores1, scores2, labels_all, weights_all, masses_all, metrics_mean\n",
    "\n",
    "\n",
    "history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer)\n",
    "    s1_val, s2_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "    fpr, tpr, _ = roc_curve(y_val, s1_val, sample_weight=w_val)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    record = {\n",
    "        \"epoch\": epoch,\n",
    "        \"roc_auc_s1\": roc_auc,\n",
    "        **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
    "        **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
    "    }\n",
    "    history.append(record)\n",
    "    print(f\"Epoch {epoch:03d} | AUC(s1)={roc_auc:.3f} | train_loss={train_metrics['loss_cls1']+train_metrics['loss_cls2']:.3f}\")\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f85b",
   "metadata": {},
   "source": [
    "\n",
    "### Training diagnostics\n",
    "\n",
    "We track the classification losses and distance-correlation penalties to verify convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls1\"] + history_df[\"train_loss_cls2\"], label=\"train\")\n",
    "ax[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls1\"] + history_df[\"val_loss_cls2\"], label=\"val\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Binary cross-entropy\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Classification loss\")\n",
    "\n",
    "if \"train_dCorr_s1_s2\" in history_df:\n",
    "    ax[1].plot(history_df[\"epoch\"], history_df[\"train_dCorr_s1_s2\"], label=\"train\")\n",
    "    ax[1].plot(history_df[\"epoch\"], history_df[\"val_dCorr_s1_s2\"], label=\"val\")\n",
    "    ax[1].set_ylabel(\"Distance correlation\")\n",
    "    ax[1].set_title(\"Mutual decorrelation (background)\")\n",
    "    ax[1].legend()\n",
    "else:\n",
    "    ax[1].axis('off')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63000124",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluation (`evaluation.py` lines 1-141)\n",
    "\n",
    "We reproduce the ABCD diagnostics: ROC curves, 2D background score maps, closure ratios, and Jensen-Shannon divergence (JSD) versus background rejection. These metrics match the ones plotted in Fig. 4 of the ABCDisCo paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b12e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluation import JSD, JSDvsR\n",
    "\n",
    "\n",
    "def collect_scores(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, weights, masses in loader:\n",
    "            features = features.to(DEVICE)\n",
    "            _, _, score1, score2 = model(features)\n",
    "            out.append((score1.cpu().numpy(), score2.cpu().numpy(), labels.numpy(), weights.numpy(), masses.numpy()))\n",
    "    s1 = np.concatenate([o[0] for o in out])\n",
    "    s2 = np.concatenate([o[1] for o in out])\n",
    "    y = np.concatenate([o[2] for o in out])\n",
    "    w = np.concatenate([o[3] for o in out])\n",
    "    m = np.concatenate([o[4] for o in out])\n",
    "    return s1, s2, y, w, m\n",
    "\n",
    "\n",
    "def abcd_counts(s1, s2, y, w, c1=S1_CUT, c2=S2_CUT):\n",
    "    mask_b = y < 0.5\n",
    "    A = (s1 < c1) & (s2 < c2) & mask_b\n",
    "    B = (s1 >= c1) & (s2 < c2) & mask_b\n",
    "    C = (s1 < c1) & (s2 >= c2) & mask_b\n",
    "    D = (s1 >= c1) & (s2 >= c2) & mask_b\n",
    "\n",
    "    def wsum(mask):\n",
    "        return float(w[mask].sum())\n",
    "\n",
    "    return wsum(A), wsum(B), wsum(C), wsum(D)\n",
    "\n",
    "\n",
    "def scan_abcd(s1, s2, y, w, grid=np.linspace(0.2, 0.95, 20)):\n",
    "    closure = []\n",
    "    rejection = []\n",
    "    for c1 in grid:\n",
    "        for c2 in grid:\n",
    "            A, B, C, D = abcd_counts(s1, s2, y, w, c1, c2)\n",
    "            if A > 0 and B > 0 and C > 0 and D > 0:\n",
    "                D_pred = (B * C) / A\n",
    "                closure.append(D_pred / D)\n",
    "                rejection.append(1.0 / (B / (A + B + C + D)))\n",
    "    return np.array(rejection), np.array(closure)\n",
    "\n",
    "\n",
    "s1_te, s2_te, y_te, w_te, m_te = collect_scores(model, test_loader)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_te, s1_te, sample_weight=w_te)\n",
    "ax[0].plot(fpr, tpr, label=f\"AUC={auc(fpr, tpr):.3f}\")\n",
    "ax[0].plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "ax[0].set_xlabel(\"False positive rate\")\n",
    "ax[0].set_ylabel(\"True positive rate\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"ROC (score 1)\")\n",
    "\n",
    "hb = ax[1].hist2d(s1_te[y_te < 0.5], s2_te[y_te < 0.5], bins=40, cmap=\"viridis\")\n",
    "ax[1].set_xlabel(\"s1 (background)\")\n",
    "ax[1].set_ylabel(\"s2 (background)\")\n",
    "ax[1].set_title(\"Background score map\")\n",
    "fig.colorbar(hb[3], ax=ax[1])\n",
    "\n",
    "rejection, closure = scan_abcd(s1_te, s2_te, y_te, w_te)\n",
    "ax[2].scatter(rejection, closure, s=12, alpha=0.6)\n",
    "ax[2].axhline(1.0, color='k', linestyle='--')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_xlabel(\"Background rejection (1/epsilon_B)\")\n",
    "ax[2].set_ylabel(\"Closure: D_pred / D_true\")\n",
    "ax[2].set_title(\"ABCD closure scan\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "sig_mask = y_te > 0.5\n",
    "bg_mask = y_te < 0.5\n",
    "metrics = {\n",
    "    eff: JSDvsR(\n",
    "        sigscore=s1_te[sig_mask],\n",
    "        bgscore=s1_te[bg_mask],\n",
    "        bgmass=s2_te[bg_mask],\n",
    "        sigweights=w_te[sig_mask],\n",
    "        bgweights=w_te[bg_mask],\n",
    "        sigeff=eff,\n",
    "        minmass=0,\n",
    "        maxmass=1,\n",
    "        nbins=40,\n",
    "    )\n",
    "    for eff in (10, 30, 50)\n",
    "}\n",
    "print(\"JSD vs R metrics:\", json.dumps(metrics, indent=2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    s1_t = torch.as_tensor(s1_te, dtype=torch.float32)\n",
    "    s2_t = torch.as_tensor(s2_te, dtype=torch.float32)\n",
    "    m_t = torch.as_tensor(m_te, dtype=torch.float32)\n",
    "    bmask = torch.as_tensor(bg_mask)\n",
    "    d_s1_m = float(distance_corr_safe(s1_t[bmask], m_t[bmask], torch.ones_like(s1_t[bmask])))\n",
    "    d_s2_m = float(distance_corr_safe(s2_t[bmask], m_t[bmask], torch.ones_like(s2_t[bmask])))\n",
    "    d_s1_s2 = float(distance_corr_safe(s1_t[bmask], s2_t[bmask], torch.ones_like(s1_t[bmask])))\n",
    "\n",
    "print({\n",
    "    \"dCorr(s1, mass | bkg)\": d_s1_m,\n",
    "    \"dCorr(s2, mass | bkg)\": d_s2_m,\n",
    "    \"dCorr(s1, s2 | bkg)\": d_s1_s2,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc154998",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Persist artefacts\n",
    "\n",
    "Save inference scores and the trained model weights so downstream scripts (`evaluation.py`, likelihood fits with `pyhf`, etc.) can be run without re-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"s1\": s1_te,\n",
    "    \"s2\": s2_te,\n",
    "    \"label\": y_te,\n",
    "    \"weight\": w_te,\n",
    "    \"mass\": m_te,\n",
    "})\n",
    "results_df.to_parquet(\"abcdisco_double_disco_scores.parquet\", index=False)\n",
    "torch.save(model.state_dict(), \"abcdisco_double_disco_model.pt\")\n",
    "print(\"Saved abcdisco_double_disco_scores.parquet and abcdisco_double_disco_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e4cf6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Extending to full ABCDisCo & QML studies\n",
    "\n",
    "- **Recovering the paper-level numbers**: set `FULL_DATASET = True`, increase `EPOCHS` to 200, and tune `LAMBDA_MUTUAL` in the range `[50, 200]` as scanned in `ABCD_topjets_HLF_DD.py` (lines 103-129).\n",
    "- **QML experiments**: install PennyLane, switch `BACKEND = \"qml\"`, and adjust `N_QUBITS` / `QML_LAYERS`. The loss and evaluation cells remain unchanged because they operate on the abstract interface shared by both backends.\n",
    "- **Likelihood fits**: export the `results_df` Parquet file and feed it into the `pyhf`-based closure or limit-setting workflows described in Section 4 of the paper.\n",
    "\n",
    "> For deeper context, revisit the original ABCDisCo publication (T. Aarrestad *et al.*, *Eur. Phys. J. C* **81**, 1003 (2021), arXiv:2007.14400) and the DisCo decorrelation proposal (M. D. Andrews *et al.*, *Phys. Rev. D* **101**, 094004 (2020), arXiv:1905.08628).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
