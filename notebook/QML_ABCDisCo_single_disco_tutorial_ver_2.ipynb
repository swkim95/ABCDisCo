{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3873f1a8",
   "metadata": {},
   "source": [
    "\n",
    "# ABCDisCo Single-DisCo Tutorial (Torch + optional PennyLane backend)\n",
    "\n",
    "This notebook reproduces the **Single-DisCo** workflow described in [T. Aarrestad *et al.*, *Eur. Phys. J. C* **81**, 1003 (2021), arXiv:2007.14400](https://arxiv.org/abs/2007.14400). It mirrors the reference scripts shipped with this repository so you can validate the mass-decorrelated baseline before moving to the Double-DisCo configuration.\n",
    "\n",
    "> **Mapping to repository scripts**\n",
    "> - Data ingestion and scaling follow `ABCD_topjets_HLF_mD.py` (lines 69-101) together with the dataset helpers in `data_loader.py` (lines 1-63).\n",
    "> - The neural-network head reuses `networks.DNNclassifier` (lines 8-44), while the DisCo penalty mirrors `model.py` (lines 24-86) plus `disco.py` (lines 14-118).\n",
    "> - Evaluation adapts the single-score diagnostics from `evaluation.py` (lines 1-70), including the Jensen-Shannon divergence vs. background rejection scan.\n",
    "\n",
    "The workflow is organised as:\n",
    "\n",
    "1. **Setup & configuration** (Single-DisCo hyperparameters).\n",
    "2. **Data loading and preprocessing** (min-max scaling, feature selection matching `ABCD_topjets_HLF_mD.py`).\n",
    "3. **Model definition** with interchangeable Torch/PennyLane heads.\n",
    "4. **Training** with the DisCo mass decorrelation penalty.\n",
    "5. **Diagnostics & evaluation**: ROC curves, distance correlations, JSD vs. background rejection, and mass sculpting checks.\n",
    "6. **Export** of trained weights and inference scores.\n",
    "\n",
    "> **Datasets**: The repository already ships reduced CMS top-tagging HLF samples (`topsample_*_tau.dat.gz`). You can run this notebook end-to-end without external downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015d9e7",
   "metadata": {},
   "source": [
    "\n",
    "## Environment preparation\n",
    "\n",
    "Run the following cell *once per environment* if you still need to install the CPU builds of PyTorch, PennyLane, and the lightweight analysis stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6df378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment the lines you need)\n",
    "# %pip install numpy pandas scikit-learn matplotlib tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install pennylane pennylane-lightning\n",
    "# %pip install pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe8e3b",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration (mirrors `ABCD_topjets_HLF_mD.py` lines 69-126)\n",
    "\n",
    "We keep the dataset limits, optimiser choices, and DisCo penalty normalisation consistent with the single-network script so that this notebook can reproduce the published baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa3f01a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Standard-library utilities used throughout the tutorial notebook\n",
    "# ---------------------------------------------------------------\n",
    "import gzip  # Read the compressed CMS top-tagging datasets that ship with the repo\n",
    "import json  # Serialize metadata for the checkpoints that we write every epoch\n",
    "import time  # Measure wall-clock runtime so we can report epoch durations\n",
    "from datetime import datetime  # Timestamp saved checkpoints for reproducibility\n",
    "from pathlib import Path  # Work with filesystem paths in a platform-agnostic way\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Scientific Python stack: numerical arrays, data frames, and plots\n",
    "# ---------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PyTorch: tensor library used for both classical and quantum models\n",
    "# ---------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Scikit-Learn: classical metrics used to interpret classifier quality\n",
    "# ---------------------------------------------------------------\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Utilities for nicely formatted progress bars inside notebooks\n",
    "# ---------------------------------------------------------------\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Repository-local modules (they mirror the original training scripts)\n",
    "# ---------------------------------------------------------------\n",
    "import sys\n",
    "project_root = Path.cwd().resolve().parent  # notebook/ -> project root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "from disco import distance_corr_unbiased  # distance correlation regulariser\n",
    "from networks import DNNclassifier  # baseline dense neural network classifier\n",
    "from evaluation import JSDvsR  # Jensen-Shannon vs. rejection diagnostic from the paper\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Optional PennyLane backend for the hybrid quantum notebook\n",
    "# (we keep the import guarded so the classical notebook works without it)\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    raise ImportError(\"PennyLane is required for the QML tutorial. Please run `%pip install pennylane pennylane-lightning`.\")\n",
    "# ---------------------------------------------------------------\n",
    "# Plot styling: use a high-contrast theme that works well in dark/light modes\n",
    "# ---------------------------------------------------------------\n",
    "plt.style.use(\"seaborn-v0_8-talk\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Reproducibility knobs: numpy and torch share a common seed for this tutorial\n",
    "# ---------------------------------------------------------------\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Dataset locations.  We keep everything relative to the project root so the\n",
    "# notebook can be executed from any environment (local, remote, or batch jobs).\n",
    "# ---------------------------------------------------------------\n",
    "DATA_ROOT = project_root\n",
    "RAW_FILES = {\n",
    "    \"train\": DATA_ROOT / \"topsample_train_tau.dat.gz\",\n",
    "    \"val\": DATA_ROOT / \"topsample_val_tau.dat.gz\",\n",
    "    \"test\": DATA_ROOT / \"topsample_test_tau.dat.gz\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Training hyper-parameters.  Flip FULL_DATASET to True for the 5M-event dataset\n",
    "# from the ABCDisCo paper; by default we use a smaller subset for quick iteration.\n",
    "# ---------------------------------------------------------------\n",
    "FULL_DATASET = False\n",
    "EVENT_LIMITS = {\n",
    "    \"train\": 500000 if not FULL_DATASET else None,\n",
    "    \"val\": 250000 if not FULL_DATASET else None,\n",
    "    \"test\": 25000 if not FULL_DATASET else None,\n",
    "}\n",
    "BATCH_SIZE = 2048  # we reduce the batch size slightly to control circuit execution cost\n",
    "EPOCHS = 50 if not FULL_DATASET else 200\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DisCo hyper-parameters.  LAMBDA_MASS rescales the distance-correlation penalty\n",
    "# that encourages the classifier score to be independent of jet mass for background.\n",
    "# ---------------------------------------------------------------\n",
    "# Lambda ramping configuration for better QML training\n",
    "LAMBDA_MASS_START = 5.0      # Start with no penalty\n",
    "LAMBDA_MASS_END = 55.0      # End with full penalty\n",
    "LAMBDA_RAMP_EPOCHS = 50      # Ramp up over first 20 epochs\n",
    "LAMBDA_MASS = 5.0            # Current lambda (will be updated during training)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Evaluation settings for ABCD-related diagnostics.\n",
    "# ---------------------------------------------------------------\n",
    "SCORE_SIGNAL_EFFICIENCIES = (0.1, 0.3, 0.6)  # working points used in the paper\n",
    "ABCD_SIGNAL_WINDOW_QUANTILES = (0.3, 0.7)  # central mass window defines the SR\n",
    "ABCD_HISTOGRAM_BINS = 40  # consistent with the JSD vs. R plots in the paper\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Filesystem layout for per-epoch checkpoints.\n",
    "# ---------------------------------------------------------------\n",
    "CHECKPOINT_DIR = Path.cwd() / \"checkpoints_qml_single_disco\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_TEMPLATE = \"epoch_{epoch:03d}.pth\"\n",
    "RESUME_CHECKPOINT: Optional[Path] = None # set to a file in CHECKPOINT_DIR to resume\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Device selection: automatically use CUDA when available, otherwise fall back\n",
    "# to CPU so the notebook works on any machine (including lightweight VMs).\n",
    "# ---------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# In the QML notebook we default to the quantum backend.\n",
    "BACKEND = \"qml\"\n",
    "N_QUBITS = 11\n",
    "QML_LAYERS = 3\n",
    "QAOA_DEPTH = 1\n",
    "QML_DEVICE = \"default.qubit\"\n",
    "# QML_DEVICE = \"lightning.gpu\"\n",
    "# QML_DEVICE = \"lightning.qubit\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Feature bookkeeping.  The Single-DisCo configuration removes the leading two\n",
    "# observables (mass and pT) from the classifier input, while still keeping them\n",
    "# available for ABCD evaluation and decorrelation studies.\n",
    "# ---------------------------------------------------------------\n",
    "ORIGINAL_FEATURES = [\n",
    "    \"mass\",\n",
    "    \"pt\",\n",
    "    \"tau1_half\",\n",
    "    \"tau2_half\",\n",
    "    \"tau3_half\",\n",
    "    \"tau1\",\n",
    "    \"tau2\",\n",
    "    \"tau3\",\n",
    "    \"tau4\",\n",
    "    \"tau1_sq\",\n",
    "    \"tau2_sq\",\n",
    "    \"tau3_sq\",\n",
    "    \"tau4_sq\",\n",
    "]\n",
    "SINGLE_FEATURE_INDICES = list(range(2, len(ORIGINAL_FEATURES)))\n",
    "FEATURE_NAMES = [ORIGINAL_FEATURES[i] for i in SINGLE_FEATURE_INDICES]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb8ffe",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data loading & scaling (`ABCD_topjets_HLF_mD.py` lines 69-95)\n",
    "\n",
    "The original script concatenates train/validation/test splits, applies a global min-max scaling to all 13 high-level features, and then selects the 11 observables used for the single-network classifier. We reproduce that procedure verbatim while keeping the jet mass available for decorrelation diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6980f93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | events= 500000 | signal= 249495 | background= 250505\n",
      "  val | events= 250000 | signal= 124860 | background= 125140\n",
      " test | events=  25000 | signal=  12482 | background=  12518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data-loading helpers: convert the gzipped CMS top-tagging tables into numpy arrays.\n",
    "# Objective: provide structured access to features, event weights, and jet mass\n",
    "#            values that feed the quantum-classical hybrid network.\n",
    "# Logic:    1) `_load_tau_file` opens the compressed .dat.gz file, filters out\n",
    "#              comment/header lines (including stray tokens like 'hello1'),\n",
    "#              accepts both comma- and whitespace-separated numbers, and builds\n",
    "#              a dense float matrix.\n",
    "#           2) `_split_features_labels` supports both historical 14-column files\n",
    "#              (label + 13 observables without weights) and 16+ column variants\n",
    "#              (13 observables + label + weight). It returns features, labels,\n",
    "#              mass, and per-event weights.\n",
    "#           3) `load_split` wraps these helpers for the train/val/test datasets\n",
    "#              so downstream training code can simply fetch numpy arrays.\n",
    "# Terms:    * Tau file — CMS open data sample storing per-jet observables.\n",
    "#           * Event weight — multiplicative factor from the simulation that\n",
    "#             allows us to emulate the luminosity-scaled yields discussed in\n",
    "#             the ABCDisCo reference analysis.\n",
    "# Expected behaviour: each helper returns numpy arrays with shapes consistent\n",
    "#                     with the raw dataset; any missing file or malformed array\n",
    "#                     surfaces a descriptive exception so users notice it early.\n",
    "# Reference: see the \"Dataset preparation\" discussion in\n",
    "#            `ref/Evaluating_ABCDisCo_Structured_WithEquations.md`, which stresses\n",
    "#            the importance of keeping mass and weights alongside the classifier\n",
    "#            inputs for later closure checks.\n",
    "# ---------------------------------------------------------------------------\n",
    "def _load_tau_file(path: Path) -> np.ndarray:\n",
    "    # Verify that the requested gzipped text file exists before attempting I/O.\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "    rows: List[np.ndarray] = []\n",
    "    with gzip.open(path, \"rt\") as handle:\n",
    "        for raw in handle:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "            if line.startswith(\"#\"):\n",
    "                continue  # skip comment blocks\n",
    "            # Skip any header or malformed line that contains alphabetic characters (e.g., 'hello1', column labels)\n",
    "            if any(ch.isalpha() for ch in line):\n",
    "                continue\n",
    "            # Accept both comma- and whitespace-separated numeric fields\n",
    "            clean = line.replace(\",\", \" \")\n",
    "            values = np.fromstring(clean, sep=\" \")\n",
    "            if values.size == 0:\n",
    "                continue\n",
    "            rows.append(values)\n",
    "    if not rows:\n",
    "        raise ValueError(f\"No numeric data found in {path}\")\n",
    "    # Ensure all rows share the same width\n",
    "    widths = {arr.shape[0] for arr in rows}\n",
    "    if len(widths) != 1:\n",
    "        raise ValueError(f\"Inconsistent column counts in {path}: {sorted(widths)}\")\n",
    "    data = np.vstack(rows)\n",
    "    # All of the ABCDisCo notebooks assume the data are shaped as [events, features].\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(f\"Expected a 2D array, received shape {data.shape}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# `_split_features_labels` extracts physics-relevant slices from the raw matrix.\n",
    "# Supports both 14-column (label + 13 observables) and >=16-column (13 observables\n",
    "# + label + weight) layouts encountered in public samples.\n",
    "# Returning the mass column is essential for the ABCD mass-window definition,\n",
    "# and weights allow closure metrics to stay consistent with Eq. (1) of the\n",
    "# reference document.\n",
    "# ---------------------------------------------------------------------------\n",
    "def _split_features_labels(matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    ncols = int(matrix.shape[1])\n",
    "    if ncols == 14:\n",
    "        # Layout: [label, mass, pt, tau1_half, ..., tau4_sq] (no explicit weights)\n",
    "        labels = matrix[:, 0]\n",
    "        obs = matrix[:, 1:14]  # 13 physics observables starting at mass\n",
    "        mass = obs[:, 0]\n",
    "        weights = np.ones_like(labels, dtype=float)\n",
    "        features = obs[:, SINGLE_FEATURE_INDICES]  # select 11 inputs used by Single-DisCo\n",
    "    elif ncols >= 16:\n",
    "        # Layout: [mass, pt, tau1_half, ..., tau4_sq, ..., label, weight]\n",
    "        obs = matrix[:, :13]\n",
    "        mass = obs[:, 0]\n",
    "        labels = matrix[:, 14]\n",
    "        weights = matrix[:, 15]\n",
    "        features = obs[:, SINGLE_FEATURE_INDICES]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unexpected dataset shape. Expected 14 columns (label+13) or at least 16 columns; \"\n",
    "            f\"received {ncols}.\"\n",
    "        )\n",
    "    return features.astype(np.float64), labels.astype(np.float64), mass.astype(np.float64), weights.astype(np.float64)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# `load_split` orchestrates reading and slicing for a named split (train/val/test).\n",
    "# By keeping the function pure (it only returns numpy arrays), we make it easy\n",
    "# to run unit tests or to plug in alternative datasets in the future.\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_split(name: str, limit: Optional[int] = None) -> Dict[str, np.ndarray]:\n",
    "    matrix = _load_tau_file(RAW_FILES[name])\n",
    "    if limit is not None:\n",
    "        matrix = matrix[:limit]\n",
    "    x, y, mass, weight = _split_features_labels(matrix)\n",
    "    return {\"x\": x, \"y\": y, \"mass\": mass, \"weight\": weight}\n",
    "\n",
    "\n",
    "# Materialise the three splits so that subsequent cells can form PyTorch datasets.\n",
    "# Each entry stores feature matrices, labels, weights, and jet masses for later\n",
    "# use in the ABCD validation routines.\n",
    "arrays = {\n",
    "    split: load_split(split, limit=EVENT_LIMITS[split])\n",
    "    for split in (\"train\", \"val\", \"test\")\n",
    "}\n",
    "for split, payload in arrays.items():\n",
    "    print(\n",
    "        f\"{split:>5} | events={len(payload['x']):>7} | \"\n",
    "        f\"signal={int(payload['y'].sum()):>7} | background={len(payload['y']) - int(payload['y'].sum()):>7}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b29a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature names (used as model inputs): ['tau1_half', 'tau2_half', 'tau3_half', 'tau1', 'tau2', 'tau3', 'tau4', 'tau1_sq', 'tau2_sq', 'tau3_sq', 'tau4_sq']\n",
      "\n",
      "=== TRAIN split ===\n",
      "shapes: x=(500000, 11), y=(500000,), mass=(500000,), weight=(500000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 250505, 1: 249495}\n",
      "feature mins (first 5): ['tau1_half=0.0000', 'tau2_half=0.0000', 'tau3_half=0.0000', 'tau1=0.0000', 'tau2=0.0000']\n",
      "feature maxs (first 5): ['tau1_half=0.7749', 'tau2_half=0.6208', 'tau3_half=0.4756', 'tau1=0.5610', 'tau2=0.3926']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.423769   0.326077   0.282639 0.231036 0.146164 0.101567 0.070804 0.078382 0.037967 0.018720 0.013638\n",
      "  0.343756   0.242735   0.172155 0.199839 0.108218 0.054387 0.037446 0.079494 0.030268 0.007936 0.006857\n",
      "  0.204169   0.130401   0.095653 0.118397 0.056607 0.024931 0.021626 0.054000 0.021543 0.003879 0.003435\n",
      "  0.436715   0.383325   0.267754 0.231975 0.189181 0.108591 0.071846 0.077295 0.048466 0.021949 0.009287\n",
      "  0.213526   0.170961   0.121988 0.090731 0.049261 0.031562 0.027488 0.036367 0.007769 0.005939 0.004858\n",
      "first 5 labels: [1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "first 5 masses: [183.404, 173.192, 138.193, 180.074, 121.001]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "=== VAL split ===\n",
      "shapes: x=(250000, 11), y=(250000,), mass=(250000,), weight=(250000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 125140, 1: 124860}\n",
      "feature mins (first 5): ['tau1_half=0.0000', 'tau2_half=0.0000', 'tau3_half=0.0000', 'tau1=0.0000', 'tau2=0.0000']\n",
      "feature maxs (first 5): ['tau1_half=0.7646', 'tau2_half=0.6510', 'tau3_half=0.4877', 'tau1=0.5575', 'tau2=0.4303']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.449352   0.229435   0.189656 0.252044 0.107713 0.064737 0.055593 0.081900 0.040113 0.013861 0.008919\n",
      "  0.343737   0.235097   0.208293 0.161765 0.091364 0.065135 0.050414 0.050339 0.026594 0.011937 0.010116\n",
      "  0.316310   0.204865   0.144719 0.204157 0.100509 0.044291 0.037784 0.098287 0.031123 0.006230 0.004675\n",
      "  0.368823   0.238256   0.199253 0.203579 0.079368 0.053136 0.044292 0.070371 0.013379 0.006421 0.005552\n",
      "  0.438729   0.303152   0.265241 0.230442 0.129690 0.096681 0.075900 0.083797 0.027414 0.013236 0.010616\n",
      "first 5 labels: [1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "first 5 masses: [165.078, 126.317, 185.131, 153.091, 186.336]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "=== TEST split ===\n",
      "shapes: x=(25000, 11), y=(25000,), mass=(25000,), weight=(25000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 12518, 1: 12482}\n",
      "feature mins (first 5): ['tau1_half=0.0113', 'tau2_half=0.0030', 'tau3_half=0.0014', 'tau1=0.0052', 'tau2=0.0013']\n",
      "feature maxs (first 5): ['tau1_half=0.7357', 'tau2_half=0.5621', 'tau3_half=0.4206', 'tau1=0.5485', 'tau2=0.3592']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.150711   0.094834   0.065195 0.037402 0.029972 0.022437 0.016085 0.010507 0.009762 0.004641 0.004510\n",
      "  0.274026   0.195765   0.175141 0.159612 0.085998 0.066269 0.054613 0.072109 0.027105 0.014830 0.011357\n",
      "  0.074004   0.056060   0.049406 0.030062 0.019415 0.015636 0.012881 0.008528 0.005426 0.003370 0.003038\n",
      "  0.458081   0.301564   0.212132 0.243207 0.123812 0.064642 0.032140 0.075759 0.023313 0.006972 0.003676\n",
      "  0.503659   0.291573   0.206024 0.315395 0.135215 0.079933 0.051821 0.130667 0.038438 0.023160 0.007166\n",
      "first 5 labels: [0.0, 1.0, 0.0, 1.0, 1.0]\n",
      "first 5 masses: [64.4549, 153.212, 59.3849, 153.555, 235.774]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Quick sanity checks for data loading: shapes, dtypes, label counts,\n",
    "# per-feature ranges, and first few rows for each split.\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Selected feature names (used as model inputs):\", FEATURE_NAMES)\n",
    "\n",
    "from typing import Dict as _Dict\n",
    "\n",
    "def _summarize_split(name: str, payload: _Dict[str, np.ndarray], feature_names: list[str]) -> None:\n",
    "    x = payload[\"x\"]; y = payload[\"y\"]; mass = payload[\"mass\"]; weight = payload[\"weight\"]\n",
    "    print(f\"\\n=== {name.upper()} split ===\")\n",
    "    print(f\"shapes: x={x.shape}, y={y.shape}, mass={mass.shape}, weight={weight.shape}\")\n",
    "    print(f\"dtypes: x={x.dtype}, y={y.dtype}, mass={mass.dtype}, weight={weight.dtype}\")\n",
    "    uniq, cnt = np.unique(y, return_counts=True)\n",
    "    print(\"label counts:\", {int(u): int(c) for u, c in zip(uniq, cnt)})\n",
    "    # Feature-wise min/max (only print first 5 to keep output compact)\n",
    "    mins = np.min(x, axis=0); maxs = np.max(x, axis=0)\n",
    "    k = min(5, x.shape[1])\n",
    "    print(\"feature mins (first 5):\", [f\"{feature_names[i]}={mins[i]:.4f}\" for i in range(k)])\n",
    "    print(\"feature maxs (first 5):\", [f\"{feature_names[i]}={maxs[i]:.4f}\" for i in range(k)])\n",
    "    # First few rows\n",
    "    df_preview = pd.DataFrame(x[:5], columns=feature_names)\n",
    "    print(\"first 5 feature rows:\")\n",
    "    print(df_preview.to_string(index=False))\n",
    "    print(\"first 5 labels:\", y[:5].tolist())\n",
    "    print(\"first 5 masses:\", mass[:5].tolist())\n",
    "    print(\"first 5 weights:\", weight[:5].tolist())\n",
    "\n",
    "for split_name in (\"train\", \"val\", \"test\"):\n",
    "    _summarize_split(split_name, arrays[split_name], FEATURE_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53adbcb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Torch datasets (`data_loader.py` lines 22-63)\n",
    "\n",
    "We wrap the min-max scaled arrays into PyTorch `Dataset` objects that expose the classifier inputs, labels, per-event weights, and jet masses used in the DisCo penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f014a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: train=245, val=123, test=13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# TorchTopTaggingDataset wraps the numpy arrays into a PyTorch-friendly object.\n",
    "# Objective: expose features, labels, sample weights, and jet masses as tensors\n",
    "#            so mini-batches can flow seamlessly into either the classical or\n",
    "#            quantum components of the hybrid network.\n",
    "# Logic:   * During __init__, we convert numpy arrays to torch.Tensor with\n",
    "#            explicit dtype casting so gradients and GPU transfers behave well.\n",
    "#          * __len__ returns the total number of events, informing DataLoader\n",
    "#            how many batches to draw per epoch.\n",
    "#          * __getitem__ selects one event and returns all four components in\n",
    "#            the order expected by downstream training code.\n",
    "# Terms:   - Sample weight: scales the contribution of each event to averages\n",
    "#            and is crucial for the weighted metrics used in the ABCDisCo\n",
    "#            evaluation workflow.\n",
    "# Expected behaviour: indexing yields a tuple (features, label, weight, mass)\n",
    "#                     where the features live in feature space R^{n_features}.\n",
    "# Reference: the need to pass masses and weights together is highlighted in the\n",
    "#            \"ABCD closure tests\" section of the evaluation document so the\n",
    "#            closure ratio can be recomputed at any stage of training.\n",
    "# ---------------------------------------------------------------------------\n",
    "class TorchTopTaggingDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, mass: np.ndarray, weight: np.ndarray | None = None):\n",
    "        # Convert all inputs to float32 tensors. This matches the precision\n",
    "        # expected by PyTorch optimisers and PennyLane's Torch interface.\n",
    "        self.x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "        self.mass = torch.as_tensor(mass, dtype=torch.float32)\n",
    "        if weight is None:\n",
    "            weight = np.ones_like(y, dtype=np.float32)\n",
    "        self.weight = torch.as_tensor(weight, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Return how many events are available; DataLoader uses this to know\n",
    "        # when an epoch finishes.\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Provide the tuple consumed by the training loop: (features, label,\n",
    "        # per-event weight, jet mass).\n",
    "        return self.x[index], self.y[index], self.weight[index], self.mass[index]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# make_dataloaders creates PyTorch DataLoader objects for train/val/test splits.\n",
    "# Objective: batch and optionally shuffle the dataset while keeping track of\n",
    "#            weights and masses required for ABCD evaluation and DisCo penalty.\n",
    "# Logic:   - Construct a TorchTopTaggingDataset for each split.\n",
    "#          - Instantiate DataLoaders with deterministic batch size and shuffling\n",
    "#            only on the training set (validation/test are left ordered so\n",
    "#            metrics such as ROC curves can be reproduced exactly).\n",
    "# Expected behaviour: downstream code can iterate over each loader and receive\n",
    "#                     batches shaped as `[batch, n_features]` plus aligned labels,\n",
    "#                     weights, and masses.\n",
    "# Reference: consistent batching is necessary to evaluate throughput statistics\n",
    "#            and closure metrics per epoch, as described in the runtime and\n",
    "#            ABCD sections of the evaluation guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def make_dataloaders(arrays: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, DataLoader]:\n",
    "    datasets = {\n",
    "        split: TorchTopTaggingDataset(payload[\"x\"], payload[\"y\"], payload[\"mass\"], payload[\"weight\"])\n",
    "        for split, payload in arrays.items()\n",
    "    }\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True, drop_last=False),\n",
    "        \"val\": DataLoader(datasets[\"val\"], batch_size=BATCH_SIZE, shuffle=False, drop_last=False),\n",
    "        \"test\": DataLoader(datasets[\"test\"], batch_size=BATCH_SIZE, shuffle=False, drop_last=False),\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "# Instantiate loaders so later cells can immediately iterate through the data.\n",
    "loaders = make_dataloaders(arrays)\n",
    "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"val\"], loaders[\"test\"]\n",
    "print(f\"Batches per epoch: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22662386",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model backends (`networks.py` lines 8-44)\n",
    "\n",
    "The Single-DisCo setup uses a single `DNNclassifier` head. We also expose an optional PennyLane quantum layer to demonstrate how the architecture can be swapped for a variational quantum classifier without changing the loss logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb324d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PennyLaneSingleDisco(\n",
       "  (compressor): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=32, out_features=11, bias=True)\n",
       "  )\n",
       "  (qlayer): <Quantum Torch Layer: func=circuit>\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=16, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Network definitions: classical head and quantum-enhanced Single DisCo model.\n",
    "# Objective: reproduce the architecture from the ABCDisCo paper while allowing\n",
    "#            PennyLane to supply a variational quantum circuit (VQC) as the\n",
    "#            feature extractor when BACKEND=\"qml\".\n",
    "# Reference: the \"Model introspection\" and \"Quantum resources\" discussions in\n",
    "#            `ref/Evaluating_ABCDisCo_Structured_WithEquations.md` motivate\n",
    "#            documenting layer counts, parameter totals, and how the circuit\n",
    "#            interfaces with classical post-processing.\n",
    "# ---------------------------------------------------------------------------\n",
    "class TorchSingleDisco(nn.Module):\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        # `DNNclassifier` mirrors the fully connected stack used in the\n",
    "        # classical Single-DisCo baseline (ReLU activations, dropout, etc.).\n",
    "        self.head = DNNclassifier(n_features, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # The head returns logits for two classes; we convert them to a\n",
    "        # background-suppression score via softmax on the signal column.\n",
    "        logits = self.head(x)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "\n",
    "class PennyLaneSingleDisco(nn.Module):\n",
    "    def __init__(self, n_features: int, n_qubits: int = 6, qaoa_depth: int = 1, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "        if not PENNYLANE_AVAILABLE:\n",
    "            raise RuntimeError(\"PennyLane is not installed.\")\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Classical compressor (encoder): single linear map R^{n_features} -> R^{n_qubits}\n",
    "        # followed by LeakyReLU to produce rotation angles for the quantum embedding.\n",
    "        self.compressor = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, n_qubits)\n",
    "        )\n",
    "        \n",
    "        # PennyLane device hosts the variational circuit; default.qubit is a\n",
    "        # noiseless statevector simulator suitable for long training jobs.\n",
    "        qdevice = qml.device(device_name, wires=n_qubits)\n",
    "        \n",
    "        # Strongly entangling layers only (no QAOA weights needed with AngleEmbedding)\n",
    "        weight_shapes = {\n",
    "            \"weights_strong\": (layers, n_qubits, 3),\n",
    "        }\n",
    "\n",
    "        @qml.qnode(qdevice, interface=\"torch\")\n",
    "        def circuit(inputs, weights_strong):\n",
    "            # -----------------------------------------------------------------\n",
    "            # Input handling: support both single-example (1D) and batched (>=2D)\n",
    "            # inputs. We right-pad/truncate to `n_qubits` along the last axis so\n",
    "            # embedding sees tensors shaped (..., n_qubits).\n",
    "            # -----------------------------------------------------------------\n",
    "            take = min(inputs.shape[-1], n_qubits)\n",
    "            if inputs.ndim == 1:\n",
    "                x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "                x_pad[:take] = inputs[:take]\n",
    "            else:\n",
    "                pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "                x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "                x_pad[..., :take] = inputs[..., :take]\n",
    "            \n",
    "            # Scale inputs to appropriate angle range for better gradient flow\n",
    "            angles = torch.pi * x_pad\n",
    "            \n",
    "            # AngleEmbedding encodes classical features via Y-rotations; more robust\n",
    "            # than QAOA for classification tasks and better gradient properties.\n",
    "            qml.templates.AngleEmbedding(angles, wires=range(n_qubits), rotation='Y')\n",
    "            qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "            \n",
    "            # Measure the Pauli-Z expectation on multiple qubits to obtain a multi-dimensional\n",
    "            # feature vector. These observables are the quantum analogue of learned features\n",
    "            # feeding the classical softmax head.\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]  # Use all qubits\n",
    "\n",
    "        # TorchLayer wraps the QNode so gradients propagate with PyTorch autograd.\n",
    "        self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "        \n",
    "        # A lightweight linear classifier turns the quantum features into\n",
    "        # logits for the binary task.\n",
    "        # self.head = nn.Linear(2, 2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Hybrid mapping: compress all n_features -> n_qubits via a small FCN\n",
    "        # to avoid truncation while keeping quantum input width fixed.\n",
    "        angles = self.compressor(x)\n",
    "        q_features = self.qlayer(angles)\n",
    "        logits = self.head(q_features)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "# HERE: deprecated QAOA version\n",
    "# class PennyLaneSingleDisco(nn.Module):\n",
    "#     def __init__(self, n_features: int, n_qubits: int = 6, qaoa_depth: int = 1, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "#         if not PENNYLANE_AVAILABLE:\n",
    "#             raise RuntimeError(\"PennyLane is not installed.\")\n",
    "#         super().__init__()\n",
    "#         self.n_qubits = n_qubits\n",
    "#         self.n_features = n_features\n",
    "#         # Classical compressor (encoder): single linear map R^{n_features} -> R^{n_qubits}\n",
    "#         # followed by ReLU to produce rotation angles for the quantum embedding.\n",
    "#         # self.compressor = nn.Sequential(\n",
    "#         #     nn.Linear(n_features, n_qubits),\n",
    "#         #     nn.LeakyReLU(),\n",
    "#         # )\n",
    "#         # PennyLane device hosts the variational circuit; default.qubit is a\n",
    "#         # noiseless statevector simulator suitable for long training jobs.\n",
    "#         qdevice = qml.device(device_name, wires=n_qubits)\n",
    "#         # One QAOA embedding layer + `layers` strongly entangling layers.\n",
    "#         weight_shapes = {\n",
    "#             \"weights_qaoa\": (qaoa_depth, 2 * n_qubits),\n",
    "#             \"weights_strong\": (layers, n_qubits, 3),\n",
    "#         }\n",
    "\n",
    "#         @qml.qnode(qdevice, interface=\"torch\")\n",
    "#         def circuit(inputs, weights_qaoa, weights_strong):\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # Input handling: support both single-example (1D) and batched (>=2D)\n",
    "#             # inputs. We right-pad/truncate to `n_qubits` along the last axis so\n",
    "#             # embedding sees tensors shaped (..., n_qubits).\n",
    "#             # -----------------------------------------------------------------\n",
    "#             take = min(inputs.shape[-1], n_qubits)\n",
    "#             if inputs.ndim == 1:\n",
    "#                 x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "#                 x_pad[:take] = inputs[:take]\n",
    "#             else:\n",
    "#                 pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "#                 x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "#                 x_pad[..., :take] = inputs[..., :take]\n",
    "#             # QAOAEmbedding encodes classical features via cost layers; a single\n",
    "#             # layer is followed by multiple strongly entangling layers to improve\n",
    "#             # expressivity.\n",
    "#             qml.templates.QAOAEmbedding(x_pad, weights=weights_qaoa, wires=range(n_qubits))\n",
    "#             qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "#             # Measure the Pauli-Z expectation on two qubits to obtain a 2D\n",
    "#             # feature vector. These observables are the quantum analogue of\n",
    "#             # learned features feeding the classical softmax head.\n",
    "\n",
    "#             # return qml.expval(qml.PauliZ(0))\n",
    "#             return [qml.expval(qml.PauliZ(i)) for i in range(2)]\n",
    "\n",
    "#         # TorchLayer wraps the QNode so gradients propagate with PyTorch autograd.\n",
    "#         self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "#         # A lightweight linear classifier turns the 2D quantum features into\n",
    "#         # logits for the binary task.\n",
    "#         self.head = nn.Linear(2, 2)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         # Hybrid mapping: compress all n_features -> n_qubits via a small FCN\n",
    "#         # to avoid truncation while keeping quantum input width fixed.\n",
    "#         # angles = self.compressor(x)\n",
    "#         # q_features = self.qlayer(angles)\n",
    "#         q_features = self.qlayer(x)\n",
    "#         logits = self.head(q_features)\n",
    "#         score = F.softmax(logits, dim=1)[:, 1]\n",
    "#         # Apply sigmoid to convert the single quantum expectation value to a probability score\n",
    "#         # Since q_features is a scalar (single expectation value from qml.expval(qml.PauliZ(0))),\n",
    "#         # we use sigmoid instead of softmax to map the range [-1, 1] to [0, 1]\n",
    "#         # logits = 1.\n",
    "#         # score = torch.sigmoid(q_features)\n",
    "#         return logits, score\n",
    "\n",
    "\n",
    "def build_model(n_features: int) -> nn.Module:\n",
    "    \"\"\"Factory: chooses classical or quantum architecture based on BACKEND.\"\"\"\n",
    "    if BACKEND == \"qml\":\n",
    "        # Use 3 StronglyEntanglingLayers as requested.\n",
    "        model = PennyLaneSingleDisco(n_features, n_qubits=N_QUBITS, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "        # model = PennyLaneSingleDisco(n_features, n_qubits=N_QUBITS, qaoa_depth=QAOA_DEPTH, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "    else:\n",
    "        model = TorchSingleDisco(n_features)\n",
    "    # Expected behaviour: the returned module is already moved to the selected\n",
    "    # device (CPU or CUDA) so later training code can call `.to(DEVICE)` safely.\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "model = build_model(len(FEATURE_NAMES))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcc2abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Learning rate: 0.0005\n",
      "Total trainable parameters: 1072\n",
      "Quantum layer parameters: 99 (per layer = N/A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>count</th>\n",
       "      <th>trainable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>compressor.0.weight</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>compressor.0.bias</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>compressor.2.weight</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compressor.2.bias</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qlayer.weights_strong</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>head.0.weight</td>\n",
       "      <td>176</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>head.0.bias</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>head.2.weight</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>head.2.bias</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               parameter  count  trainable\n",
       "0    compressor.0.weight    352       True\n",
       "1      compressor.0.bias     32       True\n",
       "2    compressor.2.weight    352       True\n",
       "3      compressor.2.bias     11       True\n",
       "4  qlayer.weights_strong     99       True\n",
       "5          head.0.weight    176       True\n",
       "6            head.0.bias     16       True\n",
       "7          head.2.weight     32       True\n",
       "8            head.2.bias      2       True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# describe_qml_model summarises parameter counts and optimiser settings.\n",
    "# Objective: provide a transparent ledger of learnable parameters, matching the\n",
    "#            \"Model introspection\" requirement from the evaluation guide so we\n",
    "#            can compare classical vs quantum capacity.\n",
    "# Logic: iterate over named_parameters(), collect counts, and print optimiser\n",
    "#        metadata (learning rate, algorithm family). The returned DataFrame is\n",
    "#        easy to display in-tabular form for notebook readers.\n",
    "# Expected behaviour: calling this function prints human-readable statements and\n",
    "#                     returns a DataFrame where each row corresponds to one\n",
    "#                     parameter tensor.\n",
    "# Reference: the evaluation document stresses reporting trainable degrees of\n",
    "#            freedom when comparing architectures (see the \"Model details\"\n",
    "#            bullet list in `ref/Evaluating_ABCDisCo_Structured_WithEquations.md`).\n",
    "# ---------------------------------------------------------------------------\n",
    "def describe_qml_model(model: nn.Module, optimizer: torch.optim.Optimizer) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        count = int(param.numel())\n",
    "        trainable = param.requires_grad\n",
    "        total_params += count if trainable else 0\n",
    "        rows.append({\n",
    "            \"parameter\": name,\n",
    "            \"count\": count,\n",
    "            \"trainable\": trainable,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"Optimizer:\", optimizer.__class__.__name__)\n",
    "    print(\"Learning rate:\", optimizer.param_groups[0][\"lr\"])\n",
    "    print(\"Total trainable parameters:\", total_params)\n",
    "    if hasattr(model, \"qlayer\"):\n",
    "        q_params = sum(p.numel() for p in model.qlayer.parameters())\n",
    "        per_layer = model.qlayer.weights.shape[1] if hasattr(model.qlayer, \"weights\") else \"N/A\"\n",
    "        print(f\"Quantum layer parameters: {q_params} (per layer = {per_layer})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "qml_architecture_df = describe_qml_model(model, optimizer)\n",
    "qml_architecture_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c9d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational circuit for zero input:\n",
      "TorchLayer parameters not found for drawing.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualise the variational quantum circuit to inspect ansatz structure.\n",
    "# Reference: the evaluation guide recommends presenting circuit diagrams when\n",
    "# discussing quantum resources so collaborators can relate expressivity to\n",
    "# physics performance.\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    # Use PennyLane's ASCII drawer to render the circuit with the current\n",
    "    # trainable weights. Passing a zero vector illustrates the gate layout\n",
    "    # independent of specific input features.\n",
    "    probe = torch.zeros((model.n_qubits,), dtype=torch.float32)\n",
    "    drawer = qml.draw(model.qlayer.qnode)\n",
    "    print(\"Variational circuit for zero input:\")\n",
    "    # Collect TorchLayer parameters expected by the QNode\n",
    "    params = {name: p.detach() for name, p in model.qlayer.named_parameters()}\n",
    "    w_qaoa = params.get(\"weights_qaoa\")\n",
    "    w_strong = params.get(\"weights_strong\")\n",
    "    if w_qaoa is not None and w_strong is not None:\n",
    "        print(drawer(probe, w_qaoa, w_strong))\n",
    "    else:\n",
    "        print(\"TorchLayer parameters not found for drawing.\")\n",
    "else:\n",
    "    print(\"Quantum circuit visualisation is only available when BACKEND='qml'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6852f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to render circuit image with qml.draw_mpl: 'TorchLayer' object has no attribute 'weights'\n",
      "If this persists, ensure matplotlib is installed and update PennyLane.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3b. Image-based QML circuit visualization (PennyLane draw_mpl)\n",
    "# Objective: render the variational circuit as an image and display it inline.\n",
    "# Logic: create a zero-input probe, use current trainable weights, call\n",
    "#        qml.draw_mpl on the QNode inside the TorchLayer, and save a PNG copy.\n",
    "# Expected behaviour: show a figure in the notebook and write to disk for reuse.\n",
    "# ---------------------------------------------------------------------------\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        import matplotlib.pyplot as plt  # already imported globally; kept for clarity\n",
    "\n",
    "        # Build a simple probe input and move weights to CPU for drawing\n",
    "        probe = torch.zeros((model.n_qubits,), dtype=torch.float32)\n",
    "        weights_cpu = model.qlayer.weights.detach().cpu()\n",
    "\n",
    "        fig, ax = qml.draw_mpl(model.qlayer.qnode)(probe, weights_cpu)\n",
    "        display(fig)\n",
    "\n",
    "        vis_dir = CHECKPOINT_DIR / \"visualizations\"\n",
    "        vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = vis_dir / \"qml_circuit.png\"\n",
    "        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        print(f\"Saved circuit image to {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to render circuit image with qml.draw_mpl:\", e)\n",
    "        print(\"If this persists, ensure matplotlib is installed and update PennyLane.\")\n",
    "else:\n",
    "    print(\"Circuit image is available only when BACKEND='qml'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "111044ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# get_current_lambda computes the current lambda value based on epoch for ramping.\n",
    "# Objective: gradually increase the decorrelation penalty to help QML models learn\n",
    "#            classification first, then decorrelation.\n",
    "# Logic: linear ramp from LAMBDA_MASS_START to LAMBDA_MASS_END over LAMBDA_RAMP_EPOCHS.\n",
    "# Expected behaviour: returns current lambda value for the given epoch.\n",
    "# ---------------------------------------------------------------------------\n",
    "def get_current_lambda(epoch: int) -> float:\n",
    "    if epoch < LAMBDA_RAMP_EPOCHS:\n",
    "        # Linear ramp from start to end over ramp epochs\n",
    "        progress = epoch / LAMBDA_RAMP_EPOCHS\n",
    "        return LAMBDA_MASS_START + progress * (LAMBDA_MASS_END - LAMBDA_MASS_START)\n",
    "    else:\n",
    "        # Use full penalty after ramp period\n",
    "        return LAMBDA_MASS_END\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# distance_corr_safe computes the distance correlation (DisCo) penalty.\n",
    "# Objective: reproduce the decorrelation regulariser described in the ABCDisCo\n",
    "#            paper so the classifier score becomes independent of jet mass on\n",
    "#            background events, satisfying the ABCD assumption.\n",
    "# Logic: Guard against tiny batches by returning zero when fewer than three\n",
    "#        events are present, then normalise the provided weights so they sum to\n",
    "#        the batch size (matching the convention in `disco.distance_corr_unbiased`).\n",
    "# Terms: distance correlation measures any statistical dependence between two\n",
    "#        variables; a value near zero signals independence. This is the core of\n",
    "#        the \"Distance Correlation penalty\" highlighted in the evaluation guide.\n",
    "# Expected behaviour: returns a torch scalar suitable for backpropagation.\n",
    "# Reference: see the \"Distance correlation penalty\" subsection in\n",
    "#            `ref/Evaluating_ABCDisCo_Structured_WithEquations.md` where the\n",
    "#            DisCo loss is tied to improved ABCD closure.\n",
    "# ---------------------------------------------------------------------------\n",
    "def distance_corr_safe(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
    "    if x.numel() <= 2 or y.numel() <= 2:\n",
    "        return torch.zeros(1, device=x.device, dtype=x.dtype)\n",
    "    normed = weight / (weight.sum() + 1e-12) * len(weight)\n",
    "    return distance_corr_unbiased(x, y, normed, power=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_weighted_classification_stats summarises confusion-matrix entries.\n",
    "# Objective: obtain weighted counts of true/false positives/negatives so we can\n",
    "#            report accuracy, precision, recall, and F1 per epoch.\n",
    "# Logic: apply a threshold (default 0.5) to scores, multiply boolean masks by\n",
    "#        sample weights, and accumulate sums.\n",
    "# Terms: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean.\n",
    "# Expected behaviour: returns a dictionary of scalar floats ready for logging.\n",
    "# Reference: the evaluation guide explicitly asks for classification metrics\n",
    "#            alongside ABCD closure to interpret signal/background separation.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_weighted_classification_stats(labels: torch.Tensor, scores: torch.Tensor, weights: torch.Tensor, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    preds = (scores >= threshold).to(labels.dtype)\n",
    "    w = weights\n",
    "    tp = torch.sum(w * (preds == 1) * (labels == 1))\n",
    "    tn = torch.sum(w * (preds == 0) * (labels == 0))\n",
    "    fp = torch.sum(w * (preds == 1) * (labels == 0))\n",
    "    fn = torch.sum(w * (preds == 0) * (labels == 1))\n",
    "    total = tp + tn + fp + fn + 1e-12\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    return {\n",
    "        \"tp\": tp.item(),\n",
    "        \"tn\": tn.item(),\n",
    "        \"fp\": fp.item(),\n",
    "        \"fn\": fn.item(),\n",
    "        \"accuracy\": accuracy.item(),\n",
    "        \"precision\": precision.item(),\n",
    "        \"recall\": recall.item(),\n",
    "        \"f1\": f1.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_losses packages the BCE classification loss and optional DisCo penalty.\n",
    "# Objective: provide a single function for forward passes that also records the\n",
    "#            metrics needed for per-epoch logging and checkpoint metadata.\n",
    "# Logic: move tensors to DEVICE, obtain logits and score, compute weighted\n",
    "#        binary cross-entropy, then append decorrelation loss for background\n",
    "#        events when `LAMBDA_MASS` > 0.\n",
    "# Expected behaviour: returns a loss tensor for backpropagation and a metrics\n",
    "#                     dictionary containing loss and classification statistics.\n",
    "# Reference: ties together the classification quality and decorrelation control\n",
    "#            emphasised in Sections \"ROC/AUC\" and \"Distance correlation\" of the\n",
    "#            evaluation document.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    weights = weights.to(DEVICE)\n",
    "    masses = masses.to(DEVICE)\n",
    "\n",
    "    logits, score = model(features)\n",
    "    loss_cls = F.binary_cross_entropy(score, labels, weight=weights)\n",
    "    loss = loss_cls\n",
    "\n",
    "    stats = compute_weighted_classification_stats(labels, score, weights)\n",
    "    metrics = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(stats[\"accuracy\"]),\n",
    "        \"precision\": float(stats[\"precision\"]),\n",
    "        \"recall\": float(stats[\"recall\"]),\n",
    "        \"f1\": float(stats[\"f1\"]),\n",
    "    }\n",
    "\n",
    "    background = labels < 0.5\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd8058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# weighted_quantile finds score thresholds at fixed signal efficiencies.\n",
    "# Objective: implement quantile selection that respects per-event weights,\n",
    "#            matching the definition of efficiency in collider analyses.\n",
    "# Logic: sort scores, build a weighted cumulative distribution, and interpolate\n",
    "#        the desired quantile.\n",
    "# Terms: quantile `q` solves CDF(q) = quantile; weights rescale contributions.\n",
    "# Expected behaviour: returns a scalar threshold such that the weighted fraction\n",
    "#                     of signal events above the threshold equals the target.\n",
    "# Reference: required for the ABCD closure scans described in the evaluation\n",
    "#            guide when constructing score cuts at specified signal efficiencies.\n",
    "# ---------------------------------------------------------------------------\n",
    "def weighted_quantile(values: np.ndarray, quantile: float, sample_weight: np.ndarray) -> float:\n",
    "    order = np.argsort(values)\n",
    "    values = values[order]\n",
    "    weights = sample_weight[order]\n",
    "    cumulative = np.cumsum(weights) - 0.5 * weights\n",
    "    cumulative /= weights.sum()\n",
    "    return float(np.interp(quantile, cumulative, values))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_epoch_classification_metrics records weighted accuracy-style metrics.\n",
    "# Objective: mirror the confusion-matrix-based diagnostics emphasised in the\n",
    "#            evaluation document for understanding classification quality.\n",
    "# Logic: apply a 0.5 threshold, evaluate sklearn metrics with weights, and store\n",
    "#        the confusion matrix for later visualisation.\n",
    "# Expected behaviour: returns a dictionary with scalar metrics and a matrix.\n",
    "# Reference: see the \"Classification metrics\" subsection in the evaluation guide\n",
    "#            for why accuracy/precision/recall/F1 complement ROC analysis.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_epoch_classification_metrics(scores: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    preds = (scores >= 0.5).astype(int)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds, sample_weight=weights),\n",
    "        \"precision\": precision_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "    }\n",
    "    cm = confusion_matrix(labels, preds, sample_weight=weights)\n",
    "    metrics[\"confusion_matrix\"] = cm.astype(float)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_roc_diagnostics traces the ROC curve and background efficiency targets.\n",
    "# Objective: capture the full trade-off between signal efficiency and background\n",
    "#            rejection, including specific working points used later in ABCD and\n",
    "#            Jensen–Shannon analyses.\n",
    "# Logic: leverage sklearn.roc_curve with weights, compute area under the curve,\n",
    "#        and interpolate background efficiencies at user-requested signal effs.\n",
    "# Expected behaviour: dictionary containing arrays (fpr, tpr, thresholds) and\n",
    "#                     scalars (AUC, background efficiencies at target points).\n",
    "# Reference: the evaluation guide's ROC/AUC discussion and Fig. 3-style plots.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_roc_diagnostics(scores: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, sample_weight=weights)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    diagnostics = {\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": thresholds, \"auc\": roc_auc}\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        diagnostics[f\"background_eff_at_{int(target*100)}pct_sig\"] = float(np.interp(target, tpr, fpr))\n",
    "    return diagnostics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_abcd_statistics evaluates closure, prediction uncertainty, and pulls.\n",
    "# Objective: implement Eq. (1) and related diagnostics from the evaluation guide\n",
    "#            to validate the ABCD method using the model's score as the x-axis,\n",
    "#            and produce aggregated summaries for transfer factors, sideband\n",
    "#            stability, and Asimov significance.\n",
    "# Logic: define a signal mass window, split events into A/B/C/D based on score\n",
    "#        cuts and mass window membership, then compute predicted background,\n",
    "#        closure ratio, percentage error, statistical pull, transfer factors,\n",
    "#        sideband stability tests, and Asimov significance.\n",
    "# Terms: regions A/B/C/D follow the ABCD method; closure ratio = N_pred/N_true;\n",
    "#        pull = (observed - predicted) / sigma_combined.\n",
    "# Expected behaviour: returns a dictionary with the mass window, per-efficiency\n",
    "#                     summaries used for plots and checkpoint metadata, and an\n",
    "#                     `aggregated` block containing dataset-level statistics.\n",
    "# Reference: \"ABCD closure tests\" and \"Pull distributions\" sections in the guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_abcd_statistics(scores: np.ndarray, masses: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    signal_mask = labels > 0.5\n",
    "    background_mask = ~signal_mask\n",
    "    if signal_mask.sum() == 0 or background_mask.sum() == 0:\n",
    "        raise ValueError(\"Need both classes present to evaluate ABCD metrics\")\n",
    "    mass_low = np.quantile(masses[signal_mask], ABCD_SIGNAL_WINDOW_QUANTILES[0])\n",
    "    mass_high = np.quantile(masses[signal_mask], ABCD_SIGNAL_WINDOW_QUANTILES[1])\n",
    "    in_signal_mass = (masses >= mass_low) & (masses <= mass_high)\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    def weighted_sum(mask: np.ndarray) -> float:\n",
    "        return float(np.sum(weights[mask]))\n",
    "\n",
    "    def asimov_significance(signal_yield: float, background_yield: float) -> float:\n",
    "        if background_yield <= 0:\n",
    "            return float(\"nan\")\n",
    "        if signal_yield <= 0:\n",
    "            return 0.0\n",
    "        term = (signal_yield + background_yield) * np.log1p(signal_yield / (background_yield + eps)) - signal_yield\n",
    "        return float(np.sqrt(max(0.0, 2.0 * term)))\n",
    "\n",
    "    results: Dict[str, Any] = {\"mass_window\": (mass_low, mass_high), \"per_efficiency\": [], \"aggregated\": {}}\n",
    "\n",
    "    closures, pulls = [], []\n",
    "    tf_bd_values, tf_cd_values = [], []\n",
    "    sideband_ratio_values, sideband_stability_values = [], []\n",
    "    asimov_values = []\n",
    "\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        score_cut = weighted_quantile(scores[signal_mask], 1 - target, weights[signal_mask])\n",
    "        pass_score = scores >= score_cut\n",
    "        region_A = pass_score & in_signal_mass\n",
    "        region_B = pass_score & (~in_signal_mass)\n",
    "        region_C = (~pass_score) & in_signal_mass\n",
    "        region_D = (~pass_score) & (~in_signal_mass)\n",
    "\n",
    "        A_bg = weighted_sum(background_mask & region_A)\n",
    "        B_bg = weighted_sum(background_mask & region_B)\n",
    "        C_bg = weighted_sum(background_mask & region_C)\n",
    "        D_bg = weighted_sum(background_mask & region_D)\n",
    "        A_sig = weighted_sum(signal_mask & region_A)\n",
    "        observed_total = A_bg + A_sig\n",
    "\n",
    "        prediction = B_bg * C_bg / (D_bg + eps)\n",
    "        closure_ratio = prediction / (A_bg + eps)\n",
    "        sigma_pred = prediction * np.sqrt(1 / (B_bg + eps) + 1 / (C_bg + eps) + 1 / (D_bg + eps))\n",
    "        sigma_obs = np.sqrt(A_bg + eps)\n",
    "        pull = (A_bg - prediction) / np.sqrt(sigma_pred**2 + sigma_obs**2)\n",
    "\n",
    "        tf_bd = B_bg / (D_bg + eps)\n",
    "        tf_cd = C_bg / (D_bg + eps)\n",
    "        sideband_ratio = B_bg / (C_bg + eps)\n",
    "        sideband_stability = (B_bg - C_bg) / (B_bg + C_bg + eps)\n",
    "        asimov = asimov_significance(A_sig, prediction)\n",
    "\n",
    "        results[\"per_efficiency\"].append({\n",
    "            \"target_signal_efficiency\": target,\n",
    "            \"score_cut\": score_cut,\n",
    "            \"A_bg\": A_bg,\n",
    "            \"B_bg\": B_bg,\n",
    "            \"C_bg\": C_bg,\n",
    "            \"D_bg\": D_bg,\n",
    "            \"predicted_bg\": prediction,\n",
    "            \"closure_ratio\": closure_ratio,\n",
    "            \"closure_error_pct\": (closure_ratio - 1.0) * 100.0,\n",
    "            \"pull\": pull,\n",
    "            \"transfer_factor_B_over_D\": tf_bd,\n",
    "            \"transfer_factor_C_over_D\": tf_cd,\n",
    "            \"sideband_ratio_B_over_C\": sideband_ratio,\n",
    "            \"sideband_stability\": sideband_stability,\n",
    "            \"signal_in_A\": A_sig,\n",
    "            \"observed_total_in_A\": observed_total,\n",
    "            \"asimov_significance\": asimov,\n",
    "        })\n",
    "\n",
    "        closures.append(closure_ratio)\n",
    "        pulls.append(pull)\n",
    "        tf_bd_values.append(tf_bd)\n",
    "        tf_cd_values.append(tf_cd)\n",
    "        sideband_ratio_values.append(sideband_ratio)\n",
    "        sideband_stability_values.append(sideband_stability)\n",
    "        asimov_values.append(asimov)\n",
    "\n",
    "    def summarise(values: List[float]) -> Dict[str, float]:\n",
    "        if len(values) == 0:\n",
    "            return {\"mean\": float(\"nan\"), \"std\": float(\"nan\"), \"min\": float(\"nan\"), \"max\": float(\"nan\"), \"median\": float(\"nan\")}\n",
    "        arr = np.asarray(values, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr, ddof=0)),\n",
    "            \"min\": float(np.min(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "            \"median\": float(np.median(arr)),\n",
    "        }\n",
    "\n",
    "    pull_stats = summarise(pulls)\n",
    "    pull_stats[\"rms\"] = float(np.sqrt(np.mean(np.square(np.asarray(pulls, dtype=float))))) if pulls else float(\"nan\")\n",
    "\n",
    "    results[\"aggregated\"] = {\n",
    "        \"closure_ratio\": summarise(closures),\n",
    "        \"pull\": pull_stats,\n",
    "        \"transfer_factor_B_over_D\": summarise(tf_bd_values),\n",
    "        \"transfer_factor_C_over_D\": summarise(tf_cd_values),\n",
    "        \"sideband_ratio_B_over_C\": summarise(sideband_ratio_values),\n",
    "        \"sideband_stability\": summarise(sideband_stability_values),\n",
    "        \"asimov_significance\": summarise(asimov_values),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_jsd_summary interfaces with the existing JSD vs rejection utility.\n",
    "# Objective: monitor how well the classifier avoids mass sculpting by relating\n",
    "#            background rejection to the inverse Jensen–Shannon divergence.\n",
    "# Logic: for each target signal efficiency, call `evaluation.JSDvsR` (which wraps\n",
    "#        the physics-inspired histogram comparison) and store the rejection +\n",
    "#        inverse JSD pair.\n",
    "# Terms: Jensen–Shannon divergence measures shape agreement between mass\n",
    "#        distributions; high inverse JSD means better agreement (less sculpting).\n",
    "# Expected behaviour: list of dictionaries keyed by target efficiency.\n",
    "# Reference: \"Mass sculpting and information-theoretic metrics\" in the guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_jsd_summary(scores: np.ndarray, masses: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> List[Dict[str, float]]:\n",
    "    background = labels < 0.5\n",
    "    signal = labels > 0.5\n",
    "    summary = []\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        rejection, inv_jsd = JSDvsR(\n",
    "            sigscore=scores[signal],\n",
    "            bgscore=scores[background],\n",
    "            bgmass=masses[background],\n",
    "            sigweights=weights[signal],\n",
    "            bgweights=weights[background],\n",
    "            sigeff=int(target * 100),\n",
    "            nbins=ABCD_HISTOGRAM_BINS,\n",
    "            minmass=float(masses.min()),\n",
    "            maxmass=float(masses.max()),\n",
    "        )\n",
    "        summary.append({\n",
    "            \"target_signal_efficiency\": target,\n",
    "            \"background_rejection\": rejection,\n",
    "            \"inverse_jsd\": inv_jsd,\n",
    "        })\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# save_checkpoint persists model/optimizer states and rich validation metadata.\n",
    "# Objective: guarantee resumable training under runtime limits by writing\n",
    "#            per-epoch files that store metrics, history, and cached arrays.\n",
    "# Logic: package model weights, optimiser buffers, history so far, and extra\n",
    "#        arrays needed to resume evaluation. Filenames follow \"epoch_XXX.pth\" for clarity.\n",
    "# Expected behaviour: writes a `.pth` file to CHECKPOINT_DIR and prints the path.\n",
    "# Reference: aligns with the runtime monitoring requirement in the evaluation guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def save_checkpoint(epoch: int, model: nn.Module, optimizer: torch.optim.Optimizer, history: List[Dict[str, Any]], train_record: Dict[str, Any], val_record: Dict[str, Any], extra: Dict[str, Any]) -> Path:\n",
    "    payload = {\n",
    "        \"epoch\": epoch,\n",
    "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"history\": history,\n",
    "        \"train_record\": train_record,\n",
    "        \"val_record\": val_record,\n",
    "    }\n",
    "    payload.update(extra)\n",
    "    path = CHECKPOINT_DIR / CHECKPOINT_TEMPLATE.format(epoch=epoch)\n",
    "    torch.save(payload, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# load_checkpoint restores model/optimizer states and returns archived metadata.\n",
    "# Objective: allow fast resumption or retrospective analysis using the stored\n",
    "#            arrays and history.\n",
    "# Expected behaviour: loads the `.pth` file on DEVICE, restores weights, and\n",
    "#                     returns the payload dictionary for further processing.\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_checkpoint(path: Path, model: nn.Module, optimizer: torch.optim.Optimizer) -> Dict[str, Any]:\n",
    "    payload = torch.load(path, map_location=DEVICE)\n",
    "    model.load_state_dict(payload[\"model_state\"])\n",
    "    optimizer.load_state_dict(payload[\"optimizer_state\"])\n",
    "    print(f\"Loaded checkpoint from {path}\")\n",
    "    return payload\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# list_available_checkpoints enumerates saved epochs for easy resumption.\n",
    "# Expected behaviour: prints filenames such as `epoch_005.pth` and returns a\n",
    "#                     sorted list so notebook users can grab the latest checkpoint.\n",
    "# ---------------------------------------------------------------------------\n",
    "def list_available_checkpoints() -> List[Path]:\n",
    "    paths = sorted(CHECKPOINT_DIR.glob(\"epoch_*.pth\"))\n",
    "    for p in paths:\n",
    "        print(p.name)\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78201ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_one_epoch function with lambda parameter\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    duration = time.perf_counter() - start\n",
    "    results = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    results.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a33af9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Loss function with DisCo penalty (`model.py` lines 24-86 & `disco.py`)\n",
    "\n",
    "We compute the weighted binary cross-entropy loss and add the unbiased distance-correlation penalty between the classifier score and jet mass on background events, following the original `train_model(..., decorr_mode='dist_unbiased')` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77a02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank to avoid redefining helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692b7844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda ramping schedule:\n",
      "  Start: 5.0\n",
      "  End: 55.0\n",
      "  Ramp over: 50 epochs\n",
      "  Final epochs (50-49): constant at 55.0\n"
     ]
    }
   ],
   "source": [
    "# Visualize the lambda ramping schedule\n",
    "epochs = range(EPOCHS)\n",
    "lambdas = [get_current_lambda(epoch) for epoch in epochs]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lambdas, 'b-', linewidth=2, label='Lambda Mass Schedule')\n",
    "plt.axvline(x=LAMBDA_RAMP_EPOCHS, color='r', linestyle='--', alpha=0.7, label=f'Ramp End (Epoch {LAMBDA_RAMP_EPOCHS})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Lambda Mass')\n",
    "plt.title('DisCo Penalty Ramping Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Lambda ramping schedule:\")\n",
    "print(f\"  Start: {LAMBDA_MASS_START}\")\n",
    "print(f\"  End: {LAMBDA_MASS_END}\")\n",
    "print(f\"  Ramp over: {LAMBDA_RAMP_EPOCHS} epochs\")\n",
    "print(f\"  Final epochs ({LAMBDA_RAMP_EPOCHS}-{EPOCHS-1}): constant at {LAMBDA_MASS_END}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9d446a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training loop (`model.py` lines 88-170)\n",
    "\n",
    "We adapt the original `train`/`val` helpers to work seamlessly on CPU/GPU and to log the DisCo decorrelation term alongside the classification loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67781dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# train_one_epoch performs a full pass over the training DataLoader.\n",
    "# Objective: accumulate gradients, update model parameters, and track runtime\n",
    "#            metrics (examples/sec, iterations/sec) required for the runtime log\n",
    "#            requested in the evaluation plan.\n",
    "# Logic: iterate over batches, compute losses, backpropagate, and average the\n",
    "#        collected metrics dictionary.\n",
    "# Expected behaviour: returns a dictionary containing mean losses/metrics and\n",
    "#                     throughput information for the epoch.\n",
    "# ---------------------------------------------------------------------------\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    duration = time.perf_counter() - start\n",
    "    results = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    results.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# evaluate mirrors train_one_epoch but without gradient updates.\n",
    "# Objective: compute validation/test metrics and collect raw arrays for ROC/ABCD\n",
    "#            analysis each epoch.\n",
    "# Logic: disable gradients, reuse compute_losses for consistency, and keep track\n",
    "#        of scores, labels, weights, and masses as numpy arrays.\n",
    "# Expected behaviour: returns the concatenated arrays alongside averaged metrics.\n",
    "# ---------------------------------------------------------------------------\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    scores, labels_all, weights_all, masses_all = [], [], [], []\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"eval\"):\n",
    "            loss, metrics = compute_losses(model, batch)\n",
    "            features, labels, weights, masses = batch\n",
    "            features = features.to(DEVICE)\n",
    "            _, score = model(features)\n",
    "            batch_size = features.shape[0]\n",
    "            total_examples += batch_size\n",
    "            scores.append(score.cpu().numpy())\n",
    "            labels_all.append(labels.numpy())\n",
    "            weights_all.append(weights.numpy())\n",
    "            masses_all.append(masses.numpy())\n",
    "            agg.setdefault(\"loss_cls\", []).append(metrics[\"loss_cls\"])\n",
    "            for key in (\"accuracy\", \"precision\", \"recall\", \"f1\"):\n",
    "                agg.setdefault(key, []).append(metrics[key])\n",
    "            if \"dCorr_s_m\" in metrics:\n",
    "                agg.setdefault(\"dCorr_s_m\", []).append(metrics[\"dCorr_s_m\"])\n",
    "    duration = time.perf_counter() - start\n",
    "    metrics_mean = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    metrics_mean.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return (\n",
    "        np.concatenate(scores),\n",
    "        np.concatenate(labels_all),\n",
    "        np.concatenate(weights_all),\n",
    "        np.concatenate(masses_all),\n",
    "        metrics_mean,\n",
    "    )\n",
    "\n",
    "history: List[Dict[str, Any]] = []\n",
    "start_epoch = 0\n",
    "if RESUME_CHECKPOINT is not None:\n",
    "    loaded_checkpoint = load_checkpoint(Path(RESUME_CHECKPOINT), model, optimizer)\n",
    "    history = loaded_checkpoint.get(\"history\", [])\n",
    "    start_epoch = int(loaded_checkpoint.get(\"epoch\", -1)) + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# History tracks per-epoch metrics for later plotting and checkpoint metadata.\n",
    "history: List[Dict[str, Any]] = []\n",
    "start_epoch = 0\n",
    "if RESUME_CHECKPOINT is not None:\n",
    "    loaded_checkpoint = load_checkpoint(Path(RESUME_CHECKPOINT), model, optimizer)\n",
    "    history = loaded_checkpoint.get(\"history\", [])\n",
    "    start_epoch = int(loaded_checkpoint.get(\"epoch\", -1)) + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# for epoch in range(start_epoch, EPOCHS):\n",
    "#     print(f\"Epoch {epoch:03d}\")\n",
    "#     train_metrics = train_one_epoch(model, train_loader, optimizer)\n",
    "#     s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "#     # Compute rich validation diagnostics as mandated by the evaluation guide.\n",
    "#     classification = compute_epoch_classification_metrics(s_val, y_val, w_val)\n",
    "#     roc_diag = compute_roc_diagnostics(s_val, y_val, w_val)\n",
    "#     abcd_stats = compute_abcd_statistics(s_val, m_val, y_val, w_val)\n",
    "#     jsd_points = compute_jsd_summary(s_val, m_val, y_val, w_val)\n",
    "\n",
    "#     record = {\n",
    "#         \"epoch\": epoch,\n",
    "#         \"train_loss_cls\": train_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "#         \"train_accuracy\": train_metrics.get(\"accuracy\", float(\"nan\")),\n",
    "#         \"train_precision\": train_metrics.get(\"precision\", float(\"nan\")),\n",
    "#         \"train_recall\": train_metrics.get(\"recall\", float(\"nan\")),\n",
    "#         \"train_f1\": train_metrics.get(\"f1\", float(\"nan\")),\n",
    "#         \"train_epoch_seconds\": train_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "#         \"train_examples_per_second\": train_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "#         \"val_loss_cls\": val_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "#         \"val_accuracy\": classification[\"accuracy\"],\n",
    "#         \"val_precision\": classification[\"precision\"],\n",
    "#         \"val_recall\": classification[\"recall\"],\n",
    "#         \"val_f1\": classification[\"f1\"],\n",
    "#         \"val_auc\": roc_diag[\"auc\"],\n",
    "#         \"val_epoch_seconds\": val_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "#         \"val_examples_per_second\": val_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "#     }\n",
    "#     if \"dCorr_s_m\" in train_metrics:\n",
    "#         record[\"train_dCorr_s_m\"] = train_metrics[\"dCorr_s_m\"]\n",
    "#     if \"dCorr_s_m\" in val_metrics:\n",
    "#         record[\"val_dCorr_s_m\"] = val_metrics[\"dCorr_s_m\"]\n",
    "#     for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "#         key = f\"background_eff_at_{int(target*100)}pct_sig\"\n",
    "#         record[key] = roc_diag[key]\n",
    "#     for item in abcd_stats[\"per_efficiency\"]:\n",
    "#         eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "#         record[f\"abcd_closure_ratio_{eff}pct\"] = item[\"closure_ratio\"]\n",
    "#         record[f\"abcd_pull_{eff}pct\"] = item[\"pull\"]\n",
    "#     aggregated = abcd_stats[\"aggregated\"]\n",
    "#     record[\"abcd_closure_ratio_mean\"] = aggregated[\"closure_ratio\"][\"mean\"]\n",
    "#     record[\"abcd_closure_ratio_std\"] = aggregated[\"closure_ratio\"][\"std\"]\n",
    "#     record[\"abcd_closure_ratio_median\"] = aggregated[\"closure_ratio\"][\"median\"]\n",
    "#     record[\"abcd_pull_mean\"] = aggregated[\"pull\"][\"mean\"]\n",
    "#     record[\"abcd_pull_std\"] = aggregated[\"pull\"][\"std\"]\n",
    "#     record[\"abcd_pull_rms\"] = aggregated[\"pull\"][\"rms\"]\n",
    "#     record[\"abcd_pull_median\"] = aggregated[\"pull\"][\"median\"]\n",
    "#     record[\"transfer_factor_B_over_D_mean\"] = aggregated[\"transfer_factor_B_over_D\"][\"mean\"]\n",
    "#     record[\"transfer_factor_B_over_D_std\"] = aggregated[\"transfer_factor_B_over_D\"][\"std\"]\n",
    "#     record[\"transfer_factor_C_over_D_mean\"] = aggregated[\"transfer_factor_C_over_D\"][\"mean\"]\n",
    "#     record[\"transfer_factor_C_over_D_std\"] = aggregated[\"transfer_factor_C_over_D\"][\"std\"]\n",
    "#     record[\"sideband_ratio_B_over_C_mean\"] = aggregated[\"sideband_ratio_B_over_C\"][\"mean\"]\n",
    "#     record[\"sideband_ratio_B_over_C_std\"] = aggregated[\"sideband_ratio_B_over_C\"][\"std\"]\n",
    "#     record[\"sideband_stability_mean\"] = aggregated[\"sideband_stability\"][\"mean\"]\n",
    "#     record[\"sideband_stability_std\"] = aggregated[\"sideband_stability\"][\"std\"]\n",
    "#     record[\"asimov_significance_mean\"] = aggregated[\"asimov_significance\"][\"mean\"]\n",
    "#     record[\"asimov_significance_max\"] = aggregated[\"asimov_significance\"][\"max\"]\n",
    "#     for item in jsd_points:\n",
    "#         eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "#         record[f\"inverse_jsd_{eff}pct\"] = item[\"inverse_jsd\"]\n",
    "#         record[f\"background_rejection_{eff}pct\"] = item[\"background_rejection\"]\n",
    "\n",
    "#     history.append(record)\n",
    "\n",
    "#     # Persist a checkpoint capturing everything needed to resume training and\n",
    "#     # redo evaluations, aligning with the per-epoch storage requirement.\n",
    "#     val_record = {\n",
    "#         \"classification\": classification,\n",
    "#         \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in roc_diag.items()},\n",
    "#         \"abcd\": abcd_stats,\n",
    "#         \"jsd\": jsd_points,\n",
    "#         \"confusion_matrix\": classification[\"confusion_matrix\"].tolist(),\n",
    "#     }\n",
    "#     train_record = train_metrics\n",
    "#     extra = {\n",
    "#         \"val_scores\": s_val.tolist(),\n",
    "#         \"val_labels\": y_val.tolist(),\n",
    "#         \"val_weights\": w_val.tolist(),\n",
    "#         \"val_masses\": m_val.tolist(),\n",
    "#     }\n",
    "#     save_checkpoint(epoch, model, optimizer, history, train_record, val_record, extra)\n",
    "\n",
    "#     print(\n",
    "#         f\"AUC={roc_diag['auc']:.3f} | \"\n",
    "#         f\"val_acc={classification['accuracy']:.3f} | \"\n",
    "#         f\"closure@30%={record['abcd_closure_ratio_30pct']:.3f} | \"\n",
    "#         f\"train_loss={train_metrics.get('loss_cls', float('nan')):.3f}\"\n",
    "#     )\n",
    "\n",
    "# history_df = pd.DataFrame(history)\n",
    "# history_df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d87e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-collapse overrides active: logits-BCE, KL prior matching, confidence penalty, gradient clipping, AdamW.\n"
     ]
    }
   ],
   "source": [
    "# Anti-collapse overrides for training and QNode. Run this cell once before training.\n",
    "\n",
    "EPS = 1e-8\n",
    "ENTROPY_COEFF = 0.02          # confidence penalty weight (small)\n",
    "PRIOR_MATCH_COEFF = 0.2       # batch prior-matching KL weight\n",
    "VARIANCE_FLOOR = 0.02         # optional: minimum score variance target\n",
    "VARIANCE_COEFF = 0.02         # weight for variance floor penalty\n",
    "CLIP_NORM = 1.0               # gradient clipping\n",
    "WEIGHT_DECAY = 1e-3           # AdamW weight decay\n",
    "LEARNING_RATE = 5e-4          # slightly lower LR for stability\n",
    "\n",
    "# ---- Re-define compute_losses to use BCE-with-logits + anti-collapse terms ----\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).float()\n",
    "    weights = weights.to(DEVICE).float()\n",
    "    masses = masses.to(DEVICE).float()\n",
    "\n",
    "    logits, score = model(features)  # score still used for metrics/DisCo\n",
    "    # Use logit difference for BCE-with-logits (binary)\n",
    "    logit = (logits[:, 1] - logits[:, 0]).contiguous()\n",
    "\n",
    "    # Weighted pos_weight for class imbalance (computed per-batch)\n",
    "    pos_mask = labels > 0.5\n",
    "    w_pos = weights[pos_mask].sum()\n",
    "    w_neg = weights[~pos_mask].sum()\n",
    "    pos_weight = (w_neg / (w_pos + EPS)).clamp(min=0.5, max=5.0)\n",
    "\n",
    "    # Weighted BCE-with-logits\n",
    "    per_example = F.binary_cross_entropy_with_logits(logit, labels, reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss_cls = (weights * per_example).sum() / (weights.sum() + EPS)\n",
    "\n",
    "    # Anti-collapse: (1) batch prior matching (weighted)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    wsum = weights.sum() + EPS\n",
    "    p_bar = (probs * weights.unsqueeze(1)).sum(dim=0) / wsum\n",
    "    prior_pos = (w_pos / (w_pos + w_neg + EPS)).clamp(1e-3, 1 - 1e-3)\n",
    "    prior = torch.stack([1.0 - prior_pos, prior_pos])\n",
    "    kl_prior = (p_bar * (p_bar.add(EPS).log() - prior.add(EPS).log())).sum()\n",
    "\n",
    "    # Anti-collapse: (2) confidence penalty (maximize entropy => add -H)\n",
    "    entropy = -(probs * (probs + EPS).log()).sum(dim=1).mean()\n",
    "    conf_penalty = -ENTROPY_COEFF * entropy\n",
    "\n",
    "    # Optional: encourage non-zero variance of the score\n",
    "    mu = (weights * score).sum() / wsum\n",
    "    var = (weights * (score - mu).pow(2)).sum() / wsum\n",
    "    var_penalty = VARIANCE_COEFF * F.relu(VARIANCE_FLOOR - var)\n",
    "\n",
    "    loss = loss_cls + PRIOR_MATCH_COEFF * kl_prior + conf_penalty + var_penalty\n",
    "\n",
    "    # DisCo decorrelation on background (unchanged)\n",
    "    background = labels < 0.5\n",
    "    metrics = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(compute_weighted_classification_stats(labels, score, weights)[\"accuracy\"]),\n",
    "    }\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "# ---- Gradient clipping in the training step ----\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for k, v in metrics.items():\n",
    "            agg.setdefault(k, []).append(v)\n",
    "        agg.setdefault(\"loss\", []).append(float(loss.detach().cpu()))\n",
    "    return {k: float(np.mean(v)) for k, v in agg.items()}\n",
    "\n",
    "# ---- Widen quantum readout and bound embedding ----\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    # Rebuild the PennyLane module with safer embedding and richer readout\n",
    "    class PennyLaneSingleDisco(nn.Module):\n",
    "        def __init__(self, n_features: int, n_qubits: int = globals().get(\"N_QUBITS\", 6), layers: int = globals().get(\"QML_LAYERS\", 3), device_name: str = globals().get(\"QML_DEVICE\", \"default.qubit\")):\n",
    "            if not PENNYLANE_AVAILABLE:\n",
    "                raise RuntimeError(\"PennyLane is not installed.\")\n",
    "            super().__init__()\n",
    "            self.n_qubits = n_qubits\n",
    "            self.n_features = n_features\n",
    "            self.compressor = nn.Sequential(\n",
    "                nn.Linear(n_features, n_qubits),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(p=0.1),\n",
    "            )\n",
    "            qdevice = qml.device(device_name, wires=n_qubits)\n",
    "            weight_shapes = { \"weights_strong\": (layers, n_qubits, 3) }\n",
    "\n",
    "            @qml.qnode(qdevice, interface=\"torch\")\n",
    "            def circuit(inputs, weights_strong):\n",
    "                take = min(inputs.shape[-1], n_qubits)\n",
    "                if inputs.ndim == 1:\n",
    "                    x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "                    x_pad[:take] = inputs[:take]\n",
    "                else:\n",
    "                    pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "                    x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "                    x_pad[..., :take] = inputs[..., :take]\n",
    "                angles = torch.pi * torch.tanh(x_pad)  # bound angles for stable grads\n",
    "                qml.templates.AngleEmbedding(angles, wires=range(n_qubits), rotation='Y')\n",
    "                qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "                return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "            self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "            self.head = nn.Linear(n_qubits, 2)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            angles = self.compressor(x)\n",
    "            q_features = self.qlayer(angles)\n",
    "            logits = self.head(q_features)\n",
    "            score = F.softmax(logits, dim=1)[:, 1]\n",
    "            return logits, score\n",
    "\n",
    "    # Rebuild model and optimizer with AdamW\n",
    "    model = build_model(len(FEATURE_NAMES))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "else:\n",
    "    # Even for classical-only, AdamW + clipping + anti-collapse helps\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Anti-collapse overrides active: logits-BCE, KL prior matching, confidence penalty, gradient clipping, AdamW.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7a74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_losses override loaded: metrics now include precision/recall/F1.\n"
     ]
    }
   ],
   "source": [
    "# Fix: compute_losses returns precision/recall/F1 (keeps anti-collapse + DisCo)\n",
    "# Run this cell once before training/evaluation\n",
    "\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).float()\n",
    "    weights = weights.to(DEVICE).float()\n",
    "    masses = masses.to(DEVICE).float()\n",
    "\n",
    "    logits, score = model(features)\n",
    "    logit = (logits[:, 1] - logits[:, 0]).contiguous()\n",
    "\n",
    "    # Batch imbalance handling\n",
    "    EPS = globals().get(\"EPS\", 1e-8)\n",
    "    pos_mask = labels > 0.5\n",
    "    w_pos = weights[pos_mask].sum()\n",
    "    w_neg = weights[~pos_mask].sum()\n",
    "    pos_weight = (w_neg / (w_pos + EPS)).clamp(min=0.5, max=5.0)\n",
    "\n",
    "    # Weighted BCE-with-logits (classification loss)\n",
    "    per_example = F.binary_cross_entropy_with_logits(logit, labels, reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss_cls = (weights * per_example).sum() / (weights.sum() + EPS)\n",
    "\n",
    "    # Anti-collapse regularizers (safe defaults if not previously defined)\n",
    "    ENTROPY_COEFF = float(globals().get(\"ENTROPY_COEFF\", 0.01))\n",
    "    PRIOR_MATCH_COEFF = float(globals().get(\"PRIOR_MATCH_COEFF\", 0.1))\n",
    "    VARIANCE_FLOOR = float(globals().get(\"VARIANCE_FLOOR\", 0.02))\n",
    "    VARIANCE_COEFF = float(globals().get(\"VARIANCE_COEFF\", 0.02))\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    wsum = weights.sum() + EPS\n",
    "    p_bar = (probs * weights.unsqueeze(1)).sum(dim=0) / wsum\n",
    "    prior_pos = (w_pos / (w_pos + w_neg + EPS)).clamp(1e-3, 1 - 1e-3)\n",
    "    prior = torch.stack([1.0 - prior_pos, prior_pos])\n",
    "    kl_prior = (p_bar * (p_bar.add(EPS).log() - prior.add(EPS).log())).sum()\n",
    "\n",
    "    entropy = -(probs * (probs + EPS).log()).sum(dim=1).mean()\n",
    "    conf_penalty = -ENTROPY_COEFF * entropy\n",
    "\n",
    "    mu = (weights * score).sum() / wsum\n",
    "    var = (weights * (score - mu).pow(2)).sum() / wsum\n",
    "    var_penalty = VARIANCE_COEFF * F.relu(VARIANCE_FLOOR - var)\n",
    "\n",
    "    loss = loss_cls + PRIOR_MATCH_COEFF * kl_prior + conf_penalty + var_penalty\n",
    "\n",
    "    # DisCo decorrelation on background\n",
    "    background = labels < 0.5\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "    else:\n",
    "        d_mass = None\n",
    "\n",
    "    # Full classification stats expected by evaluate()\n",
    "    stats = compute_weighted_classification_stats(labels, score, weights)\n",
    "    metrics: Dict[str, float] = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(stats.get(\"accuracy\", float(\"nan\"))),\n",
    "        \"precision\": float(stats.get(\"precision\", float(\"nan\"))),\n",
    "        \"recall\": float(stats.get(\"recall\", float(\"nan\"))),\n",
    "        \"f1\": float(stats.get(\"f1\", float(\"nan\"))),\n",
    "    }\n",
    "    if d_mass is not None:\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "print(\"compute_losses override loaded: metrics now include precision/recall/F1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6468b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000\n",
      "Current lambda_mass: 5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_000.pth\n",
      "AUC=0.139 | val_acc=0.501 | closure@30%=3.648 | train_loss=0.704 | λ=5.0\n",
      "Epoch 001\n",
      "Current lambda_mass: 6.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.099951945\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_001.pth\n",
      "AUC=0.255 | val_acc=0.501 | closure@30%=0.996 | train_loss=0.696 | λ=6.0\n",
      "Epoch 002\n",
      "Current lambda_mass: 7.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.2999199\n",
      "JSDvsR 60 0.59995997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_002.pth\n",
      "AUC=0.209 | val_acc=0.501 | closure@30%=1.878 | train_loss=0.695 | λ=7.0\n",
      "Epoch 003\n",
      "Current lambda_mass: 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.5999519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_003.pth\n",
      "AUC=0.296 | val_acc=0.365 | closure@30%=0.952 | train_loss=0.695 | λ=8.0\n",
      "Epoch 004\n",
      "Current lambda_mass: 9.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09997597\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_004.pth\n",
      "AUC=0.290 | val_acc=0.355 | closure@30%=1.202 | train_loss=0.695 | λ=9.0\n",
      "Epoch 005\n",
      "Current lambda_mass: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29996797\n",
      "JSDvsR 60 0.59996796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_005.pth\n",
      "AUC=0.305 | val_acc=0.356 | closure@30%=1.272 | train_loss=0.695 | λ=10.0\n",
      "Epoch 006\n",
      "Current lambda_mass: 11.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29997596\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_006.pth\n",
      "AUC=0.326 | val_acc=0.377 | closure@30%=1.220 | train_loss=0.695 | λ=11.0\n",
      "Epoch 007\n",
      "Current lambda_mass: 12.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.5999039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_007.pth\n",
      "AUC=0.424 | val_acc=0.487 | closure@30%=0.730 | train_loss=0.695 | λ=12.0\n",
      "Epoch 008\n",
      "Current lambda_mass: 13.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29993594\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_008.pth\n",
      "AUC=0.407 | val_acc=0.467 | closure@30%=0.849 | train_loss=0.695 | λ=13.0\n",
      "Epoch 009\n",
      "Current lambda_mass: 14.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.099959955\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.5999279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_009.pth\n",
      "AUC=0.453 | val_acc=0.466 | closure@30%=0.712 | train_loss=0.695 | λ=14.0\n",
      "Epoch 010\n",
      "Current lambda_mass: 15.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29994392\n",
      "JSDvsR 60 0.59995997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_010.pth\n",
      "AUC=0.430 | val_acc=0.448 | closure@30%=0.864 | train_loss=0.695 | λ=15.0\n",
      "Epoch 011\n",
      "Current lambda_mass: 16.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09997597\n",
      "JSDvsR 30 0.29997596\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_011.pth\n",
      "AUC=0.403 | val_acc=0.434 | closure@30%=1.028 | train_loss=0.695 | λ=16.0\n",
      "Epoch 012\n",
      "Current lambda_mass: 17.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_012.pth\n",
      "AUC=0.475 | val_acc=0.488 | closure@30%=0.718 | train_loss=0.695 | λ=17.0\n",
      "Epoch 013\n",
      "Current lambda_mass: 18.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_013.pth\n",
      "AUC=0.449 | val_acc=0.466 | closure@30%=0.910 | train_loss=0.695 | λ=18.0\n",
      "Epoch 014\n",
      "Current lambda_mass: 19.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_014.pth\n",
      "AUC=0.485 | val_acc=0.493 | closure@30%=0.784 | train_loss=0.695 | λ=19.0\n",
      "Epoch 015\n",
      "Current lambda_mass: 20.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09997597\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_015.pth\n",
      "AUC=0.509 | val_acc=0.494 | closure@30%=0.724 | train_loss=0.695 | λ=20.0\n",
      "Epoch 016\n",
      "Current lambda_mass: 21.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_016.pth\n",
      "AUC=0.475 | val_acc=0.477 | closure@30%=0.865 | train_loss=0.695 | λ=21.0\n",
      "Epoch 017\n",
      "Current lambda_mass: 22.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09997597\n",
      "JSDvsR 30 0.29997596\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_017.pth\n",
      "AUC=0.514 | val_acc=0.511 | closure@30%=0.737 | train_loss=0.695 | λ=22.0\n",
      "Epoch 018\n",
      "Current lambda_mass: 23.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_018.pth\n",
      "AUC=0.466 | val_acc=0.472 | closure@30%=0.981 | train_loss=0.694 | λ=23.0\n",
      "Epoch 019\n",
      "Current lambda_mass: 24.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.29995194\n",
      "JSDvsR 60 0.599992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_019.pth\n",
      "AUC=0.430 | val_acc=0.460 | closure@30%=1.240 | train_loss=0.695 | λ=24.0\n",
      "Epoch 020\n",
      "Current lambda_mass: 25.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_020.pth\n",
      "AUC=0.562 | val_acc=0.545 | closure@30%=0.679 | train_loss=0.695 | λ=25.0\n",
      "Epoch 021\n",
      "Current lambda_mass: 26.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_021.pth\n",
      "AUC=0.550 | val_acc=0.526 | closure@30%=0.709 | train_loss=0.695 | λ=26.0\n",
      "Epoch 022\n",
      "Current lambda_mass: 27.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29996797\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_022.pth\n",
      "AUC=0.489 | val_acc=0.497 | closure@30%=1.009 | train_loss=0.695 | λ=27.0\n",
      "Epoch 023\n",
      "Current lambda_mass: 28.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.59996796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_023.pth\n",
      "AUC=0.563 | val_acc=0.513 | closure@30%=0.708 | train_loss=0.695 | λ=28.0\n",
      "Epoch 024\n",
      "Current lambda_mass: 29.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29995194\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_024.pth\n",
      "AUC=0.548 | val_acc=0.539 | closure@30%=0.770 | train_loss=0.694 | λ=29.0\n",
      "Epoch 025\n",
      "Current lambda_mass: 30.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.29997596\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_025.pth\n",
      "AUC=0.541 | val_acc=0.495 | closure@30%=0.795 | train_loss=0.694 | λ=30.0\n",
      "Epoch 026\n",
      "Current lambda_mass: 31.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29996797\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_026.pth\n",
      "AUC=0.556 | val_acc=0.534 | closure@30%=0.757 | train_loss=0.695 | λ=31.0\n",
      "Epoch 027\n",
      "Current lambda_mass: 32.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_027.pth\n",
      "AUC=0.540 | val_acc=0.494 | closure@30%=0.824 | train_loss=0.694 | λ=32.0\n",
      "Epoch 028\n",
      "Current lambda_mass: 33.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.29997596\n",
      "JSDvsR 60 0.59995997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_028.pth\n",
      "AUC=0.568 | val_acc=0.498 | closure@30%=0.738 | train_loss=0.695 | λ=33.0\n",
      "Epoch 029\n",
      "Current lambda_mass: 34.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29996797\n",
      "JSDvsR 60 0.599984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_029.pth\n",
      "AUC=0.517 | val_acc=0.488 | closure@30%=0.982 | train_loss=0.694 | λ=34.0\n",
      "Epoch 030\n",
      "Current lambda_mass: 35.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_030.pth\n",
      "AUC=0.541 | val_acc=0.525 | closure@30%=0.865 | train_loss=0.695 | λ=35.0\n",
      "Epoch 031\n",
      "Current lambda_mass: 36.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09998398\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_031.pth\n",
      "AUC=0.553 | val_acc=0.543 | closure@30%=0.826 | train_loss=0.695 | λ=36.0\n",
      "Epoch 032\n",
      "Current lambda_mass: 37.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_032.pth\n",
      "AUC=0.586 | val_acc=0.566 | closure@30%=0.699 | train_loss=0.694 | λ=37.0\n",
      "Epoch 033\n",
      "Current lambda_mass: 38.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.299992\n",
      "JSDvsR 60 0.59995997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_033.pth\n",
      "AUC=0.603 | val_acc=0.565 | closure@30%=0.663 | train_loss=0.695 | λ=38.0\n",
      "Epoch 034\n",
      "Current lambda_mass: 39.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.1\n",
      "JSDvsR 30 0.3\n",
      "JSDvsR 60 0.59997594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_034.pth\n",
      "AUC=0.556 | val_acc=0.543 | closure@30%=0.853 | train_loss=0.695 | λ=39.0\n",
      "Epoch 035\n",
      "Current lambda_mass: 40.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29998398\n",
      "JSDvsR 60 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_035.pth\n",
      "AUC=0.573 | val_acc=0.498 | closure@30%=0.769 | train_loss=0.695 | λ=40.0\n",
      "Epoch 036\n",
      "Current lambda_mass: 41.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSDvsR 10 0.09999199\n",
      "JSDvsR 30 0.29996797\n",
      "JSDvsR 60 0.59996796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1531467/4283937357.py:241: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /global/u2/s/sungwon/QML_project/ABCDisCo/notebook/checkpoints_qml_single_disco/epoch_036.pth\n",
      "AUC=0.564 | val_acc=0.515 | closure@30%=0.818 | train_loss=0.695 | λ=41.0\n",
      "Epoch 037\n",
      "Current lambda_mass: 42.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  13%|█▎        | 33/245 [00:15<01:35,  2.21it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Updated training loop with lambda ramping\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"Epoch {epoch:03d}\")\n",
    "    # Get current lambda value for ramping\n",
    "    current_lambda = get_current_lambda(epoch)\n",
    "    print(f\"Current lambda_mass: {current_lambda:.2f}\")\n",
    "    \n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer, current_lambda)\n",
    "    s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "    # Compute rich validation diagnostics as mandated by the evaluation guide.\n",
    "    classification = compute_epoch_classification_metrics(s_val, y_val, w_val)\n",
    "    roc_diag = compute_roc_diagnostics(s_val, y_val, w_val)\n",
    "    abcd_stats = compute_abcd_statistics(s_val, m_val, y_val, w_val)\n",
    "    jsd_points = compute_jsd_summary(s_val, m_val, y_val, w_val)\n",
    "\n",
    "    record = {\n",
    "        \"epoch\": epoch,\n",
    "        \"lambda_mass\": current_lambda,  # Track current lambda value\n",
    "        \"train_loss_cls\": train_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "        \"train_accuracy\": train_metrics.get(\"accuracy\", float(\"nan\")),\n",
    "        \"train_precision\": train_metrics.get(\"precision\", float(\"nan\")),\n",
    "        \"train_recall\": train_metrics.get(\"recall\", float(\"nan\")),\n",
    "        \"train_f1\": train_metrics.get(\"f1\", float(\"nan\")),\n",
    "        \"train_epoch_seconds\": train_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "        \"train_examples_per_second\": train_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "        \"val_loss_cls\": val_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "        \"val_accuracy\": classification[\"accuracy\"],\n",
    "        \"val_precision\": classification[\"precision\"],\n",
    "        \"val_recall\": classification[\"recall\"],\n",
    "        \"val_f1\": classification[\"f1\"],\n",
    "        \"val_auc\": roc_diag[\"auc\"],\n",
    "        \"val_epoch_seconds\": val_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "        \"val_examples_per_second\": val_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "    }\n",
    "    if \"dCorr_s_m\" in train_metrics:\n",
    "        record[\"train_dCorr_s_m\"] = train_metrics[\"dCorr_s_m\"]\n",
    "    if \"dCorr_s_m\" in val_metrics:\n",
    "        record[\"val_dCorr_s_m\"] = val_metrics[\"dCorr_s_m\"]\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        key = f\"background_eff_at_{int(target*100)}pct_sig\"\n",
    "        record[key] = roc_diag[key]\n",
    "    for item in abcd_stats[\"per_efficiency\"]:\n",
    "        eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "        record[f\"abcd_closure_ratio_{eff}pct\"] = item[\"closure_ratio\"]\n",
    "        record[f\"abcd_pull_{eff}pct\"] = item[\"pull\"]\n",
    "    aggregated = abcd_stats[\"aggregated\"]\n",
    "    record[\"abcd_closure_ratio_mean\"] = aggregated[\"closure_ratio\"][\"mean\"]\n",
    "    record[\"abcd_closure_ratio_std\"] = aggregated[\"closure_ratio\"][\"std\"]\n",
    "    record[\"abcd_closure_ratio_median\"] = aggregated[\"closure_ratio\"][\"median\"]\n",
    "    record[\"abcd_pull_mean\"] = aggregated[\"pull\"][\"mean\"]\n",
    "    record[\"abcd_pull_std\"] = aggregated[\"pull\"][\"std\"]\n",
    "    record[\"abcd_pull_rms\"] = aggregated[\"pull\"][\"rms\"]\n",
    "    record[\"abcd_pull_median\"] = aggregated[\"pull\"][\"median\"]\n",
    "    record[\"transfer_factor_B_over_D_mean\"] = aggregated[\"transfer_factor_B_over_D\"][\"mean\"]\n",
    "    record[\"transfer_factor_B_over_D_std\"] = aggregated[\"transfer_factor_B_over_D\"][\"std\"]\n",
    "    record[\"transfer_factor_C_over_D_mean\"] = aggregated[\"transfer_factor_C_over_D\"][\"mean\"]\n",
    "    record[\"transfer_factor_C_over_D_std\"] = aggregated[\"transfer_factor_C_over_D\"][\"std\"]\n",
    "    record[\"sideband_ratio_B_over_C_mean\"] = aggregated[\"sideband_ratio_B_over_C\"][\"mean\"]\n",
    "    record[\"sideband_ratio_B_over_C_std\"] = aggregated[\"sideband_ratio_B_over_C\"][\"std\"]\n",
    "    record[\"sideband_stability_mean\"] = aggregated[\"sideband_stability\"][\"mean\"]\n",
    "    record[\"sideband_stability_std\"] = aggregated[\"sideband_stability\"][\"std\"]\n",
    "    record[\"asimov_significance_mean\"] = aggregated[\"asimov_significance\"][\"mean\"]\n",
    "    record[\"asimov_significance_max\"] = aggregated[\"asimov_significance\"][\"max\"]\n",
    "    for item in jsd_points:\n",
    "        eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "        record[f\"inverse_jsd_{eff}pct\"] = item[\"inverse_jsd\"]\n",
    "        record[f\"background_rejection_{eff}pct\"] = item[\"background_rejection\"]\n",
    "\n",
    "    history.append(record)\n",
    "\n",
    "    # Persist a checkpoint capturing everything needed to resume training and\n",
    "    # redo evaluations, aligning with the per-epoch storage requirement.\n",
    "    val_record = {\n",
    "        \"classification\": classification,\n",
    "        \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in roc_diag.items()},\n",
    "        \"abcd\": abcd_stats,\n",
    "        \"jsd\": jsd_points,\n",
    "        \"confusion_matrix\": classification[\"confusion_matrix\"].tolist(),\n",
    "    }\n",
    "    train_record = train_metrics\n",
    "    extra = {\n",
    "        \"val_scores\": s_val.tolist(),\n",
    "        \"val_labels\": y_val.tolist(),\n",
    "        \"val_weights\": w_val.tolist(),\n",
    "        \"val_masses\": m_val.tolist(),\n",
    "    }\n",
    "    save_checkpoint(epoch, model, optimizer, history, train_record, val_record, extra)\n",
    "\n",
    "    print(\n",
    "        f\"AUC={roc_diag['auc']:.3f} | \"\n",
    "        f\"val_acc={classification['accuracy']:.3f} | \"\n",
    "        f\"closure@30%={record['abcd_closure_ratio_30pct']:.3f} | \"\n",
    "        f\"train_loss={train_metrics.get('loss_cls', float('nan')):.3f} | \"\n",
    "        f\"λ={current_lambda:.1f}\"\n",
    "    )\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46a892",
   "metadata": {},
   "source": [
    "\n",
    "### Training diagnostics\n",
    "\n",
    "We track the classification loss and the distance-correlation penalty to verify convergence and decorrelation strength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec09beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visual summary of training history (losses, AUC, decorrelation, throughput).\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Panel 0: weighted binary cross-entropy on train vs validation sets.\n",
    "axes[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], label=\"train\", marker=\"o\")\n",
    "axes[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls\"], label=\"val\", marker=\"s\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Binary cross-entropy\")\n",
    "axes[0].set_title(\"Loss history\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel 1: AUC progression across epochs (higher is better signal/background separation).\n",
    "axes[1].plot(history_df[\"epoch\"], history_df[\"val_auc\"], marker=\"o\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_title(\"Validation ROC AUC\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Panel 2: distance correlation term monitoring the DisCo penalty strength.\n",
    "if \"train_dCorr_s_m\" in history_df.columns:\n",
    "    axes[2].plot(history_df[\"epoch\"], history_df[\"train_dCorr_s_m\"], label=\"train\")\n",
    "if \"val_dCorr_s_m\" in history_df.columns:\n",
    "    axes[2].plot(history_df[\"epoch\"], history_df[\"val_dCorr_s_m\"], label=\"val\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Distance correlation\")\n",
    "axes[2].set_title(\"Decorrelating score vs jet mass\")\n",
    "axes[2].legend()\n",
    "\n",
    "# Panel 3: runtime throughput useful for planning segmented jobs under time limits.\n",
    "axes[3].plot(history_df[\"epoch\"], history_df[\"train_examples_per_second\"], label=\"train\", marker=\"o\")\n",
    "axes[3].plot(history_df[\"epoch\"], history_df[\"val_examples_per_second\"], label=\"val\", marker=\"s\")\n",
    "axes[3].set_xlabel(\"Epoch\")\n",
    "axes[3].set_ylabel(\"Examples per second\")\n",
    "axes[3].set_title(\"Throughput diagnostics\")\n",
    "axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# # Loss evolution\n",
    "# axes[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], label=\"train\", marker=\"o\")\n",
    "# axes[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls\"], label=\"val\", marker=\"s\")\n",
    "# axes[0].set_xlabel(\"Epoch\")\n",
    "# axes[0].set_ylabel(\"Binary cross-entropy\")\n",
    "# axes[0].set_title(\"Loss history\")\n",
    "# axes[0].legend()\n",
    "\n",
    "# # Classification metrics\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"train_accuracy\"], label=\"train acc\", marker=\"o\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_accuracy\"], label=\"val acc\", marker=\"s\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_precision\"], label=\"val precision\", linestyle=\"--\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_recall\"], label=\"val recall\", linestyle=\":\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_f1\"], label=\"val F1\", linestyle=\"-.\" )\n",
    "# axes[1].set_xlabel(\"Epoch\")\n",
    "# axes[1].set_ylabel(\"Score\")\n",
    "# axes[1].set_ylim(0.0, 1.05)\n",
    "# axes[1].set_title(\"Classification summary\")\n",
    "# axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# # ABCD closure and ROC\n",
    "# axes[2].plot(history_df[\"epoch\"], history_df[\"val_auc\"], label=\"AUC\", marker=\"o\")\n",
    "# axes[2].plot(history_df[\"epoch\"], 1.0 / np.maximum(history_df[\"background_eff_at_30pct_sig\"], 1e-6), label=\"1/ε_B @ 30%\", marker=\"s\")\n",
    "# axes[2].plot(history_df[\"epoch\"], history_df[\"abcd_closure_ratio_30pct\"], label=\"closure ratio @ 30%\", linestyle=\"--\")\n",
    "# axes[2].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"±10% closure band\")\n",
    "# axes[2].set_xlabel(\"Epoch\")\n",
    "# axes[2].set_ylabel(\"Metric value\")\n",
    "# axes[2].set_title(\"ROC vs. ABCD stability\")\n",
    "# axes[2].legend()\n",
    "# axes[2].set_yscale(\"log\")\n",
    "\n",
    "# # Runtime diagnostics\n",
    "# axes[3].plot(history_df[\"epoch\"], history_df[\"train_epoch_seconds\"], label=\"train\", marker=\"o\")\n",
    "# axes[3].plot(history_df[\"epoch\"], history_df[\"val_epoch_seconds\"], label=\"val\", marker=\"s\")\n",
    "# axes[3].set_xlabel(\"Epoch\")\n",
    "# axes[3].set_ylabel(\"Seconds\")\n",
    "# axes[3].set_title(\"Per-epoch wall-clock time\")\n",
    "# axes[3].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Safer plotting for HPC: chunk long paths, decimate, sanitize, and limit rows\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "plot_df = history_df.tail(min(len(history_df), 2000)).copy()\n",
    "\n",
    "def decimate(n):  # reduce marker density\n",
    "    return max(1, n // 200)\n",
    "\n",
    "# Panel 0: loss\n",
    "axes[0].plot(plot_df[\"epoch\"], plot_df[\"train_loss_cls\"], label=\"train\", marker=None)\n",
    "axes[0].plot(plot_df[\"epoch\"], plot_df[\"val_loss_cls\"], label=\"val\", marker=None)\n",
    "axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Binary cross-entropy\"); axes[0].set_title(\"Loss history\"); axes[0].legend()\n",
    "\n",
    "# Panel 1: classification summary (no markers for long series)\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"train_accuracy\"], label=\"train acc\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_accuracy\"], label=\"val acc\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_precision\"], label=\"val precision\", linestyle=\"--\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_recall\"], label=\"val recall\", linestyle=\":\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_f1\"], label=\"val F1\", linestyle=\"-.\")\n",
    "axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Score\"); axes[1].set_ylim(0.0, 1.05); axes[1].set_title(\"Classification summary\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Panel 2: ROC vs ABCD stability (log-safe)\n",
    "bkg_eff = plot_df.get(\"background_eff_at_30pct_sig\")\n",
    "if bkg_eff is not None:\n",
    "    inv_bkg = 1.0 / np.clip(bkg_eff.astype(float), 1e-6, 1e6)\n",
    "    axes[2].plot(plot_df[\"epoch\"], plot_df[\"val_auc\"], label=\"AUC\")\n",
    "    axes[2].plot(plot_df[\"epoch\"], inv_bkg, label=\"1/ε_B @ 30%\")\n",
    "    if np.all(inv_bkg > 0):\n",
    "        axes[2].set_yscale(\"log\")\n",
    "    axes[2].plot(plot_df[\"epoch\"], plot_df.get(\"abcd_closure_ratio_30pct\", np.nan), label=\"closure ratio @ 30%\", linestyle=\"--\")\n",
    "    axes[2].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"±10% closure band\")\n",
    "    axes[2].set_xlabel(\"Epoch\"); axes[2].set_ylabel(\"Metric value\"); axes[2].set_title(\"ROC vs. ABCD stability\"); axes[2].legend()\n",
    "else:\n",
    "    axes[2].set_visible(False)\n",
    "\n",
    "# Panel 3: runtime diagnostics\n",
    "axes[3].plot(plot_df[\"epoch\"], plot_df[\"train_epoch_seconds\"], label=\"train\")\n",
    "axes[3].plot(plot_df[\"epoch\"], plot_df[\"val_epoch_seconds\"], label=\"val\")\n",
    "axes[3].set_xlabel(\"Epoch\"); axes[3].set_ylabel(\"Seconds\"); axes[3].set_title(\"Per-epoch wall-clock time\"); axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"metrics_compact.png\", dpi=120)  # avoid massive inline payload\n",
    "plt.close(fig)\n",
    "from IPython.display import Image, display\n",
    "display(Image(\"metrics_compact.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483491cb",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluation (`evaluation.py` lines 1-70)\n",
    "\n",
    "We reproduce the single-score diagnostics: ROC curves, background mass sculpting, and the Jensen-Shannon divergence versus background rejection figure of merit used in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd85125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation on the held-out test set following the evaluation guide.\n",
    "from importlib import reload\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "from evaluation import JSDvsR\n",
    "\n",
    "# Run the evaluate() helper to obtain raw scores, labels, weights, masses.\n",
    "s_test, y_test, w_test, m_test, test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "# Compute the suite of diagnostics requested: classification metrics, ROC, ABCD closure,\n",
    "# Jensen–Shannon vs rejection, and distance correlation on pure background.\n",
    "classification_test = compute_epoch_classification_metrics(s_test, y_test, w_test)\n",
    "roc_test = compute_roc_diagnostics(s_test, y_test, w_test)\n",
    "abcd_test = compute_abcd_statistics(s_test, m_test, y_test, w_test)\n",
    "jsd_test = compute_jsd_summary(s_test, m_test, y_test, w_test)\n",
    "\n",
    "background = y_test < 0.5\n",
    "if background.any():\n",
    "    d_test = distance_corr_safe(\n",
    "        torch.as_tensor(s_test[background]),\n",
    "        torch.as_tensor(m_test[background]),\n",
    "        torch.ones_like(torch.as_tensor(m_test[background])),\n",
    "    ).item()\n",
    "else:\n",
    "    d_test = float(\"nan\")\n",
    "\n",
    "print(\"Test-set diagnostics:\")\n",
    "print(f\"  AUC = {roc_test['auc']:.3f}\")\n",
    "print(f\"  Accuracy = {classification_test['accuracy']:.3f}\")\n",
    "print(f\"  Precision = {classification_test['precision']:.3f}\")\n",
    "print(f\"  Recall = {classification_test['recall']:.3f}\")\n",
    "print(f\"  F1-score = {classification_test['f1']:.3f}\")\n",
    "print(f\"  Background distance-correlation(score, mass) = {d_test:.4f}\")\n",
    "\n",
    "# Visualise ROC, confusion matrix, mass sculpting, and ABCD closure vs rejection.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "axes[0].plot(roc_test[\"fpr\"], roc_test[\"tpr\"], label=f\"ROC (AUC={roc_test['auc']:.3f})\")\n",
    "axes[0].set_xlabel(\"Background efficiency ε_B\")\n",
    "axes[0].set_ylabel(\"Signal efficiency ε_S\")\n",
    "axes[0].set_title(\"Receiver Operating Characteristic\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(True)\n",
    "cm = classification_test[\"confusion_matrix\"]\n",
    "im = axes[1].imshow(cm, cmap=\"viridis\")\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels([\"Predicted background\", \"Predicted signal\"])\n",
    "axes[1].set_yticklabels([\"True background\", \"True signal\"])\n",
    "axes[1].set_title(\"Weighted confusion matrix\")\n",
    "for (i, j), value in np.ndenumerate(cm):\n",
    "    axes[1].text(j, i, f\"{value:.0f}\", ha=\"center\", va=\"center\", color=\"white\" if value > cm.max() / 2 else \"black\")\n",
    "fig.colorbar(im, ax=axes[1])\n",
    "nbins = ABCD_HISTOGRAM_BINS\n",
    "mass_range = (m_test.min(), m_test.max())\n",
    "axes[2].hist(\n",
    "    m_test[background], bins=nbins, range=mass_range, weights=w_test[background],\n",
    "    histtype=\"step\", label=\"All background\", linewidth=2\n",
    ")\n",
    "cut = weighted_quantile(s_test[y_test > 0.5], 1 - 0.3, w_test[y_test > 0.5])\n",
    "sel = background & (s_test > cut)\n",
    "axes[2].hist(\n",
    "    m_test[sel], bins=nbins, range=mass_range, weights=w_test[sel],\n",
    "    histtype=\"stepfilled\", alpha=0.4, label=\"Background above 30% signal-eff cut\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Jet mass [GeV]\")\n",
    "axes[2].set_ylabel(\"Weighted events\")\n",
    "axes[2].set_title(\"Mass sculpting diagnostic\")\n",
    "axes[2].legend()\n",
    "rejections = [item[\"background_rejection\"] for item in jsd_test]\n",
    "closures = [item[\"closure_ratio\"] for item in abcd_test[\"per_efficiency\"]]\n",
    "labels_eff = [int(item[\"target_signal_efficiency\"] * 100) for item in abcd_test[\"per_efficiency\"]]\n",
    "axes[3].scatter(rejections, closures, c=labels_eff, cmap=\"plasma\", s=120)\n",
    "for rej, clo, eff in zip(rejections, closures, labels_eff):\n",
    "    axes[3].annotate(f\"{eff}%\", (rej, clo))\n",
    "axes[3].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"±10% closure band\")\n",
    "axes[3].set_xlabel(\"Background rejection 1/ε_B\")\n",
    "axes[3].set_ylabel(\"Closure ratio N_pred/N_true\")\n",
    "axes[3].set_title(\"ABCD closure vs. rejection\")\n",
    "axes[3].set_xscale(\"log\")\n",
    "axes[3].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabulate ABCD and JSD results for the notebook summary.\n",
    "abcd_rows = []\n",
    "for abcd_entry, jsd_entry in zip(abcd_test[\"per_efficiency\"], jsd_test):\n",
    "    eff = int(abcd_entry[\"target_signal_efficiency\"] * 100)\n",
    "    abcd_rows.append({\n",
    "        \"signal_efficiency_percent\": eff,\n",
    "        \"score_cut\": abcd_entry[\"score_cut\"],\n",
    "        \"background_prediction\": abcd_entry[\"predicted_bg\"],\n",
    "        \"closure_ratio\": abcd_entry[\"closure_ratio\"],\n",
    "        \"closure_error_percent\": abcd_entry[\"closure_error_pct\"],\n",
    "        \"pull\": abcd_entry[\"pull\"],\n",
    "        \"transfer_factor_B_over_D\": abcd_entry[\"transfer_factor_B_over_D\"],\n",
    "        \"transfer_factor_C_over_D\": abcd_entry[\"transfer_factor_C_over_D\"],\n",
    "        \"sideband_ratio_B_over_C\": abcd_entry[\"sideband_ratio_B_over_C\"],\n",
    "        \"sideband_stability\": abcd_entry[\"sideband_stability\"],\n",
    "        \"signal_in_A\": abcd_entry[\"signal_in_A\"],\n",
    "        \"observed_total_in_A\": abcd_entry[\"observed_total_in_A\"],\n",
    "        \"asimov_significance\": abcd_entry[\"asimov_significance\"],\n",
    "        \"background_rejection\": jsd_entry[\"background_rejection\"],\n",
    "        \"inverse_jsd\": jsd_entry[\"inverse_jsd\"],\n",
    "    })\n",
    "abcd_df = pd.DataFrame(abcd_rows)\n",
    "abcd_df\n",
    "\n",
    "aggregated_df = pd.json_normalize(abcd_test[\"aggregated\"], sep=\"_\")\n",
    "aggregated_df.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ab8b0",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Persist artefacts\n",
    "\n",
    "Save inference scores and the trained model weights for downstream ABCDisCo or `pyhf` studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd407c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_serializable(obj: Any):\n",
    "    # Convert numpy scalars/arrays and torch tensors to built-in Python types recursively.\n",
    "    if isinstance(obj, (np.floating, np.integer, np.bool_)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if torch.is_tensor(obj):\n",
    "        if obj.ndim == 0:\n",
    "            return obj.item()\n",
    "        return obj.detach().cpu().tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [to_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"score\": s_test,\n",
    "    \"label\": y_test,\n",
    "    \"weight\": w_test,\n",
    "    \"mass\": m_test,\n",
    "})\n",
    "results_df.head()\n",
    "\n",
    "summary_payload = {\n",
    "    \"classification\": {k: (v.tolist() if isinstance(v, np.ndarray) else float(v) if np.isscalar(v) else v) for k, v in classification_test.items() if k != \"confusion_matrix\"},\n",
    "    \"confusion_matrix\": classification_test[\"confusion_matrix\"].tolist(),\n",
    "    \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else float(v)) for k, v in roc_test.items()},\n",
    "    \"abcd\": abcd_test,\n",
    "    \"jsd\": jsd_test,\n",
    "    \"distance_correlation\": d_test,\n",
    "}\n",
    "summary_path = CHECKPOINT_DIR / \"test_evaluation.json\"\n",
    "with open(summary_path, \"w\") as handle:\n",
    "    json.dump(to_serializable(summary_payload), handle, indent=2)\n",
    "print(f\"Saved detailed test summary to {summary_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811ab61",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Extending to Double-DisCo & QML studies\n",
    "\n",
    "- **Recovering the paper-level numbers**: set `FULL_DATASET = True`, increase `EPOCHS` to 200, and sweep `LAMBDA_MASS` in the range 50-400 as in the reference scans.\n",
    "- **Quantum experiments**: switch `BACKEND = \"qml\"`, tune `N_QUBITS`/`QML_LAYERS`, and initialise the PennyLane device with `qml.seed(SEED)` for reproducibility.\n",
    "- **Transition to Double-DisCo**: after validating this notebook, open `ABCDisCo_tutorial.ipynb` for the two-network variant and reuse the saved preprocessing steps to initialise the dual heads.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABCDisCo",
   "language": "python",
   "name": "abcdisco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
