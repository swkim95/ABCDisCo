{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3873f1a8",
   "metadata": {},
   "source": [
    "\n",
    "# ABCDisCo Single-DisCo Tutorial (Torch + optional PennyLane backend)\n",
    "\n",
    "This notebook reproduces the **Single-DisCo** workflow described in [T. Aarrestad *et al.*, *Eur. Phys. J. C* **81**, 1003 (2021), arXiv:2007.14400](https://arxiv.org/abs/2007.14400). It mirrors the reference scripts shipped with this repository so you can validate the mass-decorrelated baseline before moving to the Double-DisCo configuration.\n",
    "\n",
    "> **Mapping to repository scripts**\n",
    "> - Data ingestion and scaling follow `ABCD_topjets_HLF_mD.py` (lines 69-101) together with the dataset helpers in `data_loader.py` (lines 1-63).\n",
    "> - The neural-network head reuses `networks.DNNclassifier` (lines 8-44), while the DisCo penalty mirrors `model.py` (lines 24-86) plus `disco.py` (lines 14-118).\n",
    "> - Evaluation adapts the single-score diagnostics from `evaluation.py` (lines 1-70), including the Jensen-Shannon divergence vs. background rejection scan.\n",
    "\n",
    "The workflow is organised as:\n",
    "\n",
    "1. **Setup & configuration** (Single-DisCo hyperparameters).\n",
    "2. **Data loading and preprocessing** (min-max scaling, feature selection matching `ABCD_topjets_HLF_mD.py`).\n",
    "3. **Model definition** with interchangeable Torch/PennyLane heads.\n",
    "4. **Training** with the DisCo mass decorrelation penalty.\n",
    "5. **Diagnostics & evaluation**: ROC curves, distance correlations, JSD vs. background rejection, and mass sculpting checks.\n",
    "6. **Export** of trained weights and inference scores.\n",
    "\n",
    "> **Datasets**: The repository already ships reduced CMS top-tagging HLF samples (`topsample_*_tau.dat.gz`). You can run this notebook end-to-end without external downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015d9e7",
   "metadata": {},
   "source": [
    "\n",
    "## Environment preparation\n",
    "\n",
    "Run the following cell *once per environment* if you still need to install the CPU builds of PyTorch, PennyLane, and the lightweight analysis stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6df378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment the lines you need)\n",
    "# %pip install numpy pandas scikit-learn matplotlib tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install pennylane pennylane-lightning\n",
    "# %pip install pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe8e3b",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration (mirrors `ABCD_topjets_HLF_mD.py` lines 69-126)\n",
    "\n",
    "We keep the dataset limits, optimiser choices, and DisCo penalty normalisation consistent with the single-network script so that this notebook can reproduce the published baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa3f01a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Standard-library utilities used throughout the tutorial notebook\n",
    "# ---------------------------------------------------------------\n",
    "import gzip  # Read the compressed CMS top-tagging datasets that ship with the repo\n",
    "import json  # Serialize metadata for the checkpoints that we write every epoch\n",
    "import time  # Measure wall-clock runtime so we can report epoch durations\n",
    "from datetime import datetime  # Timestamp saved checkpoints for reproducibility\n",
    "from pathlib import Path  # Work with filesystem paths in a platform-agnostic way\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Scientific Python stack: numerical arrays, data frames, and plots\n",
    "# ---------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PyTorch: tensor library used for both classical and quantum models\n",
    "# ---------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Scikit-Learn: classical metrics used to interpret classifier quality\n",
    "# ---------------------------------------------------------------\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Utilities for nicely formatted progress bars inside notebooks\n",
    "# ---------------------------------------------------------------\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Repository-local modules (they mirror the original training scripts)\n",
    "# ---------------------------------------------------------------\n",
    "import sys\n",
    "project_root = Path.cwd().resolve().parent  # notebook/ -> project root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "from disco import distance_corr_unbiased  # distance correlation regulariser\n",
    "from networks import DNNclassifier  # baseline dense neural network classifier\n",
    "from evaluation import JSDvsR  # Jensen-Shannon vs. rejection diagnostic from the paper\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Optional PennyLane backend for the hybrid quantum notebook\n",
    "# (we keep the import guarded so the classical notebook works without it)\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    raise ImportError(\"PennyLane is required for the QML tutorial. Please run `%pip install pennylane pennylane-lightning`.\")\n",
    "# ---------------------------------------------------------------\n",
    "# Plot styling: use a high-contrast theme that works well in dark/light modes\n",
    "# ---------------------------------------------------------------\n",
    "plt.style.use(\"seaborn-v0_8-talk\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Reproducibility knobs: numpy and torch share a common seed for this tutorial\n",
    "# ---------------------------------------------------------------\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Dataset locations.  We keep everything relative to the project root so the\n",
    "# notebook can be executed from any environment (local, remote, or batch jobs).\n",
    "# ---------------------------------------------------------------\n",
    "DATA_ROOT = project_root\n",
    "RAW_FILES = {\n",
    "    \"train\": DATA_ROOT / \"topsample_train_tau.dat.gz\",\n",
    "    \"val\": DATA_ROOT / \"topsample_val_tau.dat.gz\",\n",
    "    \"test\": DATA_ROOT / \"topsample_test_tau.dat.gz\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Training hyper-parameters.  Flip FULL_DATASET to True for the 5M-event dataset\n",
    "# from the ABCDisCo paper; by default we use a smaller subset for quick iteration.\n",
    "# ---------------------------------------------------------------\n",
    "FULL_DATASET = False\n",
    "EVENT_LIMITS = {\n",
    "    \"train\": 200000 if not FULL_DATASET else None,\n",
    "    \"val\": 200000 if not FULL_DATASET else None,\n",
    "    \"test\": 25000 if not FULL_DATASET else None,\n",
    "}\n",
    "BATCH_SIZE = 10240  # we reduce the batch size slightly to control circuit execution cost\n",
    "EPOCHS = 50 if not FULL_DATASET else 200\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DisCo hyper-parameters.  LAMBDA_MASS rescales the distance-correlation penalty\n",
    "# that encourages the classifier score to be independent of jet mass for background.\n",
    "# ---------------------------------------------------------------\n",
    "# Lambda ramping configuration for better QML training\n",
    "LAMBDA_MASS_START = 20.0      # Start with no penalty\n",
    "LAMBDA_MASS_END = 20.0      # End with full penalty\n",
    "LAMBDA_RAMP_EPOCHS = 0      # Ramp up over first 20 epochs\n",
    "LAMBDA_MASS = 20.0            # Current lambda (will be updated during training)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Evaluation settings for ABCD-related diagnostics.\n",
    "# ---------------------------------------------------------------\n",
    "SCORE_SIGNAL_EFFICIENCIES = (0.1, 0.3, 0.6)  # working points used in the paper\n",
    "ABCD_SIGNAL_WINDOW_QUANTILES = (0.3, 0.7)  # central mass window defines the SR\n",
    "ABCD_HISTOGRAM_BINS = 40  # consistent with the JSD vs. R plots in the paper\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Filesystem layout for per-epoch checkpoints.\n",
    "# ---------------------------------------------------------------\n",
    "CHECKPOINT_DIR = Path.cwd() / \"checkpoints_qml_single_disco\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_TEMPLATE = \"epoch_{epoch:03d}.pth\"\n",
    "RESUME_CHECKPOINT: Optional[Path] = None # set to a file in CHECKPOINT_DIR to resume\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Device selection: automatically use CUDA when available, otherwise fall back\n",
    "# to CPU so the notebook works on any machine (including lightweight VMs).\n",
    "# ---------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# In the QML notebook we default to the quantum backend.\n",
    "BACKEND = \"qml\"\n",
    "N_QUBITS = 11\n",
    "QML_LAYERS = 4\n",
    "QAOA_DEPTH = 1\n",
    "QML_DEVICE = \"default.qubit\"\n",
    "# QML_DEVICE = \"lightning.gpu\"\n",
    "# QML_DEVICE = \"lightning.qubit\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Feature bookkeeping.  The Single-DisCo configuration removes the leading two\n",
    "# observables (mass and pT) from the classifier input, while still keeping them\n",
    "# available for ABCD evaluation and decorrelation studies.\n",
    "# ---------------------------------------------------------------\n",
    "ORIGINAL_FEATURES = [\n",
    "    \"mass\",\n",
    "    \"pt\",\n",
    "    \"tau1_half\",\n",
    "    \"tau2_half\",\n",
    "    \"tau3_half\",\n",
    "    \"tau1\",\n",
    "    \"tau2\",\n",
    "    \"tau3\",\n",
    "    \"tau4\",\n",
    "    \"tau1_sq\",\n",
    "    \"tau2_sq\",\n",
    "    \"tau3_sq\",\n",
    "    \"tau4_sq\",\n",
    "]\n",
    "SINGLE_FEATURE_INDICES = list(range(2, len(ORIGINAL_FEATURES)))\n",
    "FEATURE_NAMES = [ORIGINAL_FEATURES[i] for i in SINGLE_FEATURE_INDICES]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb8ffe",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data loading & scaling (`ABCD_topjets_HLF_mD.py` lines 69-95)\n",
    "\n",
    "The original script concatenates train/validation/test splits, applies a global min-max scaling to all 13 high-level features, and then selects the 11 observables used for the single-network classifier. We reproduce that procedure verbatim while keeping the jet mass available for decorrelation diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6980f93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | events= 200000 | signal=  99996 | background= 100004\n",
      "  val | events= 200000 | signal=  99999 | background= 100001\n",
      " test | events=  25000 | signal=  12505 | background=  12495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Simplified .dat loader (uncompressed) with global min-max scaling and\n",
    "# stratified subsampling. Mirrors ABCD_topjets_HLF_mD_smear.py semantics.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Point to the uncompressed .dat files at the project root\n",
    "RAW_FILES = {\n",
    "    \"train\": DATA_ROOT / \"topsample_train_tau.dat\",\n",
    "    \"val\": DATA_ROOT / \"topsample_val_tau.dat\",\n",
    "    \"test\": DATA_ROOT / \"topsample_test_tau.dat\",\n",
    "}\n",
    "\n",
    "\n",
    "def _load_tau_dat(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing dataset: {path}\")\n",
    "    # The historical files are comma-separated with 15 header lines\n",
    "    data = np.loadtxt(path, delimiter=\",\", skiprows=15)\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(f\"Expected a 2D array, received shape {data.shape}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def _split_labels_and_observables(matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Layout follows the original scripts: first column is label, next 13 columns are observables\n",
    "    if matrix.shape[1] < 14:\n",
    "        raise ValueError(\"Expected at least 14 columns: [label] + 13 observables\")\n",
    "    labels = matrix[:, 0]\n",
    "    observables = matrix[:, 1:14]\n",
    "    return labels.astype(np.float64), observables.astype(np.float64)\n",
    "\n",
    "\n",
    "def _compute_global_minmax(obs_train: np.ndarray, obs_val: np.ndarray, obs_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    all_obs = np.vstack([obs_train, obs_val, obs_test])\n",
    "    obs_min = np.min(all_obs, axis=0)\n",
    "    obs_max = np.max(all_obs, axis=0)\n",
    "    # Avoid division by zero for any constant columns\n",
    "    obs_max = np.where(obs_max == obs_min, obs_min + 1.0, obs_max)\n",
    "    return obs_min, obs_max\n",
    "\n",
    "\n",
    "def _prepare_split(labels: np.ndarray, observables: np.ndarray, obs_min: np.ndarray, obs_max: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Min-max scale all 13 observables globally, then drop mass and pT\n",
    "    scaled = (observables - obs_min) / (obs_max - obs_min)\n",
    "    features = scaled[:, SINGLE_FEATURE_INDICES]\n",
    "    mass = observables[:, 0]  # keep raw mass (not scaled) for ABCD\n",
    "    weights = np.ones_like(labels, dtype=float)\n",
    "    return features.astype(np.float64), labels.astype(np.float64), mass.astype(np.float64), weights.astype(np.float64)\n",
    "\n",
    "\n",
    "def _stratified_subsample(x: np.ndarray, y: np.ndarray, mass: np.ndarray, w: np.ndarray, limit: Optional[int], seed: int = SEED):\n",
    "    if limit is None or limit >= len(y):\n",
    "        return x, y, mass, w\n",
    "    uniq, counts = np.unique(y, return_counts=True)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    target = {int(c): limit * (counts[i] / len(y)) for i, c in enumerate(uniq)}\n",
    "    take_per_class = {cls: int(np.floor(n)) for cls, n in target.items()}\n",
    "    shortfall = limit - sum(take_per_class.values())\n",
    "    if shortfall > 0:\n",
    "        fracs = sorted(((cls, target[cls] - take_per_class[cls]) for cls in take_per_class), key=lambda t: -t[1])\n",
    "        for cls, _ in fracs[:shortfall]:\n",
    "            take_per_class[cls] += 1\n",
    "    sel_parts = []\n",
    "    for cls in uniq.astype(int):\n",
    "        cls_idx = np.flatnonzero(y == cls)\n",
    "        rng.shuffle(cls_idx)\n",
    "        sel_parts.append(cls_idx[: take_per_class[int(cls)]])\n",
    "    sel = np.concatenate(sel_parts, axis=0)\n",
    "    rng.shuffle(sel)\n",
    "    return x[sel], y[sel], mass[sel], w[sel]\n",
    "\n",
    "\n",
    "def load_all_splits(limit_map: Dict[str, Optional[int]] | None = None) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    # Load raw matrices\n",
    "    mats = {name: _load_tau_dat(path) for name, path in RAW_FILES.items()}\n",
    "    # Extract labels and observables\n",
    "    y_train, obs_train = _split_labels_and_observables(mats[\"train\"])\n",
    "    y_val, obs_val = _split_labels_and_observables(mats[\"val\"])\n",
    "    y_test, obs_test = _split_labels_and_observables(mats[\"test\"])\n",
    "    # Compute global min/max across all 13 observables\n",
    "    obs_min, obs_max = _compute_global_minmax(obs_train, obs_val, obs_test)\n",
    "    # Prepare each split\n",
    "    x_train, y_train, mass_train, w_train = _prepare_split(y_train, obs_train, obs_min, obs_max)\n",
    "    x_val, y_val, mass_val, w_val = _prepare_split(y_val, obs_val, obs_min, obs_max)\n",
    "    x_test, y_test, mass_test, w_test = _prepare_split(y_test, obs_test, obs_min, obs_max)\n",
    "    arrays_local = {\n",
    "        \"train\": {\"x\": x_train, \"y\": y_train, \"mass\": mass_train, \"weight\": w_train},\n",
    "        \"val\": {\"x\": x_val, \"y\": y_val, \"mass\": mass_val, \"weight\": w_val},\n",
    "        \"test\": {\"x\": x_test, \"y\": y_test, \"mass\": mass_test, \"weight\": w_test},\n",
    "    }\n",
    "    # Apply stratified subsampling if limits are requested\n",
    "    if limit_map is not None:\n",
    "        for split_name, limit in limit_map.items():\n",
    "            x, y, mass, w = arrays_local[split_name][\"x\"], arrays_local[split_name][\"y\"], arrays_local[split_name][\"mass\"], arrays_local[split_name][\"weight\"]\n",
    "            x, y, mass, w = _stratified_subsample(x, y, mass, w, limit, seed=SEED)\n",
    "            arrays_local[split_name] = {\"x\": x, \"y\": y, \"mass\": mass, \"weight\": w}\n",
    "    return arrays_local\n",
    "\n",
    "\n",
    "# Materialize arrays with requested event limits per split\n",
    "arrays = load_all_splits(EVENT_LIMITS)\n",
    "for split, payload in arrays.items():\n",
    "    print(\n",
    "        f\"{split:>5} | events={len(payload['x']):>7} | \"\n",
    "        f\"signal={int(payload['y'].sum()):>7} | background={len(payload['y']) - int(payload['y'].sum()):>7}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b29a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature names (used as model inputs): ['tau1_half', 'tau2_half', 'tau3_half', 'tau1', 'tau2', 'tau3', 'tau4', 'tau1_sq', 'tau2_sq', 'tau3_sq', 'tau4_sq']\n",
      "\n",
      "=== TRAIN split ===\n",
      "shapes: x=(200000, 11), y=(200000,), mass=(200000,), weight=(200000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 100004, 1: 99996}\n",
      "feature mins (first 5): ['tau1_half=0.0000', 'tau2_half=0.0000', 'tau3_half=0.0000', 'tau1=0.0000', 'tau2=0.0000']\n",
      "feature maxs (first 5): ['tau1_half=0.9649', 'tau2_half=0.9317', 'tau3_half=0.9310', 'tau1=0.9528', 'tau2=0.9123']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.146264   0.130287   0.144693 0.068211 0.049124 0.068906 0.075848 0.030580 0.019499 0.031601 0.037469\n",
      "  0.444672   0.340390   0.379198 0.255357 0.146325 0.197583 0.227737 0.099295 0.064308 0.081549 0.133741\n",
      "  0.468357   0.275429   0.239373 0.435070 0.216425 0.148722 0.167612 0.285281 0.165708 0.053157 0.080731\n",
      "  0.501987   0.360852   0.356830 0.402142 0.218195 0.190631 0.195596 0.199333 0.102198 0.068174 0.099161\n",
      "  0.423520   0.449315   0.521513 0.283291 0.270945 0.386850 0.358371 0.179442 0.134489 0.176014 0.206322\n",
      "first 5 labels: [0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "first 5 masses: [56.0776, 111.091, 175.04, 159.241, 138.829]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "=== VAL split ===\n",
      "shapes: x=(200000, 11), y=(200000,), mass=(200000,), weight=(200000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 100001, 1: 99999}\n",
      "feature mins (first 5): ['tau1_half=0.0000', 'tau2_half=0.0000', 'tau3_half=0.0000', 'tau1=0.0000', 'tau2=0.0000']\n",
      "feature maxs (first 5): ['tau1_half=0.9685', 'tau2_half=1.0000', 'tau3_half=0.9921', 'tau1=0.9522', 'tau2=1.0000']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.239749   0.266475   0.280777 0.105819 0.123083 0.166973 0.180378 0.056135 0.068812 0.110235 0.108732\n",
      "  0.375690   0.347061   0.340145 0.285380 0.227041 0.208534 0.214612 0.212779 0.096752 0.080085 0.087218\n",
      "  0.327403   0.323946   0.403425 0.190502 0.138173 0.199087 0.219356 0.111123 0.048094 0.070529 0.118026\n",
      "  0.506363   0.286112   0.307415 0.392286 0.200477 0.209826 0.165867 0.221070 0.166260 0.089500 0.054537\n",
      "  0.374527   0.328807   0.362351 0.290198 0.209017 0.199716 0.190289 0.226313 0.142002 0.076576 0.120584\n",
      "first 5 labels: [0.0, 1.0, 0.0, 1.0, 1.0]\n",
      "first 5 masses: [86.7765, 164.945, 107.209, 174.184, 170.617]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "=== TEST split ===\n",
      "shapes: x=(25000, 11), y=(25000,), mass=(25000,), weight=(25000,)\n",
      "dtypes: x=float64, y=float64, mass=float64, weight=float64\n",
      "label counts: {0: 12495, 1: 12505}\n",
      "feature mins (first 5): ['tau1_half=0.0010', 'tau2_half=0.0000', 'tau3_half=0.0000', 'tau1=0.0008', 'tau2=0.0000']\n",
      "feature maxs (first 5): ['tau1_half=0.9066', 'tau2_half=0.9353', 'tau3_half=0.8598', 'tau1=0.8794', 'tau2=0.8797']\n",
      "first 5 feature rows:\n",
      " tau1_half  tau2_half  tau3_half     tau1     tau2     tau3     tau4  tau1_sq  tau2_sq  tau3_sq  tau4_sq\n",
      "  0.096666   0.095156   0.115663 0.068728 0.059602 0.079342 0.085361 0.057834 0.038120 0.035336 0.054486\n",
      "  0.408822   0.344779   0.243183 0.291857 0.221798 0.132603 0.146546 0.177008 0.089494 0.050568 0.065449\n",
      "  0.104991   0.097530   0.111998 0.022755 0.025056 0.036905 0.037108 0.005762 0.004560 0.007065 0.012189\n",
      "  0.344384   0.323778   0.318078 0.214480 0.143976 0.172954 0.190888 0.164748 0.048159 0.067160 0.110169\n",
      "  0.229850   0.102914   0.055408 0.088385 0.045263 0.028925 0.026112 0.013346 0.013305 0.014240 0.024161\n",
      "first 5 labels: [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "first 5 masses: [87.833, 148.864, 26.2268, 147.041, 40.8466]\n",
      "first 5 weights: [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Quick sanity checks for data loading: shapes, dtypes, label counts,\n",
    "# per-feature ranges, and first few rows for each split.\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Selected feature names (used as model inputs):\", FEATURE_NAMES)\n",
    "\n",
    "from typing import Dict as _Dict\n",
    "\n",
    "def _summarize_split(name: str, payload: _Dict[str, np.ndarray], feature_names: list[str]) -> None:\n",
    "    x = payload[\"x\"]; y = payload[\"y\"]; mass = payload[\"mass\"]; weight = payload[\"weight\"]\n",
    "    print(f\"\\n=== {name.upper()} split ===\")\n",
    "    print(f\"shapes: x={x.shape}, y={y.shape}, mass={mass.shape}, weight={weight.shape}\")\n",
    "    print(f\"dtypes: x={x.dtype}, y={y.dtype}, mass={mass.dtype}, weight={weight.dtype}\")\n",
    "    uniq, cnt = np.unique(y, return_counts=True)\n",
    "    print(\"label counts:\", {int(u): int(c) for u, c in zip(uniq, cnt)})\n",
    "    # Feature-wise min/max (only print first 5 to keep output compact)\n",
    "    mins = np.min(x, axis=0); maxs = np.max(x, axis=0)\n",
    "    k = min(5, x.shape[1])\n",
    "    print(\"feature mins (first 5):\", [f\"{feature_names[i]}={mins[i]:.4f}\" for i in range(k)])\n",
    "    print(\"feature maxs (first 5):\", [f\"{feature_names[i]}={maxs[i]:.4f}\" for i in range(k)])\n",
    "    # First few rows\n",
    "    df_preview = pd.DataFrame(x[:5], columns=feature_names)\n",
    "    print(\"first 5 feature rows:\")\n",
    "    print(df_preview.to_string(index=False))\n",
    "    print(\"first 5 labels:\", y[:5].tolist())\n",
    "    print(\"first 5 masses:\", mass[:5].tolist())\n",
    "    print(\"first 5 weights:\", weight[:5].tolist())\n",
    "\n",
    "for split_name in (\"train\", \"val\", \"test\"):\n",
    "    _summarize_split(split_name, arrays[split_name], FEATURE_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53adbcb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Torch datasets (`data_loader.py` lines 22-63)\n",
    "\n",
    "We wrap the min-max scaled arrays into PyTorch `Dataset` objects that expose the classifier inputs, labels, per-event weights, and jet masses used in the DisCo penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f014a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: train=20, val=20, test=3\n",
      "train class counts:  {0: 100004, 1: 99996}\n",
      "val class counts:  {0: 100001, 1: 99999}\n",
      "test class counts:  {0: 12495, 1: 12505}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# TorchTopTaggingDataset wraps the numpy arrays into a PyTorch-friendly object.\n",
    "# Objective: expose features, labels, sample weights, and jet masses as tensors\n",
    "#            so mini-batches can flow seamlessly into either the classical or\n",
    "#            quantum components of the hybrid network.\n",
    "# Logic:   * During __init__, we convert numpy arrays to torch.Tensor with\n",
    "#            explicit dtype casting so gradients and GPU transfers behave well.\n",
    "#          * __len__ returns the total number of events, informing DataLoader\n",
    "#            how many batches to draw per epoch.\n",
    "#          * __getitem__ selects one event and returns all four components in\n",
    "#            the order expected by downstream training code.\n",
    "# Terms:   - Sample weight: scales the contribution of each event to averages\n",
    "#            and is crucial for the weighted metrics used in the ABCDisCo\n",
    "#            evaluation workflow.\n",
    "# Expected behaviour: indexing yields a tuple (features, label, weight, mass)\n",
    "#                     where the features live in feature space R^{n_features}.\n",
    "# Reference: the need to pass masses and weights together is highlighted in the\n",
    "#            \"ABCD closure tests\" section of the evaluation document so the\n",
    "#            closure ratio can be recomputed at any stage of training.\n",
    "# ---------------------------------------------------------------------------\n",
    "class TorchTopTaggingDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, mass: np.ndarray, weight: np.ndarray | None = None):\n",
    "        # Convert all inputs to float32 tensors. This matches the precision\n",
    "        # expected by PyTorch optimisers and PennyLane's Torch interface.\n",
    "        self.x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "        self.mass = torch.as_tensor(mass, dtype=torch.float32)\n",
    "        if weight is None:\n",
    "            weight = np.ones_like(y, dtype=np.float32)\n",
    "        self.weight = torch.as_tensor(weight, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Return how many events are available; DataLoader uses this to know\n",
    "        # when an epoch finishes.\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Provide the tuple consumed by the training loop: (features, label,\n",
    "        # per-event weight, jet mass).\n",
    "        return self.x[index], self.y[index], self.weight[index], self.mass[index]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# make_dataloaders creates PyTorch DataLoader objects for train/val/test splits.\n",
    "# Objective: batch and optionally shuffle the dataset while keeping track of\n",
    "#            weights and masses required for ABCD evaluation and DisCo penalty.\n",
    "# Logic:   - Construct a TorchTopTaggingDataset for each split.\n",
    "#          - Instantiate DataLoaders with deterministic batch size and shuffling\n",
    "#            only on the training set (validation/test are left ordered so\n",
    "#            metrics such as ROC curves can be reproduced exactly).\n",
    "# Expected behaviour: downstream code can iterate over each loader and receive\n",
    "#                     batches shaped as `[batch, n_features]` plus aligned labels,\n",
    "#                     weights, and masses.\n",
    "# Reference: consistent batching is necessary to evaluate throughput statistics\n",
    "#            and closure metrics per epoch, as described in the runtime and\n",
    "#            ABCD sections of the evaluation guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def make_dataloaders(arrays: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, DataLoader]:\n",
    "    datasets = {\n",
    "        split: TorchTopTaggingDataset(payload[\"x\"], payload[\"y\"], payload[\"mass\"], payload[\"weight\"])\n",
    "        for split, payload in arrays.items()\n",
    "    }\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True, drop_last=False),\n",
    "        \"val\": DataLoader(datasets[\"val\"], batch_size=BATCH_SIZE, shuffle=False, drop_last=False),\n",
    "        \"test\": DataLoader(datasets[\"test\"], batch_size=BATCH_SIZE, shuffle=False, drop_last=False),\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "# Instantiate loaders so later cells can immediately iterate through the data.\n",
    "loaders = make_dataloaders(arrays)\n",
    "train_loader, val_loader, test_loader = loaders[\"train\"], loaders[\"val\"], loaders[\"test\"]\n",
    "print(f\"Batches per epoch: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\")\n",
    "# Sanity: preserve class ratios after subsampling\n",
    "for name, dl in (\"train\", train_loader), (\"val\", val_loader), (\"test\", test_loader):\n",
    "    ys = np.concatenate([batch[1].numpy() for batch in dl], axis=0)\n",
    "    uniq, cnt = np.unique(ys, return_counts=True)\n",
    "    print(f\"{name} class counts: \", {int(u): int(c) for u, c in zip(uniq, cnt)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22662386",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model backends (`networks.py` lines 8-44)\n",
    "\n",
    "The Single-DisCo setup uses a single `DNNclassifier` head. We also expose an optional PennyLane quantum layer to demonstrate how the architecture can be swapped for a variational quantum classifier without changing the loss logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb324d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PennyLaneSingleDisco(\n",
       "  (compressor): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=32, out_features=11, bias=True)\n",
       "  )\n",
       "  (qlayer): <Quantum Torch Layer: func=circuit>\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=16, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Network definitions: classical head and quantum-enhanced Single DisCo model.\n",
    "# Objective: reproduce the architecture from the ABCDisCo paper while allowing\n",
    "#            PennyLane to supply a variational quantum circuit (VQC) as the\n",
    "#            feature extractor when BACKEND=\"qml\".\n",
    "# Reference: the \"Model introspection\" and \"Quantum resources\" discussions in\n",
    "#            `ref/Evaluating_ABCDisCo_Structured_WithEquations.md` motivate\n",
    "#            documenting layer counts, parameter totals, and how the circuit\n",
    "#            interfaces with classical post-processing.\n",
    "# ---------------------------------------------------------------------------\n",
    "class TorchSingleDisco(nn.Module):\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        # `DNNclassifier` mirrors the fully connected stack used in the\n",
    "        # classical Single-DisCo baseline (ReLU activations, dropout, etc.).\n",
    "        self.head = DNNclassifier(n_features, 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # The head returns logits for two classes; we convert them to a\n",
    "        # background-suppression score via softmax on the signal column.\n",
    "        logits = self.head(x)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "\n",
    "class PennyLaneSingleDisco(nn.Module):\n",
    "    def __init__(self, n_features: int, n_qubits: int = 6, qaoa_depth: int = 1, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "        if not PENNYLANE_AVAILABLE:\n",
    "            raise RuntimeError(\"PennyLane is not installed.\")\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Classical compressor (encoder): single linear map R^{n_features} -> R^{n_qubits}\n",
    "        # followed by LeakyReLU to produce rotation angles for the quantum embedding.\n",
    "        self.compressor = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, n_qubits)\n",
    "        )\n",
    "        \n",
    "        # PennyLane device hosts the variational circuit; default.qubit is a\n",
    "        # noiseless statevector simulator suitable for long training jobs.\n",
    "        qdevice = qml.device(device_name, wires=n_qubits)\n",
    "        \n",
    "        # Strongly entangling layers only (no QAOA weights needed with AngleEmbedding)\n",
    "        weight_shapes = {\n",
    "            \"weights_strong\": (layers, n_qubits, 3),\n",
    "        }\n",
    "\n",
    "        @qml.qnode(qdevice, interface=\"torch\")\n",
    "        def circuit(inputs, weights_strong):\n",
    "            # -----------------------------------------------------------------\n",
    "            # Input handling: support both single-example (1D) and batched (>=2D)\n",
    "            # inputs. We right-pad/truncate to `n_qubits` along the last axis so\n",
    "            # embedding sees tensors shaped (..., n_qubits).\n",
    "            # -----------------------------------------------------------------\n",
    "            take = min(inputs.shape[-1], n_qubits)\n",
    "            if inputs.ndim == 1:\n",
    "                x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "                x_pad[:take] = inputs[:take]\n",
    "            else:\n",
    "                pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "                x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "                x_pad[..., :take] = inputs[..., :take]\n",
    "            \n",
    "            # Scale inputs to appropriate angle range for better gradient flow\n",
    "            angles = torch.pi * x_pad\n",
    "            \n",
    "            # AngleEmbedding encodes classical features via Y-rotations; more robust\n",
    "            # than QAOA for classification tasks and better gradient properties.\n",
    "            qml.templates.AngleEmbedding(angles, wires=range(n_qubits), rotation='Y')\n",
    "            qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "            \n",
    "            # Measure the Pauli-Z expectation on multiple qubits to obtain a multi-dimensional\n",
    "            # feature vector. These observables are the quantum analogue of learned features\n",
    "            # feeding the classical softmax head.\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]  # Use all qubits\n",
    "\n",
    "        # TorchLayer wraps the QNode so gradients propagate with PyTorch autograd.\n",
    "        self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "        \n",
    "        # A lightweight linear classifier turns the quantum features into\n",
    "        # logits for the binary task.\n",
    "        # self.head = nn.Linear(2, 2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Hybrid mapping: compress all n_features -> n_qubits via a small FCN\n",
    "        # to avoid truncation while keeping quantum input width fixed.\n",
    "        angles = self.compressor(x)\n",
    "        q_features = self.qlayer(angles)\n",
    "        logits = self.head(q_features)\n",
    "        score = F.softmax(logits, dim=1)[:, 1]\n",
    "        return logits, score\n",
    "\n",
    "# HERE: deprecated QAOA version\n",
    "# class PennyLaneSingleDisco(nn.Module):\n",
    "#     def __init__(self, n_features: int, n_qubits: int = 6, qaoa_depth: int = 1, layers: int = 2, device_name: str = \"default.qubit\"):\n",
    "#         if not PENNYLANE_AVAILABLE:\n",
    "#             raise RuntimeError(\"PennyLane is not installed.\")\n",
    "#         super().__init__()\n",
    "#         self.n_qubits = n_qubits\n",
    "#         self.n_features = n_features\n",
    "#         # Classical compressor (encoder): single linear map R^{n_features} -> R^{n_qubits}\n",
    "#         # followed by ReLU to produce rotation angles for the quantum embedding.\n",
    "#         # self.compressor = nn.Sequential(\n",
    "#         #     nn.Linear(n_features, n_qubits),\n",
    "#         #     nn.LeakyReLU(),\n",
    "#         # )\n",
    "#         # PennyLane device hosts the variational circuit; default.qubit is a\n",
    "#         # noiseless statevector simulator suitable for long training jobs.\n",
    "#         qdevice = qml.device(device_name, wires=n_qubits)\n",
    "#         # One QAOA embedding layer + `layers` strongly entangling layers.\n",
    "#         weight_shapes = {\n",
    "#             \"weights_qaoa\": (qaoa_depth, 2 * n_qubits),\n",
    "#             \"weights_strong\": (layers, n_qubits, 3),\n",
    "#         }\n",
    "\n",
    "#         @qml.qnode(qdevice, interface=\"torch\")\n",
    "#         def circuit(inputs, weights_qaoa, weights_strong):\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # Input handling: support both single-example (1D) and batched (>=2D)\n",
    "#             # inputs. We right-pad/truncate to `n_qubits` along the last axis so\n",
    "#             # embedding sees tensors shaped (..., n_qubits).\n",
    "#             # -----------------------------------------------------------------\n",
    "#             take = min(inputs.shape[-1], n_qubits)\n",
    "#             if inputs.ndim == 1:\n",
    "#                 x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "#                 x_pad[:take] = inputs[:take]\n",
    "#             else:\n",
    "#                 pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "#                 x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "#                 x_pad[..., :take] = inputs[..., :take]\n",
    "#             # QAOAEmbedding encodes classical features via cost layers; a single\n",
    "#             # layer is followed by multiple strongly entangling layers to improve\n",
    "#             # expressivity.\n",
    "#             qml.templates.QAOAEmbedding(x_pad, weights=weights_qaoa, wires=range(n_qubits))\n",
    "#             qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "#             # Measure the Pauli-Z expectation on two qubits to obtain a 2D\n",
    "#             # feature vector. These observables are the quantum analogue of\n",
    "#             # learned features feeding the classical softmax head.\n",
    "\n",
    "#             # return qml.expval(qml.PauliZ(0))\n",
    "#             return [qml.expval(qml.PauliZ(i)) for i in range(2)]\n",
    "\n",
    "#         # TorchLayer wraps the QNode so gradients propagate with PyTorch autograd.\n",
    "#         self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "#         # A lightweight linear classifier turns the 2D quantum features into\n",
    "#         # logits for the binary task.\n",
    "#         self.head = nn.Linear(2, 2)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         # Hybrid mapping: compress all n_features -> n_qubits via a small FCN\n",
    "#         # to avoid truncation while keeping quantum input width fixed.\n",
    "#         # angles = self.compressor(x)\n",
    "#         # q_features = self.qlayer(angles)\n",
    "#         q_features = self.qlayer(x)\n",
    "#         logits = self.head(q_features)\n",
    "#         score = F.softmax(logits, dim=1)[:, 1]\n",
    "#         # Apply sigmoid to convert the single quantum expectation value to a probability score\n",
    "#         # Since q_features is a scalar (single expectation value from qml.expval(qml.PauliZ(0))),\n",
    "#         # we use sigmoid instead of softmax to map the range [-1, 1] to [0, 1]\n",
    "#         # logits = 1.\n",
    "#         # score = torch.sigmoid(q_features)\n",
    "#         return logits, score\n",
    "\n",
    "\n",
    "def build_model(n_features: int) -> nn.Module:\n",
    "    \"\"\"Factory: chooses classical or quantum architecture based on BACKEND.\"\"\"\n",
    "    if BACKEND == \"qml\":\n",
    "        # Use 3 StronglyEntanglingLayers as requested.\n",
    "        model = PennyLaneSingleDisco(n_features, n_qubits=N_QUBITS, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "        # model = PennyLaneSingleDisco(n_features, n_qubits=N_QUBITS, qaoa_depth=QAOA_DEPTH, layers=QML_LAYERS, device_name=QML_DEVICE)\n",
    "    else:\n",
    "        model = TorchSingleDisco(n_features)\n",
    "    # Expected behaviour: the returned module is already moved to the selected\n",
    "    # device (CPU or CUDA) so later training code can call `.to(DEVICE)` safely.\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "model = build_model(len(FEATURE_NAMES))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcc2abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Learning rate: 0.001\n",
      "Total trainable parameters: 1105\n",
      "Quantum layer parameters: 132 (per layer = N/A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>count</th>\n",
       "      <th>trainable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>compressor.0.weight</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>compressor.0.bias</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>compressor.2.weight</td>\n",
       "      <td>352</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compressor.2.bias</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qlayer.weights_strong</td>\n",
       "      <td>132</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>head.0.weight</td>\n",
       "      <td>176</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>head.0.bias</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>head.2.weight</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>head.2.bias</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               parameter  count  trainable\n",
       "0    compressor.0.weight    352       True\n",
       "1      compressor.0.bias     32       True\n",
       "2    compressor.2.weight    352       True\n",
       "3      compressor.2.bias     11       True\n",
       "4  qlayer.weights_strong    132       True\n",
       "5          head.0.weight    176       True\n",
       "6            head.0.bias     16       True\n",
       "7          head.2.weight     32       True\n",
       "8            head.2.bias      2       True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# describe_qml_model summarises parameter counts and optimiser settings.\n",
    "# Objective: provide a transparent ledger of learnable parameters, matching the\n",
    "#            \"Model introspection\" requirement from the evaluation guide so we\n",
    "#            can compare classical vs quantum capacity.\n",
    "# Logic: iterate over named_parameters(), collect counts, and print optimiser\n",
    "#        metadata (learning rate, algorithm family). The returned DataFrame is\n",
    "#        easy to display in-tabular form for notebook readers.\n",
    "# Expected behaviour: calling this function prints human-readable statements and\n",
    "#                     returns a DataFrame where each row corresponds to one\n",
    "#                     parameter tensor.\n",
    "# Reference: the evaluation document stresses reporting trainable degrees of\n",
    "#            freedom when comparing architectures (see the \"Model details\"\n",
    "#            bullet list in `ref/Evaluating_ABCDisCo_Structured_WithEquations.md`).\n",
    "# ---------------------------------------------------------------------------\n",
    "def describe_qml_model(model: nn.Module, optimizer: torch.optim.Optimizer) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        count = int(param.numel())\n",
    "        trainable = param.requires_grad\n",
    "        total_params += count if trainable else 0\n",
    "        rows.append({\n",
    "            \"parameter\": name,\n",
    "            \"count\": count,\n",
    "            \"trainable\": trainable,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"Optimizer:\", optimizer.__class__.__name__)\n",
    "    print(\"Learning rate:\", optimizer.param_groups[0][\"lr\"])\n",
    "    print(\"Total trainable parameters:\", total_params)\n",
    "    if hasattr(model, \"qlayer\"):\n",
    "        q_params = sum(p.numel() for p in model.qlayer.parameters())\n",
    "        per_layer = model.qlayer.weights.shape[1] if hasattr(model.qlayer, \"weights\") else \"N/A\"\n",
    "        print(f\"Quantum layer parameters: {q_params} (per layer = {per_layer})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "qml_architecture_df = describe_qml_model(model, optimizer)\n",
    "qml_architecture_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c9d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational circuit for zero input:\n",
      "TorchLayer parameters not found for drawing.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualise the variational quantum circuit to inspect ansatz structure.\n",
    "# Reference: the evaluation guide recommends presenting circuit diagrams when\n",
    "# discussing quantum resources so collaborators can relate expressivity to\n",
    "# physics performance.\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    # Use PennyLane's ASCII drawer to render the circuit with the current\n",
    "    # trainable weights. Passing a zero vector illustrates the gate layout\n",
    "    # independent of specific input features.\n",
    "    probe = torch.zeros((model.n_qubits,), dtype=torch.float32)\n",
    "    drawer = qml.draw(model.qlayer.qnode)\n",
    "    print(\"Variational circuit for zero input:\")\n",
    "    # Collect TorchLayer parameters expected by the QNode\n",
    "    params = {name: p.detach() for name, p in model.qlayer.named_parameters()}\n",
    "    w_qaoa = params.get(\"weights_qaoa\")\n",
    "    w_strong = params.get(\"weights_strong\")\n",
    "    if w_qaoa is not None and w_strong is not None:\n",
    "        print(drawer(probe, w_qaoa, w_strong))\n",
    "    else:\n",
    "        print(\"TorchLayer parameters not found for drawing.\")\n",
    "else:\n",
    "    print(\"Quantum circuit visualisation is only available when BACKEND='qml'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6852f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to render circuit image with qml.draw_mpl: 'TorchLayer' object has no attribute 'weights'\n",
      "If this persists, ensure matplotlib is installed and update PennyLane.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3b. Image-based QML circuit visualization (PennyLane draw_mpl)\n",
    "# Objective: render the variational circuit as an image and display it inline.\n",
    "# Logic: create a zero-input probe, use current trainable weights, call\n",
    "#        qml.draw_mpl on the QNode inside the TorchLayer, and save a PNG copy.\n",
    "# Expected behaviour: show a figure in the notebook and write to disk for reuse.\n",
    "# ---------------------------------------------------------------------------\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        import matplotlib.pyplot as plt  # already imported globally; kept for clarity\n",
    "\n",
    "        # Build a simple probe input and move weights to CPU for drawing\n",
    "        probe = torch.zeros((model.n_qubits,), dtype=torch.float32)\n",
    "        weights_cpu = model.qlayer.weights.detach().cpu()\n",
    "\n",
    "        fig, ax = qml.draw_mpl(model.qlayer.qnode)(probe, weights_cpu)\n",
    "        display(fig)\n",
    "\n",
    "        vis_dir = CHECKPOINT_DIR / \"visualizations\"\n",
    "        vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = vis_dir / \"qml_circuit.png\"\n",
    "        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        print(f\"Saved circuit image to {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to render circuit image with qml.draw_mpl:\", e)\n",
    "        print(\"If this persists, ensure matplotlib is installed and update PennyLane.\")\n",
    "else:\n",
    "    print(\"Circuit image is available only when BACKEND='qml'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "111044ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# get_current_lambda computes the current lambda value based on epoch for ramping.\n",
    "# Objective: gradually increase the decorrelation penalty to help QML models learn\n",
    "#            classification first, then decorrelation.\n",
    "# Logic: linear ramp from LAMBDA_MASS_START to LAMBDA_MASS_END over LAMBDA_RAMP_EPOCHS.\n",
    "# Expected behaviour: returns current lambda value for the given epoch.\n",
    "# ---------------------------------------------------------------------------\n",
    "def get_current_lambda(epoch: int) -> float:\n",
    "    if epoch < LAMBDA_RAMP_EPOCHS:\n",
    "        # Linear ramp from start to end over ramp epochs\n",
    "        progress = epoch / LAMBDA_RAMP_EPOCHS\n",
    "        return LAMBDA_MASS_START + progress * (LAMBDA_MASS_END - LAMBDA_MASS_START)\n",
    "    else:\n",
    "        # Use full penalty after ramp period\n",
    "        return LAMBDA_MASS_END\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# distance_corr_safe computes the distance correlation (DisCo) penalty.\n",
    "# Objective: reproduce the decorrelation regulariser described in the ABCDisCo\n",
    "#            paper so the classifier score becomes independent of jet mass on\n",
    "#            background events, satisfying the ABCD assumption.\n",
    "# Logic: Guard against tiny batches by returning zero when fewer than three\n",
    "#        events are present, then normalise the provided weights so they sum to\n",
    "#        the batch size (matching the convention in `disco.distance_corr_unbiased`).\n",
    "# Terms: distance correlation measures any statistical dependence between two\n",
    "#        variables; a value near zero signals independence. This is the core of\n",
    "#        the \"Distance Correlation penalty\" highlighted in the evaluation guide.\n",
    "# Expected behaviour: returns a torch scalar suitable for backpropagation.\n",
    "# Reference: see the \"Distance correlation penalty\" subsection in\n",
    "#            `ref/Evaluating_ABCDisCo_Structured_WithEquations.md` where the\n",
    "#            DisCo loss is tied to improved ABCD closure.\n",
    "# ---------------------------------------------------------------------------\n",
    "def distance_corr_safe(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
    "    if x.numel() <= 2 or y.numel() <= 2:\n",
    "        return torch.zeros(1, device=x.device, dtype=x.dtype)\n",
    "    normed = weight / (weight.sum() + 1e-12) * len(weight)\n",
    "    return distance_corr_unbiased(x, y, normed, power=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_weighted_classification_stats summarises confusion-matrix entries.\n",
    "# Objective: obtain weighted counts of true/false positives/negatives so we can\n",
    "#            report accuracy, precision, recall, and F1 per epoch.\n",
    "# Logic: apply a threshold (default 0.5) to scores, multiply boolean masks by\n",
    "#        sample weights, and accumulate sums.\n",
    "# Terms: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean.\n",
    "# Expected behaviour: returns a dictionary of scalar floats ready for logging.\n",
    "# Reference: the evaluation guide explicitly asks for classification metrics\n",
    "#            alongside ABCD closure to interpret signal/background separation.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_weighted_classification_stats(labels: torch.Tensor, scores: torch.Tensor, weights: torch.Tensor, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    preds = (scores >= threshold).to(labels.dtype)\n",
    "    w = weights\n",
    "    tp = torch.sum(w * (preds == 1) * (labels == 1))\n",
    "    tn = torch.sum(w * (preds == 0) * (labels == 0))\n",
    "    fp = torch.sum(w * (preds == 1) * (labels == 0))\n",
    "    fn = torch.sum(w * (preds == 0) * (labels == 1))\n",
    "    total = tp + tn + fp + fn + 1e-12\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    return {\n",
    "        \"tp\": tp.item(),\n",
    "        \"tn\": tn.item(),\n",
    "        \"fp\": fp.item(),\n",
    "        \"fn\": fn.item(),\n",
    "        \"accuracy\": accuracy.item(),\n",
    "        \"precision\": precision.item(),\n",
    "        \"recall\": recall.item(),\n",
    "        \"f1\": f1.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_losses packages the BCE classification loss and optional DisCo penalty.\n",
    "# Objective: provide a single function for forward passes that also records the\n",
    "#            metrics needed for per-epoch logging and checkpoint metadata.\n",
    "# Logic: move tensors to DEVICE, obtain logits and score, compute weighted\n",
    "#        binary cross-entropy, then append decorrelation loss for background\n",
    "#        events when `LAMBDA_MASS` > 0.\n",
    "# Expected behaviour: returns a loss tensor for backpropagation and a metrics\n",
    "#                     dictionary containing loss and classification statistics.\n",
    "# Reference: ties together the classification quality and decorrelation control\n",
    "#            emphasised in Sections \"ROC/AUC\" and \"Distance correlation\" of the\n",
    "#            evaluation document.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    weights = weights.to(DEVICE)\n",
    "    masses = masses.to(DEVICE)\n",
    "\n",
    "    logits, score = model(features)\n",
    "    loss_cls = F.binary_cross_entropy(score, labels, weight=weights)\n",
    "    loss = loss_cls\n",
    "\n",
    "    stats = compute_weighted_classification_stats(labels, score, weights)\n",
    "    metrics = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(stats[\"accuracy\"]),\n",
    "        \"precision\": float(stats[\"precision\"]),\n",
    "        \"recall\": float(stats[\"recall\"]),\n",
    "        \"f1\": float(stats[\"f1\"]),\n",
    "    }\n",
    "\n",
    "    background = labels < 0.5\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd8058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# weighted_quantile finds score thresholds at fixed signal efficiencies.\n",
    "# Objective: implement quantile selection that respects per-event weights,\n",
    "#            matching the definition of efficiency in collider analyses.\n",
    "# Logic: sort scores, build a weighted cumulative distribution, and interpolate\n",
    "#        the desired quantile.\n",
    "# Terms: quantile `q` solves CDF(q) = quantile; weights rescale contributions.\n",
    "# Expected behaviour: returns a scalar threshold such that the weighted fraction\n",
    "#                     of signal events above the threshold equals the target.\n",
    "# Reference: required for the ABCD closure scans described in the evaluation\n",
    "#            guide when constructing score cuts at specified signal efficiencies.\n",
    "# ---------------------------------------------------------------------------\n",
    "def weighted_quantile(values: np.ndarray, quantile: float, sample_weight: np.ndarray) -> float:\n",
    "    order = np.argsort(values)\n",
    "    values = values[order]\n",
    "    weights = sample_weight[order]\n",
    "    cumulative = np.cumsum(weights) - 0.5 * weights\n",
    "    cumulative /= weights.sum()\n",
    "    return float(np.interp(quantile, cumulative, values))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_epoch_classification_metrics records weighted accuracy-style metrics.\n",
    "# Objective: mirror the confusion-matrix-based diagnostics emphasised in the\n",
    "#            evaluation document for understanding classification quality.\n",
    "# Logic: apply a 0.5 threshold, evaluate sklearn metrics with weights, and store\n",
    "#        the confusion matrix for later visualisation.\n",
    "# Expected behaviour: returns a dictionary with scalar metrics and a matrix.\n",
    "# Reference: see the \"Classification metrics\" subsection in the evaluation guide\n",
    "#            for why accuracy/precision/recall/F1 complement ROC analysis.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_epoch_classification_metrics(scores: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    preds = (scores >= 0.5).astype(int)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds, sample_weight=weights),\n",
    "        \"precision\": precision_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, sample_weight=weights, zero_division=0),\n",
    "    }\n",
    "    cm = confusion_matrix(labels, preds, sample_weight=weights)\n",
    "    metrics[\"confusion_matrix\"] = cm.astype(float)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_roc_diagnostics traces the ROC curve and background efficiency targets.\n",
    "# Objective: capture the full trade-off between signal efficiency and background\n",
    "#            rejection, including specific working points used later in ABCD and\n",
    "#            JensenShannon analyses.\n",
    "# Logic: leverage sklearn.roc_curve with weights, compute area under the curve,\n",
    "#        and interpolate background efficiencies at user-requested signal effs.\n",
    "# Expected behaviour: dictionary containing arrays (fpr, tpr, thresholds) and\n",
    "#                     scalars (AUC, background efficiencies at target points).\n",
    "# Reference: the evaluation guide's ROC/AUC discussion and Fig. 3-style plots.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_roc_diagnostics(scores: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, sample_weight=weights)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    diagnostics = {\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": thresholds, \"auc\": roc_auc}\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        diagnostics[f\"background_eff_at_{int(target*100)}pct_sig\"] = float(np.interp(target, tpr, fpr))\n",
    "    return diagnostics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_abcd_statistics evaluates closure, prediction uncertainty, and pulls.\n",
    "# Objective: implement Eq. (1) and related diagnostics from the evaluation guide\n",
    "#            to validate the ABCD method using the model's score as the x-axis,\n",
    "#            and produce aggregated summaries for transfer factors, sideband\n",
    "#            stability, and Asimov significance.\n",
    "# Logic: define a signal mass window, split events into A/B/C/D based on score\n",
    "#        cuts and mass window membership, then compute predicted background,\n",
    "#        closure ratio, percentage error, statistical pull, transfer factors,\n",
    "#        sideband stability tests, and Asimov significance.\n",
    "# Terms: regions A/B/C/D follow the ABCD method; closure ratio = N_pred/N_true;\n",
    "#        pull = (observed - predicted) / sigma_combined.\n",
    "# Expected behaviour: returns a dictionary with the mass window, per-efficiency\n",
    "#                     summaries used for plots and checkpoint metadata, and an\n",
    "#                     `aggregated` block containing dataset-level statistics.\n",
    "# Reference: \"ABCD closure tests\" and \"Pull distributions\" sections in the guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_abcd_statistics(scores: np.ndarray, masses: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> Dict[str, Any]:\n",
    "    signal_mask = labels > 0.5\n",
    "    background_mask = ~signal_mask\n",
    "    if signal_mask.sum() == 0 or background_mask.sum() == 0:\n",
    "        raise ValueError(\"Need both classes present to evaluate ABCD metrics\")\n",
    "    mass_low = np.quantile(masses[signal_mask], ABCD_SIGNAL_WINDOW_QUANTILES[0])\n",
    "    mass_high = np.quantile(masses[signal_mask], ABCD_SIGNAL_WINDOW_QUANTILES[1])\n",
    "    in_signal_mass = (masses >= mass_low) & (masses <= mass_high)\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    def weighted_sum(mask: np.ndarray) -> float:\n",
    "        return float(np.sum(weights[mask]))\n",
    "\n",
    "    def asimov_significance(signal_yield: float, background_yield: float) -> float:\n",
    "        if background_yield <= 0:\n",
    "            return float(\"nan\")\n",
    "        if signal_yield <= 0:\n",
    "            return 0.0\n",
    "        term = (signal_yield + background_yield) * np.log1p(signal_yield / (background_yield + eps)) - signal_yield\n",
    "        return float(np.sqrt(max(0.0, 2.0 * term)))\n",
    "\n",
    "    results: Dict[str, Any] = {\"mass_window\": (mass_low, mass_high), \"per_efficiency\": [], \"aggregated\": {}}\n",
    "\n",
    "    closures, pulls = [], []\n",
    "    tf_bd_values, tf_cd_values = [], []\n",
    "    sideband_ratio_values, sideband_stability_values = [], []\n",
    "    asimov_values = []\n",
    "\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        score_cut = weighted_quantile(scores[signal_mask], 1 - target, weights[signal_mask])\n",
    "        pass_score = scores >= score_cut\n",
    "        region_A = pass_score & in_signal_mass\n",
    "        region_B = pass_score & (~in_signal_mass)\n",
    "        region_C = (~pass_score) & in_signal_mass\n",
    "        region_D = (~pass_score) & (~in_signal_mass)\n",
    "\n",
    "        A_bg = weighted_sum(background_mask & region_A)\n",
    "        B_bg = weighted_sum(background_mask & region_B)\n",
    "        C_bg = weighted_sum(background_mask & region_C)\n",
    "        D_bg = weighted_sum(background_mask & region_D)\n",
    "        A_sig = weighted_sum(signal_mask & region_A)\n",
    "        observed_total = A_bg + A_sig\n",
    "\n",
    "        prediction = B_bg * C_bg / (D_bg + eps)\n",
    "        closure_ratio = prediction / (A_bg + eps)\n",
    "        sigma_pred = prediction * np.sqrt(1 / (B_bg + eps) + 1 / (C_bg + eps) + 1 / (D_bg + eps))\n",
    "        sigma_obs = np.sqrt(A_bg + eps)\n",
    "        pull = (A_bg - prediction) / np.sqrt(sigma_pred**2 + sigma_obs**2)\n",
    "\n",
    "        tf_bd = B_bg / (D_bg + eps)\n",
    "        tf_cd = C_bg / (D_bg + eps)\n",
    "        sideband_ratio = B_bg / (C_bg + eps)\n",
    "        sideband_stability = (B_bg - C_bg) / (B_bg + C_bg + eps)\n",
    "        asimov = asimov_significance(A_sig, prediction)\n",
    "\n",
    "        results[\"per_efficiency\"].append({\n",
    "            \"target_signal_efficiency\": target,\n",
    "            \"score_cut\": score_cut,\n",
    "            \"A_bg\": A_bg,\n",
    "            \"B_bg\": B_bg,\n",
    "            \"C_bg\": C_bg,\n",
    "            \"D_bg\": D_bg,\n",
    "            \"predicted_bg\": prediction,\n",
    "            \"closure_ratio\": closure_ratio,\n",
    "            \"closure_error_pct\": (closure_ratio - 1.0) * 100.0,\n",
    "            \"pull\": pull,\n",
    "            \"transfer_factor_B_over_D\": tf_bd,\n",
    "            \"transfer_factor_C_over_D\": tf_cd,\n",
    "            \"sideband_ratio_B_over_C\": sideband_ratio,\n",
    "            \"sideband_stability\": sideband_stability,\n",
    "            \"signal_in_A\": A_sig,\n",
    "            \"observed_total_in_A\": observed_total,\n",
    "            \"asimov_significance\": asimov,\n",
    "        })\n",
    "\n",
    "        closures.append(closure_ratio)\n",
    "        pulls.append(pull)\n",
    "        tf_bd_values.append(tf_bd)\n",
    "        tf_cd_values.append(tf_cd)\n",
    "        sideband_ratio_values.append(sideband_ratio)\n",
    "        sideband_stability_values.append(sideband_stability)\n",
    "        asimov_values.append(asimov)\n",
    "\n",
    "    def summarise(values: List[float]) -> Dict[str, float]:\n",
    "        if len(values) == 0:\n",
    "            return {\"mean\": float(\"nan\"), \"std\": float(\"nan\"), \"min\": float(\"nan\"), \"max\": float(\"nan\"), \"median\": float(\"nan\")}\n",
    "        arr = np.asarray(values, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr, ddof=0)),\n",
    "            \"min\": float(np.min(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "            \"median\": float(np.median(arr)),\n",
    "        }\n",
    "\n",
    "    pull_stats = summarise(pulls)\n",
    "    pull_stats[\"rms\"] = float(np.sqrt(np.mean(np.square(np.asarray(pulls, dtype=float))))) if pulls else float(\"nan\")\n",
    "\n",
    "    results[\"aggregated\"] = {\n",
    "        \"closure_ratio\": summarise(closures),\n",
    "        \"pull\": pull_stats,\n",
    "        \"transfer_factor_B_over_D\": summarise(tf_bd_values),\n",
    "        \"transfer_factor_C_over_D\": summarise(tf_cd_values),\n",
    "        \"sideband_ratio_B_over_C\": summarise(sideband_ratio_values),\n",
    "        \"sideband_stability\": summarise(sideband_stability_values),\n",
    "        \"asimov_significance\": summarise(asimov_values),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# compute_jsd_summary interfaces with the existing JSD vs rejection utility.\n",
    "# Objective: monitor how well the classifier avoids mass sculpting by relating\n",
    "#            background rejection to the inverse JensenShannon divergence.\n",
    "# Logic: for each target signal efficiency, call `evaluation.JSDvsR` (which wraps\n",
    "#        the physics-inspired histogram comparison) and store the rejection +\n",
    "#        inverse JSD pair.\n",
    "# Terms: JensenShannon divergence measures shape agreement between mass\n",
    "#        distributions; high inverse JSD means better agreement (less sculpting).\n",
    "# Expected behaviour: list of dictionaries keyed by target efficiency.\n",
    "# Reference: \"Mass sculpting and information-theoretic metrics\" in the guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_jsd_summary(scores: np.ndarray, masses: np.ndarray, labels: np.ndarray, weights: np.ndarray) -> List[Dict[str, float]]:\n",
    "    background = labels < 0.5\n",
    "    signal = labels > 0.5\n",
    "    summary = []\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        rejection, inv_jsd = JSDvsR(\n",
    "            sigscore=scores[signal],\n",
    "            bgscore=scores[background],\n",
    "            bgmass=masses[background],\n",
    "            sigweights=weights[signal],\n",
    "            bgweights=weights[background],\n",
    "            sigeff=int(target * 100),\n",
    "            nbins=ABCD_HISTOGRAM_BINS,\n",
    "            minmass=float(masses.min()),\n",
    "            maxmass=float(masses.max()),\n",
    "        )\n",
    "        summary.append({\n",
    "            \"target_signal_efficiency\": target,\n",
    "            \"background_rejection\": rejection,\n",
    "            \"inverse_jsd\": inv_jsd,\n",
    "        })\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# save_checkpoint persists model/optimizer states and rich validation metadata.\n",
    "# Objective: guarantee resumable training under runtime limits by writing\n",
    "#            per-epoch files that store metrics, history, and cached arrays.\n",
    "# Logic: package model weights, optimiser buffers, history so far, and extra\n",
    "#        arrays needed to resume evaluation. Filenames follow \"epoch_XXX.pth\" for clarity.\n",
    "# Expected behaviour: writes a `.pth` file to CHECKPOINT_DIR and prints the path.\n",
    "# Reference: aligns with the runtime monitoring requirement in the evaluation guide.\n",
    "# ---------------------------------------------------------------------------\n",
    "def save_checkpoint(epoch: int, model: nn.Module, optimizer: torch.optim.Optimizer, history: List[Dict[str, Any]], train_record: Dict[str, Any], val_record: Dict[str, Any], extra: Dict[str, Any]) -> Path:\n",
    "    payload = {\n",
    "        \"epoch\": epoch,\n",
    "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"history\": history,\n",
    "        \"train_record\": train_record,\n",
    "        \"val_record\": val_record,\n",
    "    }\n",
    "    payload.update(extra)\n",
    "    path = CHECKPOINT_DIR / CHECKPOINT_TEMPLATE.format(epoch=epoch)\n",
    "    torch.save(payload, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# load_checkpoint restores model/optimizer states and returns archived metadata.\n",
    "# Objective: allow fast resumption or retrospective analysis using the stored\n",
    "#            arrays and history.\n",
    "# Expected behaviour: loads the `.pth` file on DEVICE, restores weights, and\n",
    "#                     returns the payload dictionary for further processing.\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_checkpoint(path: Path, model: nn.Module, optimizer: torch.optim.Optimizer) -> Dict[str, Any]:\n",
    "    payload = torch.load(path, map_location=DEVICE)\n",
    "    model.load_state_dict(payload[\"model_state\"])\n",
    "    optimizer.load_state_dict(payload[\"optimizer_state\"])\n",
    "    print(f\"Loaded checkpoint from {path}\")\n",
    "    return payload\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# list_available_checkpoints enumerates saved epochs for easy resumption.\n",
    "# Expected behaviour: prints filenames such as `epoch_005.pth` and returns a\n",
    "#                     sorted list so notebook users can grab the latest checkpoint.\n",
    "# ---------------------------------------------------------------------------\n",
    "def list_available_checkpoints() -> List[Path]:\n",
    "    paths = sorted(CHECKPOINT_DIR.glob(\"epoch_*.pth\"))\n",
    "    for p in paths:\n",
    "        print(p.name)\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78201ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_one_epoch function with lambda parameter\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    duration = time.perf_counter() - start\n",
    "    results = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    results.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a33af9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Loss function with DisCo penalty (`model.py` lines 24-86 & `disco.py`)\n",
    "\n",
    "We compute the weighted binary cross-entropy loss and add the unbiased distance-correlation penalty between the classifier score and jet mass on background events, following the original `train_model(..., decorr_mode='dist_unbiased')` implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77a02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank to avoid redefining helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692b7844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda ramping schedule:\n",
      "  Start: 20.0\n",
      "  End: 20.0\n",
      "  Ramp over: 0 epochs\n",
      "  Final epochs (0-49): constant at 20.0\n"
     ]
    }
   ],
   "source": [
    "# Visualize the lambda ramping schedule\n",
    "epochs = range(EPOCHS)\n",
    "lambdas = [get_current_lambda(epoch) for epoch in epochs]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lambdas, 'b-', linewidth=2, label='Lambda Mass Schedule')\n",
    "plt.axvline(x=LAMBDA_RAMP_EPOCHS, color='r', linestyle='--', alpha=0.7, label=f'Ramp End (Epoch {LAMBDA_RAMP_EPOCHS})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Lambda Mass')\n",
    "plt.title('DisCo Penalty Ramping Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Lambda ramping schedule:\")\n",
    "print(f\"  Start: {LAMBDA_MASS_START}\")\n",
    "print(f\"  End: {LAMBDA_MASS_END}\")\n",
    "print(f\"  Ramp over: {LAMBDA_RAMP_EPOCHS} epochs\")\n",
    "print(f\"  Final epochs ({LAMBDA_RAMP_EPOCHS}-{EPOCHS-1}): constant at {LAMBDA_MASS_END}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9d446a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training loop (`model.py` lines 88-170)\n",
    "\n",
    "We adapt the original `train`/`val` helpers to work seamlessly on CPU/GPU and to log the DisCo decorrelation term alongside the classification loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67781dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# train_one_epoch performs a full pass over the training DataLoader.\n",
    "# Objective: accumulate gradients, update model parameters, and track runtime\n",
    "#            metrics (examples/sec, iterations/sec) required for the runtime log\n",
    "#            requested in the evaluation plan.\n",
    "# Logic: iterate over batches, compute losses, backpropagate, and average the\n",
    "#        collected metrics dictionary.\n",
    "# Expected behaviour: returns a dictionary containing mean losses/metrics and\n",
    "#                     throughput information for the epoch.\n",
    "# ---------------------------------------------------------------------------\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for key, value in metrics.items():\n",
    "            agg.setdefault(key, []).append(value)\n",
    "    duration = time.perf_counter() - start\n",
    "    results = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    results.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# evaluate mirrors train_one_epoch but without gradient updates.\n",
    "# Objective: compute validation/test metrics and collect raw arrays for ROC/ABCD\n",
    "#            analysis each epoch.\n",
    "# Logic: disable gradients, reuse compute_losses for consistency, and keep track\n",
    "#        of scores, labels, weights, and masses as numpy arrays.\n",
    "# Expected behaviour: returns the concatenated arrays alongside averaged metrics.\n",
    "# ---------------------------------------------------------------------------\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    scores, labels_all, weights_all, masses_all = [], [], [], []\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"eval\"):\n",
    "            loss, metrics = compute_losses(model, batch)\n",
    "            features, labels, weights, masses = batch\n",
    "            features = features.to(DEVICE)\n",
    "            _, score = model(features)\n",
    "            batch_size = features.shape[0]\n",
    "            total_examples += batch_size\n",
    "            scores.append(score.cpu().numpy())\n",
    "            labels_all.append(labels.numpy())\n",
    "            weights_all.append(weights.numpy())\n",
    "            masses_all.append(masses.numpy())\n",
    "            agg.setdefault(\"loss_cls\", []).append(metrics[\"loss_cls\"])\n",
    "            for key in (\"accuracy\", \"precision\", \"recall\", \"f1\"):\n",
    "                agg.setdefault(key, []).append(metrics[key])\n",
    "            if \"dCorr_s_m\" in metrics:\n",
    "                agg.setdefault(\"dCorr_s_m\", []).append(metrics[\"dCorr_s_m\"])\n",
    "    duration = time.perf_counter() - start\n",
    "    metrics_mean = {key: float(np.mean(values)) for key, values in agg.items()}\n",
    "    metrics_mean.update({\n",
    "        \"epoch_seconds\": duration,\n",
    "        \"examples_per_second\": total_examples / duration if duration > 0 else float(\"nan\"),\n",
    "        \"iterations_per_second\": len(loader) / duration if duration > 0 else float(\"nan\"),\n",
    "    })\n",
    "    return (\n",
    "        np.concatenate(scores),\n",
    "        np.concatenate(labels_all),\n",
    "        np.concatenate(weights_all),\n",
    "        np.concatenate(masses_all),\n",
    "        metrics_mean,\n",
    "    )\n",
    "\n",
    "history: List[Dict[str, Any]] = []\n",
    "start_epoch = 0\n",
    "if RESUME_CHECKPOINT is not None:\n",
    "    loaded_checkpoint = load_checkpoint(Path(RESUME_CHECKPOINT), model, optimizer)\n",
    "    history = loaded_checkpoint.get(\"history\", [])\n",
    "    start_epoch = int(loaded_checkpoint.get(\"epoch\", -1)) + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# History tracks per-epoch metrics for later plotting and checkpoint metadata.\n",
    "history: List[Dict[str, Any]] = []\n",
    "start_epoch = 0\n",
    "if RESUME_CHECKPOINT is not None:\n",
    "    loaded_checkpoint = load_checkpoint(Path(RESUME_CHECKPOINT), model, optimizer)\n",
    "    history = loaded_checkpoint.get(\"history\", [])\n",
    "    start_epoch = int(loaded_checkpoint.get(\"epoch\", -1)) + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# for epoch in range(start_epoch, EPOCHS):\n",
    "#     print(f\"Epoch {epoch:03d}\")\n",
    "#     train_metrics = train_one_epoch(model, train_loader, optimizer)\n",
    "#     s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "#     # Compute rich validation diagnostics as mandated by the evaluation guide.\n",
    "#     classification = compute_epoch_classification_metrics(s_val, y_val, w_val)\n",
    "#     roc_diag = compute_roc_diagnostics(s_val, y_val, w_val)\n",
    "#     abcd_stats = compute_abcd_statistics(s_val, m_val, y_val, w_val)\n",
    "#     jsd_points = compute_jsd_summary(s_val, m_val, y_val, w_val)\n",
    "\n",
    "#     record = {\n",
    "#         \"epoch\": epoch,\n",
    "#         \"train_loss_cls\": train_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "#         \"train_accuracy\": train_metrics.get(\"accuracy\", float(\"nan\")),\n",
    "#         \"train_precision\": train_metrics.get(\"precision\", float(\"nan\")),\n",
    "#         \"train_recall\": train_metrics.get(\"recall\", float(\"nan\")),\n",
    "#         \"train_f1\": train_metrics.get(\"f1\", float(\"nan\")),\n",
    "#         \"train_epoch_seconds\": train_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "#         \"train_examples_per_second\": train_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "#         \"val_loss_cls\": val_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "#         \"val_accuracy\": classification[\"accuracy\"],\n",
    "#         \"val_precision\": classification[\"precision\"],\n",
    "#         \"val_recall\": classification[\"recall\"],\n",
    "#         \"val_f1\": classification[\"f1\"],\n",
    "#         \"val_auc\": roc_diag[\"auc\"],\n",
    "#         \"val_epoch_seconds\": val_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "#         \"val_examples_per_second\": val_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "#     }\n",
    "#     if \"dCorr_s_m\" in train_metrics:\n",
    "#         record[\"train_dCorr_s_m\"] = train_metrics[\"dCorr_s_m\"]\n",
    "#     if \"dCorr_s_m\" in val_metrics:\n",
    "#         record[\"val_dCorr_s_m\"] = val_metrics[\"dCorr_s_m\"]\n",
    "#     for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "#         key = f\"background_eff_at_{int(target*100)}pct_sig\"\n",
    "#         record[key] = roc_diag[key]\n",
    "#     for item in abcd_stats[\"per_efficiency\"]:\n",
    "#         eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "#         record[f\"abcd_closure_ratio_{eff}pct\"] = item[\"closure_ratio\"]\n",
    "#         record[f\"abcd_pull_{eff}pct\"] = item[\"pull\"]\n",
    "#     aggregated = abcd_stats[\"aggregated\"]\n",
    "#     record[\"abcd_closure_ratio_mean\"] = aggregated[\"closure_ratio\"][\"mean\"]\n",
    "#     record[\"abcd_closure_ratio_std\"] = aggregated[\"closure_ratio\"][\"std\"]\n",
    "#     record[\"abcd_closure_ratio_median\"] = aggregated[\"closure_ratio\"][\"median\"]\n",
    "#     record[\"abcd_pull_mean\"] = aggregated[\"pull\"][\"mean\"]\n",
    "#     record[\"abcd_pull_std\"] = aggregated[\"pull\"][\"std\"]\n",
    "#     record[\"abcd_pull_rms\"] = aggregated[\"pull\"][\"rms\"]\n",
    "#     record[\"abcd_pull_median\"] = aggregated[\"pull\"][\"median\"]\n",
    "#     record[\"transfer_factor_B_over_D_mean\"] = aggregated[\"transfer_factor_B_over_D\"][\"mean\"]\n",
    "#     record[\"transfer_factor_B_over_D_std\"] = aggregated[\"transfer_factor_B_over_D\"][\"std\"]\n",
    "#     record[\"transfer_factor_C_over_D_mean\"] = aggregated[\"transfer_factor_C_over_D\"][\"mean\"]\n",
    "#     record[\"transfer_factor_C_over_D_std\"] = aggregated[\"transfer_factor_C_over_D\"][\"std\"]\n",
    "#     record[\"sideband_ratio_B_over_C_mean\"] = aggregated[\"sideband_ratio_B_over_C\"][\"mean\"]\n",
    "#     record[\"sideband_ratio_B_over_C_std\"] = aggregated[\"sideband_ratio_B_over_C\"][\"std\"]\n",
    "#     record[\"sideband_stability_mean\"] = aggregated[\"sideband_stability\"][\"mean\"]\n",
    "#     record[\"sideband_stability_std\"] = aggregated[\"sideband_stability\"][\"std\"]\n",
    "#     record[\"asimov_significance_mean\"] = aggregated[\"asimov_significance\"][\"mean\"]\n",
    "#     record[\"asimov_significance_max\"] = aggregated[\"asimov_significance\"][\"max\"]\n",
    "#     for item in jsd_points:\n",
    "#         eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "#         record[f\"inverse_jsd_{eff}pct\"] = item[\"inverse_jsd\"]\n",
    "#         record[f\"background_rejection_{eff}pct\"] = item[\"background_rejection\"]\n",
    "\n",
    "#     history.append(record)\n",
    "\n",
    "#     # Persist a checkpoint capturing everything needed to resume training and\n",
    "#     # redo evaluations, aligning with the per-epoch storage requirement.\n",
    "#     val_record = {\n",
    "#         \"classification\": classification,\n",
    "#         \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in roc_diag.items()},\n",
    "#         \"abcd\": abcd_stats,\n",
    "#         \"jsd\": jsd_points,\n",
    "#         \"confusion_matrix\": classification[\"confusion_matrix\"].tolist(),\n",
    "#     }\n",
    "#     train_record = train_metrics\n",
    "#     extra = {\n",
    "#         \"val_scores\": s_val.tolist(),\n",
    "#         \"val_labels\": y_val.tolist(),\n",
    "#         \"val_weights\": w_val.tolist(),\n",
    "#         \"val_masses\": m_val.tolist(),\n",
    "#     }\n",
    "#     save_checkpoint(epoch, model, optimizer, history, train_record, val_record, extra)\n",
    "\n",
    "#     print(\n",
    "#         f\"AUC={roc_diag['auc']:.3f} | \"\n",
    "#         f\"val_acc={classification['accuracy']:.3f} | \"\n",
    "#         f\"closure@30%={record['abcd_closure_ratio_30pct']:.3f} | \"\n",
    "#         f\"train_loss={train_metrics.get('loss_cls', float('nan')):.3f}\"\n",
    "#     )\n",
    "\n",
    "# history_df = pd.DataFrame(history)\n",
    "# history_df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d87e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-collapse overrides active: logits-BCE, KL prior matching, confidence penalty, gradient clipping, AdamW.\n"
     ]
    }
   ],
   "source": [
    "# Anti-collapse overrides for training and QNode. Run this cell once before training.\n",
    "\n",
    "EPS = 1e-8\n",
    "ENTROPY_COEFF = 0.02          # confidence penalty weight (small)\n",
    "PRIOR_MATCH_COEFF = 0.2       # batch prior-matching KL weight\n",
    "VARIANCE_FLOOR = 0.02         # optional: minimum score variance target\n",
    "VARIANCE_COEFF = 0.02         # weight for variance floor penalty\n",
    "CLIP_NORM = 1.0               # gradient clipping\n",
    "WEIGHT_DECAY = 1e-3           # AdamW weight decay\n",
    "LEARNING_RATE = 5e-4          # slightly lower LR for stability\n",
    "\n",
    "# ---- Re-define compute_losses to use BCE-with-logits + anti-collapse terms ----\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).float()\n",
    "    weights = weights.to(DEVICE).float()\n",
    "    masses = masses.to(DEVICE).float()\n",
    "\n",
    "    logits, score = model(features)  # score still used for metrics/DisCo\n",
    "    # Use logit difference for BCE-with-logits (binary)\n",
    "    logit = (logits[:, 1] - logits[:, 0]).contiguous()\n",
    "\n",
    "    # Weighted pos_weight for class imbalance (computed per-batch)\n",
    "    pos_mask = labels > 0.5\n",
    "    w_pos = weights[pos_mask].sum()\n",
    "    w_neg = weights[~pos_mask].sum()\n",
    "    pos_weight = (w_neg / (w_pos + EPS)).clamp(min=0.5, max=5.0)\n",
    "\n",
    "    # Weighted BCE-with-logits\n",
    "    per_example = F.binary_cross_entropy_with_logits(logit, labels, reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss_cls = (weights * per_example).sum() / (weights.sum() + EPS)\n",
    "\n",
    "    # Anti-collapse: (1) batch prior matching (weighted)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    wsum = weights.sum() + EPS\n",
    "    p_bar = (probs * weights.unsqueeze(1)).sum(dim=0) / wsum\n",
    "    prior_pos = (w_pos / (w_pos + w_neg + EPS)).clamp(1e-3, 1 - 1e-3)\n",
    "    prior = torch.stack([1.0 - prior_pos, prior_pos])\n",
    "    kl_prior = (p_bar * (p_bar.add(EPS).log() - prior.add(EPS).log())).sum()\n",
    "\n",
    "    # Anti-collapse: (2) confidence penalty (maximize entropy => add -H)\n",
    "    entropy = -(probs * (probs + EPS).log()).sum(dim=1).mean()\n",
    "    conf_penalty = -ENTROPY_COEFF * entropy\n",
    "\n",
    "    # Optional: encourage non-zero variance of the score\n",
    "    mu = (weights * score).sum() / wsum\n",
    "    var = (weights * (score - mu).pow(2)).sum() / wsum\n",
    "    var_penalty = VARIANCE_COEFF * F.relu(VARIANCE_FLOOR - var)\n",
    "\n",
    "    loss = loss_cls + PRIOR_MATCH_COEFF * kl_prior + conf_penalty + var_penalty\n",
    "\n",
    "    # DisCo decorrelation on background (unchanged)\n",
    "    background = labels < 0.5\n",
    "    metrics = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(compute_weighted_classification_stats(labels, score, weights)[\"accuracy\"]),\n",
    "    }\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "# ---- Gradient clipping in the training step ----\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, lambda_mass: float = LAMBDA_MASS) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    agg: Dict[str, List[float]] = {}\n",
    "    total_examples = 0\n",
    "    for batch in tqdm(loader, leave=False, desc=\"train\"):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, metrics = compute_losses(model, batch, lambda_mass)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_examples += batch_size\n",
    "        for k, v in metrics.items():\n",
    "            agg.setdefault(k, []).append(v)\n",
    "        agg.setdefault(\"loss\", []).append(float(loss.detach().cpu()))\n",
    "    return {k: float(np.mean(v)) for k, v in agg.items()}\n",
    "\n",
    "# ---- Widen quantum readout and bound embedding ----\n",
    "if BACKEND == \"qml\" and hasattr(model, \"qlayer\"):\n",
    "    # Rebuild the PennyLane module with safer embedding and richer readout\n",
    "    class PennyLaneSingleDisco(nn.Module):\n",
    "        def __init__(self, n_features: int, n_qubits: int = globals().get(\"N_QUBITS\", 6), layers: int = globals().get(\"QML_LAYERS\", 3), device_name: str = globals().get(\"QML_DEVICE\", \"default.qubit\")):\n",
    "            if not PENNYLANE_AVAILABLE:\n",
    "                raise RuntimeError(\"PennyLane is not installed.\")\n",
    "            super().__init__()\n",
    "            self.n_qubits = n_qubits\n",
    "            self.n_features = n_features\n",
    "            self.compressor = nn.Sequential(\n",
    "                nn.Linear(n_features, n_qubits),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(p=0.1),\n",
    "            )\n",
    "            qdevice = qml.device(device_name, wires=n_qubits)\n",
    "            weight_shapes = { \"weights_strong\": (layers, n_qubits, 3) }\n",
    "\n",
    "            @qml.qnode(qdevice, interface=\"torch\")\n",
    "            def circuit(inputs, weights_strong):\n",
    "                take = min(inputs.shape[-1], n_qubits)\n",
    "                if inputs.ndim == 1:\n",
    "                    x_pad = torch.zeros((n_qubits,), dtype=inputs.dtype, device=inputs.device)\n",
    "                    x_pad[:take] = inputs[:take]\n",
    "                else:\n",
    "                    pad_shape = tuple(list(inputs.shape[:-1]) + [n_qubits])\n",
    "                    x_pad = torch.zeros(pad_shape, dtype=inputs.dtype, device=inputs.device)\n",
    "                    x_pad[..., :take] = inputs[..., :take]\n",
    "                angles = torch.pi * torch.tanh(x_pad)  # bound angles for stable grads\n",
    "                qml.templates.AngleEmbedding(angles, wires=range(n_qubits), rotation='Y')\n",
    "                qml.templates.StronglyEntanglingLayers(weights=weights_strong, wires=range(n_qubits))\n",
    "                return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "            self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
    "            self.head = nn.Linear(n_qubits, 2)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            angles = self.compressor(x)\n",
    "            q_features = self.qlayer(angles)\n",
    "            logits = self.head(q_features)\n",
    "            score = F.softmax(logits, dim=1)[:, 1]\n",
    "            return logits, score\n",
    "\n",
    "    # Rebuild model and optimizer with AdamW\n",
    "    model = build_model(len(FEATURE_NAMES))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "else:\n",
    "    # Even for classical-only, AdamW + clipping + anti-collapse helps\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Anti-collapse overrides active: logits-BCE, KL prior matching, confidence penalty, gradient clipping, AdamW.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7a74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_losses override loaded: metrics now include precision/recall/F1.\n"
     ]
    }
   ],
   "source": [
    "# Fix: compute_losses returns precision/recall/F1 (keeps anti-collapse + DisCo)\n",
    "# Run this cell once before training/evaluation\n",
    "\n",
    "def compute_losses(model: nn.Module, batch: Tuple[torch.Tensor, ...], lambda_mass: float = LAMBDA_MASS) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    features, labels, weights, masses = batch\n",
    "    features = features.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).float()\n",
    "    weights = weights.to(DEVICE).float()\n",
    "    masses = masses.to(DEVICE).float()\n",
    "\n",
    "    logits, score = model(features)\n",
    "    logit = (logits[:, 1] - logits[:, 0]).contiguous()\n",
    "\n",
    "    # Batch imbalance handling\n",
    "    EPS = globals().get(\"EPS\", 1e-8)\n",
    "    pos_mask = labels > 0.5\n",
    "    w_pos = weights[pos_mask].sum()\n",
    "    w_neg = weights[~pos_mask].sum()\n",
    "    pos_weight = (w_neg / (w_pos + EPS)).clamp(min=0.5, max=5.0)\n",
    "\n",
    "    # Weighted BCE-with-logits (classification loss)\n",
    "    per_example = F.binary_cross_entropy_with_logits(logit, labels, reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss_cls = (weights * per_example).sum() / (weights.sum() + EPS)\n",
    "\n",
    "    # Anti-collapse regularizers (safe defaults if not previously defined)\n",
    "    ENTROPY_COEFF = float(globals().get(\"ENTROPY_COEFF\", 0.01))\n",
    "    PRIOR_MATCH_COEFF = float(globals().get(\"PRIOR_MATCH_COEFF\", 0.1))\n",
    "    VARIANCE_FLOOR = float(globals().get(\"VARIANCE_FLOOR\", 0.02))\n",
    "    VARIANCE_COEFF = float(globals().get(\"VARIANCE_COEFF\", 0.02))\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    wsum = weights.sum() + EPS\n",
    "    p_bar = (probs * weights.unsqueeze(1)).sum(dim=0) / wsum\n",
    "    prior_pos = (w_pos / (w_pos + w_neg + EPS)).clamp(1e-3, 1 - 1e-3)\n",
    "    prior = torch.stack([1.0 - prior_pos, prior_pos])\n",
    "    kl_prior = (p_bar * (p_bar.add(EPS).log() - prior.add(EPS).log())).sum()\n",
    "\n",
    "    entropy = -(probs * (probs + EPS).log()).sum(dim=1).mean()\n",
    "    conf_penalty = -ENTROPY_COEFF * entropy\n",
    "\n",
    "    mu = (weights * score).sum() / wsum\n",
    "    var = (weights * (score - mu).pow(2)).sum() / wsum\n",
    "    var_penalty = VARIANCE_COEFF * F.relu(VARIANCE_FLOOR - var)\n",
    "\n",
    "    loss = loss_cls + PRIOR_MATCH_COEFF * kl_prior + conf_penalty + var_penalty\n",
    "\n",
    "    # DisCo decorrelation on background\n",
    "    background = labels < 0.5\n",
    "    if background.any() and lambda_mass > 0.0:\n",
    "        w_bkg = torch.ones_like(weights[background])\n",
    "        d_mass = distance_corr_safe(score[background], masses[background], w_bkg)\n",
    "        loss = loss + lambda_mass * d_mass\n",
    "    else:\n",
    "        d_mass = None\n",
    "\n",
    "    # Full classification stats expected by evaluate()\n",
    "    stats = compute_weighted_classification_stats(labels, score, weights)\n",
    "    metrics: Dict[str, float] = {\n",
    "        \"loss_cls\": float(loss_cls.detach().cpu()),\n",
    "        \"accuracy\": float(stats.get(\"accuracy\", float(\"nan\"))),\n",
    "        \"precision\": float(stats.get(\"precision\", float(\"nan\"))),\n",
    "        \"recall\": float(stats.get(\"recall\", float(\"nan\"))),\n",
    "        \"f1\": float(stats.get(\"f1\", float(\"nan\"))),\n",
    "    }\n",
    "    if d_mass is not None:\n",
    "        metrics[\"dCorr_s_m\"] = float(d_mass.detach().cpu())\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "print(\"compute_losses override loaded: metrics now include precision/recall/F1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6468b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000\n",
      "Current lambda_mass: 20.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 179.06 MiB is free. Process 788523 has 11.70 GiB memory in use. Process 467927 has 1.56 GiB memory in use. Process 1647418 has 9.32 GiB memory in use. Process 1069582 has 15.67 GiB memory in use. Including non-PyTorch memory, this process has 844.00 MiB memory in use. Of the allocated memory 331.63 MiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m current_lambda = get_current_lambda(epoch)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent lambda_mass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_lambda\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_metrics = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Compute rich validation diagnostics as mandated by the evaluation guide.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, lambda_mass)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, desc=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     73\u001b[39m     optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     loss, metrics = \u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_mass\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     loss.backward()\n\u001b[32m     76\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcompute_losses\u001b[39m\u001b[34m(model, batch, lambda_mass)\u001b[39m\n\u001b[32m      8\u001b[39m weights = weights.to(DEVICE).float()\n\u001b[32m      9\u001b[39m masses = masses.to(DEVICE).float()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits, score = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m logit = (logits[:, \u001b[32m1\u001b[39m] - logits[:, \u001b[32m0\u001b[39m]).contiguous()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Batch imbalance handling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mPennyLaneSingleDisco.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    122\u001b[39m     angles = \u001b[38;5;28mself\u001b[39m.compressor(x)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     q_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.head(q_features)\n\u001b[32m    125\u001b[39m     score = F.softmax(logits, dim=\u001b[32m1\u001b[39m)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/qnn/torch.py:408\u001b[39m, in \u001b[36mTorchLayer.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    405\u001b[39m     inputs = torch.reshape(inputs, (-\u001b[32m1\u001b[39m, inputs.shape[-\u001b[32m1\u001b[39m]))\n\u001b[32m    407\u001b[39m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/qnn/torch.py:434\u001b[39m, in \u001b[36mTorchLayer._evaluate_qnode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[32m    423\u001b[39m \n\u001b[32m    424\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m \u001b[33;03m    tensor: output datapoint\u001b[39;00m\n\u001b[32m    429\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    430\u001b[39m kwargs = {\n\u001b[32m    431\u001b[39m     **{\u001b[38;5;28mself\u001b[39m.input_arg: x},\n\u001b[32m    432\u001b[39m     **{arg: weight.to(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.qnode_weights.items()},\n\u001b[32m    433\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch.Tensor):\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.type(x.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/workflow/qnode.py:922\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/workflow/qnode.py:895\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._transform_program.set_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m res = \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m res = res[\u001b[32m0\u001b[39m]\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/workflow/execution.py:233\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, device, diff_method, interface, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, transform_program, executor_backend)\u001b[39m\n\u001b[32m    229\u001b[39m tapes, outer_post_processing = outer_transform(tapes)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outer_transform.is_informative, \u001b[33m\"\u001b[39m\u001b[33mshould only contain device preprocessing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user_post_processing(outer_post_processing(results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/workflow/run.py:291\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(tapes, device, config, inner_transform_program)\u001b[39m\n\u001b[32m    287\u001b[39m no_interface_boundary_required = (\n\u001b[32m    288\u001b[39m     config.interface == Interface.NUMPY \u001b[38;5;129;01mor\u001b[39;00m config.gradient_method == \u001b[33m\"\u001b[39m\u001b[33mbackprop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m )\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     results = \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# TODO: Prune once support for tf-autograph is dropped\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/workflow/run.py:256\u001b[39m, in \u001b[36m_make_inner_execute.<locals>.inner_execute\u001b[39m\u001b[34m(tapes)\u001b[39m\n\u001b[32m    253\u001b[39m transformed_tapes, transform_post_processing = inner_transform(tapes)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_tapes:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     results = \u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_tapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     results = ()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/modifiers/simulator_tracking.py:28\u001b[39m, in \u001b[36m_track_execute.<locals>.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config=DefaultExecutionConfig):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     results = \u001b[43muntracked_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[32m     30\u001b[39m         batch = (circuits,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/modifiers/single_tape_support.py:30\u001b[39m, in \u001b[36m_make_execute.<locals>.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     28\u001b[39m     is_single_circuit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m     circuits = (circuits,)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m results = \u001b[43mbatch_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/default_qubit.py:707\u001b[39m, in \u001b[36mDefaultQubit.execute\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m    697\u001b[39m     warnings.warn(\n\u001b[32m    698\u001b[39m         (\n\u001b[32m    699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mJitting executions with many circuits may have substantial classical overhead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    702\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebugger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprng_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcm_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpostselect_mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostselect_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    723\u001b[39m vanilla_circuits = convert_to_numpy_parameters(circuits)[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m seeds = \u001b[38;5;28mself\u001b[39m._rng.integers(\u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m, size=\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/default_qubit.py:708\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    697\u001b[39m     warnings.warn(\n\u001b[32m    698\u001b[39m         (\n\u001b[32m    699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mJitting executions with many circuits may have substantial classical overhead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    702\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebugger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprng_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcm_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpostselect_mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmcm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostselect_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m c, _key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(circuits, prng_keys)\n\u001b[32m    721\u001b[39m     )\n\u001b[32m    723\u001b[39m vanilla_circuits = convert_to_numpy_parameters(circuits)[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m seeds = \u001b[38;5;28mself\u001b[39m._rng.integers(\u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m, size=\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/default_qubit.py:1055\u001b[39m, in \u001b[36m_simulate_wrapper\u001b[39m\u001b[34m(circuit, kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_simulate_wrapper\u001b[39m(circuit, kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/qubit/simulate.py:359\u001b[39m, in \u001b[36msimulate\u001b[39m\u001b[34m(circuit, debugger, state_cache, **execution_kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(results)\n\u001b[32m    358\u001b[39m ops_key, meas_key = jax_random_split(prng_key)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m state, is_state_batched = \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mops_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexecution_kwargs\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     state_cache[circuit.hash] = state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/qubit/simulate.py:192\u001b[39m, in \u001b[36mget_final_state\u001b[39m\u001b[34m(circuit, debugger, **execution_kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, MidMeasureMP):\n\u001b[32m    191\u001b[39m     prng_key, key = jax_random_split(prng_key)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m state = \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtape_shots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexecution_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml.Projector):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/functools.py:912\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    910\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/qubit/apply_operation.py:232\u001b[39m, in \u001b[36mapply_operation\u001b[39m\u001b[34m(op, state, is_state_batched, debugger, **_)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_operation\u001b[39m(\n\u001b[32m    168\u001b[39m     op: qml.operation.Operator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     **_,\n\u001b[32m    173\u001b[39m ):\n\u001b[32m    174\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[32m    175\u001b[39m \n\u001b[32m    176\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m \n\u001b[32m    231\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_operation_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/qubit/apply_operation.py:258\u001b[39m, in \u001b[36m_apply_operation_default\u001b[39m\u001b[34m(op, state, is_state_batched, debugger)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_csr_matrix(op, state, is_state_batched=is_state_batched)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mlen\u001b[39m(op.wires) < EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m math.ndim(state) < EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[32m    257\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m (op.batch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_operation_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched=is_state_batched)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/devices/qubit/apply_operation.py:109\u001b[39m, in \u001b[36mapply_operation_einsum\u001b[39m\u001b[34m(op, state, is_state_batched)\u001b[39m\n\u001b[32m    106\u001b[39m         op._batch_size = batch_size  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[32m    107\u001b[39m reshaped_mat = math.reshape(mat, new_mat_shape)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshaped_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/pennylane/math/multi_dispatch.py:572\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(indices, like, optimize, *operands)\u001b[39m\n\u001b[32m    569\u001b[39m operands = np.coerce(operands, like=like)\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m like == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    571\u001b[39m     \u001b[38;5;66;03m# torch einsum doesn't support the optimize keyword argument\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlike\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m like == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# Unpacking and casting necessary for higher order derivatives,\u001b[39;00m\n\u001b[32m    575\u001b[39m     \u001b[38;5;66;03m# and avoiding implicit fp32 down-conversions.\u001b[39;00m\n\u001b[32m    576\u001b[39m     op1, op2 = operands\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/autoray/autoray.py:81\u001b[39m, in \u001b[36mdo\u001b[39m\u001b[34m(fn, like, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m backend = _choose_backend(fn, args, kwargs, like=like)\n\u001b[32m     80\u001b[39m func = get_lib_fn(backend, fn)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/common/software/m4138/ABCDisCo/lib/python3.12/site-packages/torch/functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 179.06 MiB is free. Process 788523 has 11.70 GiB memory in use. Process 467927 has 1.56 GiB memory in use. Process 1647418 has 9.32 GiB memory in use. Process 1069582 has 15.67 GiB memory in use. Including non-PyTorch memory, this process has 844.00 MiB memory in use. Of the allocated memory 331.63 MiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Updated training loop with lambda ramping\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"Epoch {epoch:03d}\")\n",
    "    # Get current lambda value for ramping\n",
    "    current_lambda = get_current_lambda(epoch)\n",
    "    print(f\"Current lambda_mass: {current_lambda:.2f}\")\n",
    "    \n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer, current_lambda)\n",
    "    s_val, y_val, w_val, m_val, val_metrics = evaluate(model, val_loader)\n",
    "\n",
    "    # Compute rich validation diagnostics as mandated by the evaluation guide.\n",
    "    classification = compute_epoch_classification_metrics(s_val, y_val, w_val)\n",
    "    roc_diag = compute_roc_diagnostics(s_val, y_val, w_val)\n",
    "    abcd_stats = compute_abcd_statistics(s_val, m_val, y_val, w_val)\n",
    "    jsd_points = compute_jsd_summary(s_val, m_val, y_val, w_val)\n",
    "\n",
    "    record = {\n",
    "        \"epoch\": epoch,\n",
    "        \"lambda_mass\": current_lambda,  # Track current lambda value\n",
    "        \"train_loss_cls\": train_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "        \"train_accuracy\": train_metrics.get(\"accuracy\", float(\"nan\")),\n",
    "        \"train_precision\": train_metrics.get(\"precision\", float(\"nan\")),\n",
    "        \"train_recall\": train_metrics.get(\"recall\", float(\"nan\")),\n",
    "        \"train_f1\": train_metrics.get(\"f1\", float(\"nan\")),\n",
    "        \"train_epoch_seconds\": train_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "        \"train_examples_per_second\": train_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "        \"val_loss_cls\": val_metrics.get(\"loss_cls\", float(\"nan\")),\n",
    "        \"val_accuracy\": classification[\"accuracy\"],\n",
    "        \"val_precision\": classification[\"precision\"],\n",
    "        \"val_recall\": classification[\"recall\"],\n",
    "        \"val_f1\": classification[\"f1\"],\n",
    "        \"val_auc\": roc_diag[\"auc\"],\n",
    "        \"val_epoch_seconds\": val_metrics.get(\"epoch_seconds\", float(\"nan\")),\n",
    "        \"val_examples_per_second\": val_metrics.get(\"examples_per_second\", float(\"nan\")),\n",
    "    }\n",
    "    if \"dCorr_s_m\" in train_metrics:\n",
    "        record[\"train_dCorr_s_m\"] = train_metrics[\"dCorr_s_m\"]\n",
    "    if \"dCorr_s_m\" in val_metrics:\n",
    "        record[\"val_dCorr_s_m\"] = val_metrics[\"dCorr_s_m\"]\n",
    "    for target in SCORE_SIGNAL_EFFICIENCIES:\n",
    "        key = f\"background_eff_at_{int(target*100)}pct_sig\"\n",
    "        record[key] = roc_diag[key]\n",
    "    for item in abcd_stats[\"per_efficiency\"]:\n",
    "        eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "        record[f\"abcd_closure_ratio_{eff}pct\"] = item[\"closure_ratio\"]\n",
    "        record[f\"abcd_pull_{eff}pct\"] = item[\"pull\"]\n",
    "    aggregated = abcd_stats[\"aggregated\"]\n",
    "    record[\"abcd_closure_ratio_mean\"] = aggregated[\"closure_ratio\"][\"mean\"]\n",
    "    record[\"abcd_closure_ratio_std\"] = aggregated[\"closure_ratio\"][\"std\"]\n",
    "    record[\"abcd_closure_ratio_median\"] = aggregated[\"closure_ratio\"][\"median\"]\n",
    "    record[\"abcd_pull_mean\"] = aggregated[\"pull\"][\"mean\"]\n",
    "    record[\"abcd_pull_std\"] = aggregated[\"pull\"][\"std\"]\n",
    "    record[\"abcd_pull_rms\"] = aggregated[\"pull\"][\"rms\"]\n",
    "    record[\"abcd_pull_median\"] = aggregated[\"pull\"][\"median\"]\n",
    "    record[\"transfer_factor_B_over_D_mean\"] = aggregated[\"transfer_factor_B_over_D\"][\"mean\"]\n",
    "    record[\"transfer_factor_B_over_D_std\"] = aggregated[\"transfer_factor_B_over_D\"][\"std\"]\n",
    "    record[\"transfer_factor_C_over_D_mean\"] = aggregated[\"transfer_factor_C_over_D\"][\"mean\"]\n",
    "    record[\"transfer_factor_C_over_D_std\"] = aggregated[\"transfer_factor_C_over_D\"][\"std\"]\n",
    "    record[\"sideband_ratio_B_over_C_mean\"] = aggregated[\"sideband_ratio_B_over_C\"][\"mean\"]\n",
    "    record[\"sideband_ratio_B_over_C_std\"] = aggregated[\"sideband_ratio_B_over_C\"][\"std\"]\n",
    "    record[\"sideband_stability_mean\"] = aggregated[\"sideband_stability\"][\"mean\"]\n",
    "    record[\"sideband_stability_std\"] = aggregated[\"sideband_stability\"][\"std\"]\n",
    "    record[\"asimov_significance_mean\"] = aggregated[\"asimov_significance\"][\"mean\"]\n",
    "    record[\"asimov_significance_max\"] = aggregated[\"asimov_significance\"][\"max\"]\n",
    "    for item in jsd_points:\n",
    "        eff = int(item[\"target_signal_efficiency\"] * 100)\n",
    "        record[f\"inverse_jsd_{eff}pct\"] = item[\"inverse_jsd\"]\n",
    "        record[f\"background_rejection_{eff}pct\"] = item[\"background_rejection\"]\n",
    "\n",
    "    history.append(record)\n",
    "\n",
    "    # Persist a checkpoint capturing everything needed to resume training and\n",
    "    # redo evaluations, aligning with the per-epoch storage requirement.\n",
    "    val_record = {\n",
    "        \"classification\": classification,\n",
    "        \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in roc_diag.items()},\n",
    "        \"abcd\": abcd_stats,\n",
    "        \"jsd\": jsd_points,\n",
    "        \"confusion_matrix\": classification[\"confusion_matrix\"].tolist(),\n",
    "    }\n",
    "    train_record = train_metrics\n",
    "    extra = {\n",
    "        \"val_scores\": s_val.tolist(),\n",
    "        \"val_labels\": y_val.tolist(),\n",
    "        \"val_weights\": w_val.tolist(),\n",
    "        \"val_masses\": m_val.tolist(),\n",
    "    }\n",
    "    save_checkpoint(epoch, model, optimizer, history, train_record, val_record, extra)\n",
    "\n",
    "    print(\n",
    "        f\"AUC={roc_diag['auc']:.3f} | \"\n",
    "        f\"val_acc={classification['accuracy']:.3f} | \"\n",
    "        f\"closure@30%={record['abcd_closure_ratio_30pct']:.3f} | \"\n",
    "        f\"train_loss={train_metrics.get('loss_cls', float('nan')):.3f} | \"\n",
    "        f\"={current_lambda:.1f}\"\n",
    "    )\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46a892",
   "metadata": {},
   "source": [
    "\n",
    "### Training diagnostics\n",
    "\n",
    "We track the classification loss and the distance-correlation penalty to verify convergence and decorrelation strength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec09beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visual summary of training history (losses, AUC, decorrelation, throughput).\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Panel 0: weighted binary cross-entropy on train vs validation sets.\n",
    "axes[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], label=\"train\", marker=\"o\")\n",
    "axes[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls\"], label=\"val\", marker=\"s\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Binary cross-entropy\")\n",
    "axes[0].set_title(\"Loss history\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel 1: AUC progression across epochs (higher is better signal/background separation).\n",
    "axes[1].plot(history_df[\"epoch\"], history_df[\"val_auc\"], marker=\"o\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[1].set_title(\"Validation ROC AUC\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Panel 2: distance correlation term monitoring the DisCo penalty strength.\n",
    "if \"train_dCorr_s_m\" in history_df.columns:\n",
    "    axes[2].plot(history_df[\"epoch\"], history_df[\"train_dCorr_s_m\"], label=\"train\")\n",
    "if \"val_dCorr_s_m\" in history_df.columns:\n",
    "    axes[2].plot(history_df[\"epoch\"], history_df[\"val_dCorr_s_m\"], label=\"val\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Distance correlation\")\n",
    "axes[2].set_title(\"Decorrelating score vs jet mass\")\n",
    "axes[2].legend()\n",
    "\n",
    "# Panel 3: runtime throughput useful for planning segmented jobs under time limits.\n",
    "axes[3].plot(history_df[\"epoch\"], history_df[\"train_examples_per_second\"], label=\"train\", marker=\"o\")\n",
    "axes[3].plot(history_df[\"epoch\"], history_df[\"val_examples_per_second\"], label=\"val\", marker=\"s\")\n",
    "axes[3].set_xlabel(\"Epoch\")\n",
    "axes[3].set_ylabel(\"Examples per second\")\n",
    "axes[3].set_title(\"Throughput diagnostics\")\n",
    "axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# # Loss evolution\n",
    "# axes[0].plot(history_df[\"epoch\"], history_df[\"train_loss_cls\"], label=\"train\", marker=\"o\")\n",
    "# axes[0].plot(history_df[\"epoch\"], history_df[\"val_loss_cls\"], label=\"val\", marker=\"s\")\n",
    "# axes[0].set_xlabel(\"Epoch\")\n",
    "# axes[0].set_ylabel(\"Binary cross-entropy\")\n",
    "# axes[0].set_title(\"Loss history\")\n",
    "# axes[0].legend()\n",
    "\n",
    "# # Classification metrics\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"train_accuracy\"], label=\"train acc\", marker=\"o\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_accuracy\"], label=\"val acc\", marker=\"s\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_precision\"], label=\"val precision\", linestyle=\"--\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_recall\"], label=\"val recall\", linestyle=\":\")\n",
    "# axes[1].plot(history_df[\"epoch\"], history_df[\"val_f1\"], label=\"val F1\", linestyle=\"-.\" )\n",
    "# axes[1].set_xlabel(\"Epoch\")\n",
    "# axes[1].set_ylabel(\"Score\")\n",
    "# axes[1].set_ylim(0.0, 1.05)\n",
    "# axes[1].set_title(\"Classification summary\")\n",
    "# axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# # ABCD closure and ROC\n",
    "# axes[2].plot(history_df[\"epoch\"], history_df[\"val_auc\"], label=\"AUC\", marker=\"o\")\n",
    "# axes[2].plot(history_df[\"epoch\"], 1.0 / np.maximum(history_df[\"background_eff_at_30pct_sig\"], 1e-6), label=\"1/_B @ 30%\", marker=\"s\")\n",
    "# axes[2].plot(history_df[\"epoch\"], history_df[\"abcd_closure_ratio_30pct\"], label=\"closure ratio @ 30%\", linestyle=\"--\")\n",
    "# axes[2].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"10% closure band\")\n",
    "# axes[2].set_xlabel(\"Epoch\")\n",
    "# axes[2].set_ylabel(\"Metric value\")\n",
    "# axes[2].set_title(\"ROC vs. ABCD stability\")\n",
    "# axes[2].legend()\n",
    "# axes[2].set_yscale(\"log\")\n",
    "\n",
    "# # Runtime diagnostics\n",
    "# axes[3].plot(history_df[\"epoch\"], history_df[\"train_epoch_seconds\"], label=\"train\", marker=\"o\")\n",
    "# axes[3].plot(history_df[\"epoch\"], history_df[\"val_epoch_seconds\"], label=\"val\", marker=\"s\")\n",
    "# axes[3].set_xlabel(\"Epoch\")\n",
    "# axes[3].set_ylabel(\"Seconds\")\n",
    "# axes[3].set_title(\"Per-epoch wall-clock time\")\n",
    "# axes[3].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Safer plotting for HPC: chunk long paths, decimate, sanitize, and limit rows\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "plot_df = history_df.tail(min(len(history_df), 2000)).copy()\n",
    "\n",
    "def decimate(n):  # reduce marker density\n",
    "    return max(1, n // 200)\n",
    "\n",
    "# Panel 0: loss\n",
    "axes[0].plot(plot_df[\"epoch\"], plot_df[\"train_loss_cls\"], label=\"train\", marker=None)\n",
    "axes[0].plot(plot_df[\"epoch\"], plot_df[\"val_loss_cls\"], label=\"val\", marker=None)\n",
    "axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Binary cross-entropy\"); axes[0].set_title(\"Loss history\"); axes[0].legend()\n",
    "\n",
    "# Panel 1: classification summary (no markers for long series)\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"train_accuracy\"], label=\"train acc\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_accuracy\"], label=\"val acc\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_precision\"], label=\"val precision\", linestyle=\"--\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_recall\"], label=\"val recall\", linestyle=\":\")\n",
    "axes[1].plot(plot_df[\"epoch\"], plot_df[\"val_f1\"], label=\"val F1\", linestyle=\"-.\")\n",
    "axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Score\"); axes[1].set_ylim(0.0, 1.05); axes[1].set_title(\"Classification summary\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Panel 2: ROC vs ABCD stability (log-safe)\n",
    "bkg_eff = plot_df.get(\"background_eff_at_30pct_sig\")\n",
    "if bkg_eff is not None:\n",
    "    inv_bkg = 1.0 / np.clip(bkg_eff.astype(float), 1e-6, 1e6)\n",
    "    axes[2].plot(plot_df[\"epoch\"], plot_df[\"val_auc\"], label=\"AUC\")\n",
    "    axes[2].plot(plot_df[\"epoch\"], inv_bkg, label=\"1/_B @ 30%\")\n",
    "    if np.all(inv_bkg > 0):\n",
    "        axes[2].set_yscale(\"log\")\n",
    "    axes[2].plot(plot_df[\"epoch\"], plot_df.get(\"abcd_closure_ratio_30pct\", np.nan), label=\"closure ratio @ 30%\", linestyle=\"--\")\n",
    "    axes[2].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"10% closure band\")\n",
    "    axes[2].set_xlabel(\"Epoch\"); axes[2].set_ylabel(\"Metric value\"); axes[2].set_title(\"ROC vs. ABCD stability\"); axes[2].legend()\n",
    "else:\n",
    "    axes[2].set_visible(False)\n",
    "\n",
    "# Panel 3: runtime diagnostics\n",
    "axes[3].plot(plot_df[\"epoch\"], plot_df[\"train_epoch_seconds\"], label=\"train\")\n",
    "axes[3].plot(plot_df[\"epoch\"], plot_df[\"val_epoch_seconds\"], label=\"val\")\n",
    "axes[3].set_xlabel(\"Epoch\"); axes[3].set_ylabel(\"Seconds\"); axes[3].set_title(\"Per-epoch wall-clock time\"); axes[3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"metrics_compact.png\", dpi=120)  # avoid massive inline payload\n",
    "plt.close(fig)\n",
    "from IPython.display import Image, display\n",
    "display(Image(\"metrics_compact.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483491cb",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluation (`evaluation.py` lines 1-70)\n",
    "\n",
    "We reproduce the single-score diagnostics: ROC curves, background mass sculpting, and the Jensen-Shannon divergence versus background rejection figure of merit used in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd85125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation on the held-out test set following the evaluation guide.\n",
    "from importlib import reload\n",
    "import evaluation\n",
    "reload(evaluation)\n",
    "from evaluation import JSDvsR\n",
    "\n",
    "# Run the evaluate() helper to obtain raw scores, labels, weights, masses.\n",
    "s_test, y_test, w_test, m_test, test_metrics = evaluate(model, test_loader)\n",
    "\n",
    "# Compute the suite of diagnostics requested: classification metrics, ROC, ABCD closure,\n",
    "# JensenShannon vs rejection, and distance correlation on pure background.\n",
    "classification_test = compute_epoch_classification_metrics(s_test, y_test, w_test)\n",
    "roc_test = compute_roc_diagnostics(s_test, y_test, w_test)\n",
    "abcd_test = compute_abcd_statistics(s_test, m_test, y_test, w_test)\n",
    "jsd_test = compute_jsd_summary(s_test, m_test, y_test, w_test)\n",
    "\n",
    "background = y_test < 0.5\n",
    "if background.any():\n",
    "    d_test = distance_corr_safe(\n",
    "        torch.as_tensor(s_test[background]),\n",
    "        torch.as_tensor(m_test[background]),\n",
    "        torch.ones_like(torch.as_tensor(m_test[background])),\n",
    "    ).item()\n",
    "else:\n",
    "    d_test = float(\"nan\")\n",
    "\n",
    "print(\"Test-set diagnostics:\")\n",
    "print(f\"  AUC = {roc_test['auc']:.3f}\")\n",
    "print(f\"  Accuracy = {classification_test['accuracy']:.3f}\")\n",
    "print(f\"  Precision = {classification_test['precision']:.3f}\")\n",
    "print(f\"  Recall = {classification_test['recall']:.3f}\")\n",
    "print(f\"  F1-score = {classification_test['f1']:.3f}\")\n",
    "print(f\"  Background distance-correlation(score, mass) = {d_test:.4f}\")\n",
    "\n",
    "# Visualise ROC, confusion matrix, mass sculpting, and ABCD closure vs rejection.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "axes[0].plot(roc_test[\"fpr\"], roc_test[\"tpr\"], label=f\"ROC (AUC={roc_test['auc']:.3f})\")\n",
    "axes[0].set_xlabel(\"Background efficiency _B\")\n",
    "axes[0].set_ylabel(\"Signal efficiency _S\")\n",
    "axes[0].set_title(\"Receiver Operating Characteristic\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(True)\n",
    "cm = classification_test[\"confusion_matrix\"]\n",
    "im = axes[1].imshow(cm, cmap=\"viridis\")\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels([\"Predicted background\", \"Predicted signal\"])\n",
    "axes[1].set_yticklabels([\"True background\", \"True signal\"])\n",
    "axes[1].set_title(\"Weighted confusion matrix\")\n",
    "for (i, j), value in np.ndenumerate(cm):\n",
    "    axes[1].text(j, i, f\"{value:.0f}\", ha=\"center\", va=\"center\", color=\"white\" if value > cm.max() / 2 else \"black\")\n",
    "fig.colorbar(im, ax=axes[1])\n",
    "nbins = ABCD_HISTOGRAM_BINS\n",
    "mass_range = (m_test.min(), m_test.max())\n",
    "axes[2].hist(\n",
    "    m_test[background], bins=nbins, range=mass_range, weights=w_test[background],\n",
    "    histtype=\"step\", label=\"All background\", linewidth=2\n",
    ")\n",
    "cut = weighted_quantile(s_test[y_test > 0.5], 1 - 0.3, w_test[y_test > 0.5])\n",
    "sel = background & (s_test > cut)\n",
    "axes[2].hist(\n",
    "    m_test[sel], bins=nbins, range=mass_range, weights=w_test[sel],\n",
    "    histtype=\"stepfilled\", alpha=0.4, label=\"Background above 30% signal-eff cut\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Jet mass [GeV]\")\n",
    "axes[2].set_ylabel(\"Weighted events\")\n",
    "axes[2].set_title(\"Mass sculpting diagnostic\")\n",
    "axes[2].legend()\n",
    "rejections = [item[\"background_rejection\"] for item in jsd_test]\n",
    "closures = [item[\"closure_ratio\"] for item in abcd_test[\"per_efficiency\"]]\n",
    "labels_eff = [int(item[\"target_signal_efficiency\"] * 100) for item in abcd_test[\"per_efficiency\"]]\n",
    "axes[3].scatter(rejections, closures, c=labels_eff, cmap=\"plasma\", s=120)\n",
    "for rej, clo, eff in zip(rejections, closures, labels_eff):\n",
    "    axes[3].annotate(f\"{eff}%\", (rej, clo))\n",
    "axes[3].axhspan(0.9, 1.1, color=\"grey\", alpha=0.15, label=\"10% closure band\")\n",
    "axes[3].set_xlabel(\"Background rejection 1/_B\")\n",
    "axes[3].set_ylabel(\"Closure ratio N_pred/N_true\")\n",
    "axes[3].set_title(\"ABCD closure vs. rejection\")\n",
    "axes[3].set_xscale(\"log\")\n",
    "axes[3].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabulate ABCD and JSD results for the notebook summary.\n",
    "abcd_rows = []\n",
    "for abcd_entry, jsd_entry in zip(abcd_test[\"per_efficiency\"], jsd_test):\n",
    "    eff = int(abcd_entry[\"target_signal_efficiency\"] * 100)\n",
    "    abcd_rows.append({\n",
    "        \"signal_efficiency_percent\": eff,\n",
    "        \"score_cut\": abcd_entry[\"score_cut\"],\n",
    "        \"background_prediction\": abcd_entry[\"predicted_bg\"],\n",
    "        \"closure_ratio\": abcd_entry[\"closure_ratio\"],\n",
    "        \"closure_error_percent\": abcd_entry[\"closure_error_pct\"],\n",
    "        \"pull\": abcd_entry[\"pull\"],\n",
    "        \"transfer_factor_B_over_D\": abcd_entry[\"transfer_factor_B_over_D\"],\n",
    "        \"transfer_factor_C_over_D\": abcd_entry[\"transfer_factor_C_over_D\"],\n",
    "        \"sideband_ratio_B_over_C\": abcd_entry[\"sideband_ratio_B_over_C\"],\n",
    "        \"sideband_stability\": abcd_entry[\"sideband_stability\"],\n",
    "        \"signal_in_A\": abcd_entry[\"signal_in_A\"],\n",
    "        \"observed_total_in_A\": abcd_entry[\"observed_total_in_A\"],\n",
    "        \"asimov_significance\": abcd_entry[\"asimov_significance\"],\n",
    "        \"background_rejection\": jsd_entry[\"background_rejection\"],\n",
    "        \"inverse_jsd\": jsd_entry[\"inverse_jsd\"],\n",
    "    })\n",
    "abcd_df = pd.DataFrame(abcd_rows)\n",
    "abcd_df\n",
    "\n",
    "aggregated_df = pd.json_normalize(abcd_test[\"aggregated\"], sep=\"_\")\n",
    "aggregated_df.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ab8b0",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Persist artefacts\n",
    "\n",
    "Save inference scores and the trained model weights for downstream ABCDisCo or `pyhf` studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd407c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_serializable(obj: Any):\n",
    "    # Convert numpy scalars/arrays and torch tensors to built-in Python types recursively.\n",
    "    if isinstance(obj, (np.floating, np.integer, np.bool_)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if torch.is_tensor(obj):\n",
    "        if obj.ndim == 0:\n",
    "            return obj.item()\n",
    "        return obj.detach().cpu().tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [to_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"score\": s_test,\n",
    "    \"label\": y_test,\n",
    "    \"weight\": w_test,\n",
    "    \"mass\": m_test,\n",
    "})\n",
    "results_df.head()\n",
    "\n",
    "summary_payload = {\n",
    "    \"classification\": {k: (v.tolist() if isinstance(v, np.ndarray) else float(v) if np.isscalar(v) else v) for k, v in classification_test.items() if k != \"confusion_matrix\"},\n",
    "    \"confusion_matrix\": classification_test[\"confusion_matrix\"].tolist(),\n",
    "    \"roc\": {k: (v.tolist() if isinstance(v, np.ndarray) else float(v)) for k, v in roc_test.items()},\n",
    "    \"abcd\": abcd_test,\n",
    "    \"jsd\": jsd_test,\n",
    "    \"distance_correlation\": d_test,\n",
    "}\n",
    "summary_path = CHECKPOINT_DIR / \"test_evaluation.json\"\n",
    "with open(summary_path, \"w\") as handle:\n",
    "    json.dump(to_serializable(summary_payload), handle, indent=2)\n",
    "print(f\"Saved detailed test summary to {summary_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811ab61",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Extending to Double-DisCo & QML studies\n",
    "\n",
    "- **Recovering the paper-level numbers**: set `FULL_DATASET = True`, increase `EPOCHS` to 200, and sweep `LAMBDA_MASS` in the range 50-400 as in the reference scans.\n",
    "- **Quantum experiments**: switch `BACKEND = \"qml\"`, tune `N_QUBITS`/`QML_LAYERS`, and initialise the PennyLane device with `qml.seed(SEED)` for reproducibility.\n",
    "- **Transition to Double-DisCo**: after validating this notebook, open `ABCDisCo_tutorial.ipynb` for the two-network variant and reuse the saved preprocessing steps to initialise the dual heads.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABCDisCo",
   "language": "python",
   "name": "abcdisco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
